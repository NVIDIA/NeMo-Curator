# Copyright (c) 2025, NVIDIA CORPORATION.  All rights reserved.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

[build-system]
requires = ["setuptools"]
build-backend = "setuptools.build_meta"

[project]
name = "nemo_curator"
description = "Scalable Data Preprocessing Tool for Training Large Language Models"
readme = { file = "README.md", content-type = "text/markdown" }
authors = [
    { name = "Ryan Wolf", email = "rywolf@nvidia.com" },
    { name = "Joseph Jennings", email = "jjennings@nvidia.com" },
    { name = "Mostofa Patwary", email = "mpatwary@nvidia.com" },
    { name = "Sandeep Subramanian", email = "sasubramania@nvidia.com" },
    { name = "Shrimai Prabhumoye", email = "sprabhumoye@nvidia.com" },
    { name = "Ayush Dattagupta", email = "adattagupta@nvidia.com" },
    { name = "Vibhu Jawa", email = "vjawa@nvidia.com" },
    { name = "Jiwei Liu", email = "jiweil@nvidia.com" },
    { name = "Sarah Yurick", email = "syurick@nvidia.com" },
]
classifiers = [
    "Development Status :: 3 - Alpha",
    "Programming Language :: Python :: 3",
    "Programming Language :: Python :: 3.10",
    "Programming Language :: Python :: 3.12",
]
requires-python = ">=3.10"
dependencies = [
    "awscli>=1.22.55",
    "beautifulsoup4",
    "charset_normalizer>=3.1.0",
    "comment_parser",
    "crossfit>=0.0.9",
    "dask-mpi>=2021.11.0",
    "dask[complete]>=2021.7.1",
    "datasets",
    "distributed>=2021.7.1",
    "fasttext==0.9.3",
    "ftfy==6.1.1",
    "huggingface-hub",
    "in-place==0.5.0",
    "jieba==0.42.1",
    "justext==3.0.1",
    "lxml_html_clean",
    "mecab-python3",
    "mwparserfromhell==0.6.5",
    "numpy<2",
    "openai",
    "peft",
    "platformdirs",
    "presidio-analyzer==2.2.351",
    "presidio-anonymizer==2.2.351",
    "pycld2",
    "resiliparse",
    "sentencepiece",
    "spacy>=3.6.0, <3.8.0",
    "tqdm",
    "trafilatura",
    "transformers>=4.48.0",
    "unidic-lite==1.0.8",
    "usaddress==0.5.10",
    "warcio==1.7.4",
    "zstandard==0.18.0",
]
dynamic = ["version"]

[project.optional-dependencies]
dev = [
    "pre-commit",
    "ruff==0.11.4",
    "pytest"
]
# Installs CPU + GPU text curation modules
cuda12x = [
    "cudf-cu12==25.04",
    "cugraph-cu12==25.04",
    "cuml-cu12==25.04",
    "dask-cuda==25.04",
    "dask-cudf-cu12==25.04",
    "spacy[cuda12x]>=3.6.0, <3.8.0",
]
# Installs CPU + GPU text curation modules with RAPIDS Nightlies
cuda12x_nightly = [
    "cudf-cu12>=25.06.0a0,<=25.06",
    "cugraph-cu12>=25.06.0a0,<=25.06",
    "cuml-cu12>=25.06.0a0,<=25.06",
    "dask-cuda>=25.06.0a0,<=25.06",
    "dask-cudf-cu12>=25.06.0a0,<=25.06",
    "spacy[cuda12x]>=3.6.0, <3.8.0",
]
# Installs CPU + GPU text and image curation modules
image = [
    "nvidia-dali-cuda120",
    "nvidia-nvjpeg2k-cu12",
    "timm>=1.0.8",
    "nemo_curator[cuda12x]",
]
# Installs CPU + GPU text and image curation modules with RAPIDS Nightlies
image_nightly = [
    "nvidia-dali-cuda120",
    "nvidia-nvjpeg2k-cu12",
    "timm>=1.0.8",
    "nemo_curator[cuda12x_nightly]",
]
# Installs all of the above with Stable RAPIDS
all = [
    "nemo_curator[image]",
]
# Installs all of the above with RAPIDS Nightlies
all_nightly = [
    "nemo_curator[image_nightly]",
]

[project.scripts]
get_common_crawl_urls = "nemo_curator.scripts.get_common_crawl_urls:console_script"
get_wikipedia_urls = "nemo_curator.scripts.get_wikipedia_urls:console_script"
download_and_extract = "nemo_curator.scripts.download_and_extract:console_script"
text_cleaning = "nemo_curator.scripts.text_cleaning:console_script"
add_id = "nemo_curator.scripts.add_id:console_script"
make_data_shards = "nemo_curator.scripts.make_data_shards:console_script"
prepare_fasttext_training_data = "nemo_curator.scripts.prepare_fasttext_training_data:console_script"
train_fasttext = "nemo_curator.scripts.train_fasttext:console_script"
filter_documents = "nemo_curator.scripts.filter_documents:console_script"
separate_by_metadata = "nemo_curator.scripts.separate_by_metadata:console_script"
prepare_task_data = "nemo_curator.scripts.prepare_task_data:console_script"
find_matching_ngrams = "nemo_curator.scripts.find_matching_ngrams:console_script"
remove_matching_ngrams = "nemo_curator.scripts.remove_matching_ngrams:console_script"
gpu_compute_minhashes = "nemo_curator.scripts.fuzzy_deduplication.compute_minhashes:console_script"
minhash_buckets = "nemo_curator.scripts.fuzzy_deduplication.minhash_lsh:console_script"
jaccard_map_buckets = "nemo_curator.scripts.fuzzy_deduplication.map_buckets:console_script"
jaccard_shuffle = "nemo_curator.scripts.fuzzy_deduplication.jaccard_shuffle:console_script"
jaccard_compute = "nemo_curator.scripts.fuzzy_deduplication.jaccard_compute:console_script"
gpu_connected_component = "nemo_curator.scripts.fuzzy_deduplication.connected_components:console_script"
buckets_to_edges = "nemo_curator.scripts.fuzzy_deduplication.buckets_to_edges:console_script"
gpu_exact_dups = "nemo_curator.scripts.find_exact_duplicates:console_script"
deidentify = "nemo_curator.scripts.find_pii_and_deidentify:console_script"
domain_classifier_inference = "nemo_curator.scripts.classifiers.domain_classifier_inference:console_script"
quality_classifier_inference = "nemo_curator.scripts.classifiers.quality_classifier_inference:console_script"
aegis_classifier_inference = "nemo_curator.scripts.classifiers.aegis_classifier_inference:console_script"
fineweb_edu_classifier_inference = "nemo_curator.scripts.classifiers.fineweb_edu_classifier_inference:console_script"
instruction_data_guard_classifier_inference = "nemo_curator.scripts.classifiers.instruction_data_guard_classifier_inference:console_script"
multilingual_domain_classifier_inference = "nemo_curator.scripts.classifiers.multilingual_domain_classifier_inference:console_script"
content_type_classifier_inference = "nemo_curator.scripts.classifiers.content_type_classifier_inference:console_script"
prompt_task_complexity_classifier_inference = "nemo_curator.scripts.classifiers.prompt_task_complexity_classifier_inference:console_script"
fineweb_mixtral_edu_classifier_inference = "nemo_curator.scripts.classifiers.fineweb_mixtral_edu_classifier_inference:console_script"
fineweb_nemotron_edu_classifier_inference = "nemo_curator.scripts.classifiers.fineweb_nemotron_edu_classifier_inference:console_script"
blend_datasets = "nemo_curator.scripts.blend_datasets:console_script"
semdedup_extract_embeddings = "nemo_curator.scripts.semdedup.compute_embeddings:console_script"
semdedup_clustering = "nemo_curator.scripts.semdedup.clustering:console_script"
semdedup_extract_unique_ids = "nemo_curator.scripts.semdedup.extract_dedup_data:console_script"
async_llm_pii_redaction = "nemo_curator.scripts.async_llm_pii_redaction:console_script"
llm_pii_redaction = "nemo_curator.scripts.llm_pii_redaction:console_script"

[project.urls]
Homepage = "https://github.com/NVIDIA/NeMo-Curator"

[tool.pytest.ini_options]
markers = [
    "gpu: marks tests as GPU tests (deselect with '-m \"not gpu\"')",
    "asyncio: mark a test as asyncio (used by pytest-asyncio)"
]

[tool.setuptools.dynamic]
version = { attr = "nemo_curator.package_info.__version__" }

[tool.setuptools.packages.find]
include = ["*"]
exclude = ["tests", "tests.*"]

[tool.ruff]
line-length = 119
[tool.ruff.lint]
select = ["ALL"]
ignore = [
    "D",  # pydocstyle
    "PTH",  # use pathlib
    "G",  # no enforcement during logging
    "FBT",  # allow booleans in function / class arguments
    "T20",  # allow printing
    "E501",  # Allow line length violations, leave it to Black
    "ANN002",  # don't annotate **args
    "ANN003",  # don't annotate **kwargs
    "ANN204",  # don't annotate self/cls/special methods (__new__)
    "PT013",  # how to import pytest
    "PERF401",  # don't enforce list comprehension
    "RET505", "RET506", "RET507", "RET508",  # allow branching (if else after return)
    "PGH004",  # allow generic noqa
    "PD901",  # allow naming dataframes df
    "TD002",  # Allow TODO without author
    "TD003",  # Allow TODO without link
    "FIX002",  # Allow TODO to exist,
    "EXE002", # Don't require for a shebang to be present if it's executable
    "COM812", # Disable the trailing comma in linter, because ruff formatter ensures it
]
fixable = ["ALL"]
[tool.ruff.lint.per-file-ignores]
"nemo_curator/modules/__init__.py" = ["E402"]
"nemo_curator/__init__.py" = ["F401", "F403"]
"**.ipynb" = ["EXE001", "EXE003", "EXE005"]
"tests/**" = ["S101", "INP001"]
"tutorials/**" = ["INP001", "N999"]
"tutorials/distributed_data_classification/**" = ["ERA001"]
"tutorials/nemo-retriever-synthetic-data-generation/notebooks/quickstart.ipynb" = ["E402"]
"tutorials/distributed_data_classification/content-type-classification.ipynb" = ["PLE2515"]
"tutorials/nemotron-cc/kenlm_utility.py" = ["RUF001"]
"tutorials/single_node_tutorial/**" = ["ERA001"]
"examples/**" = ["INP001"]
"nemo_curator/utils/import_utils.py" = ["ANN001"]
"nemo_curator/utils/aegis_utils.py" = ["W291", "RUF001"]
