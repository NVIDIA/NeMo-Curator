{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "832b33da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mDEPRECATION: Loading egg at /usr/local/lib/python3.12/dist-packages/lightning_thunder-0.2.0.dev0-py3.12.egg is deprecated. pip 25.1 will enforce this behaviour change. A possible replacement is to use pip for package installation. Discussion can be found at https://github.com/pypa/pip/issues/12330\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mDEPRECATION: Loading egg at /usr/local/lib/python3.12/dist-packages/looseversion-1.3.0-py3.12.egg is deprecated. pip 25.1 will enforce this behaviour change. A possible replacement is to use pip for package installation. Discussion can be found at https://github.com/pypa/pip/issues/12330\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mDEPRECATION: Loading egg at /usr/local/lib/python3.12/dist-packages/lightning_utilities-0.12.0.dev0-py3.12.egg is deprecated. pip 25.1 will enforce this behaviour change. A possible replacement is to use pip for package installation. Discussion can be found at https://github.com/pypa/pip/issues/12330\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mDEPRECATION: Loading egg at /usr/local/lib/python3.12/dist-packages/nvfuser-0.2.23a0+6627725-py3.12-linux-x86_64.egg is deprecated. pip 25.1 will enforce this behaviour change. A possible replacement is to use pip for package installation. Discussion can be found at https://github.com/pypa/pip/issues/12330\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mDEPRECATION: Loading egg at /usr/local/lib/python3.12/dist-packages/opt_einsum-3.4.0-py3.12.egg is deprecated. pip 25.1 will enforce this behaviour change. A possible replacement is to use pip for package installation. Discussion can be found at https://github.com/pypa/pip/issues/12330\u001b[0m\u001b[33m\n",
      "\u001b[0mLooking in indexes: https://pypi.org/simple, https://pypi.ngc.nvidia.com\n",
      "Collecting ipywidgets\n",
      "  Downloading ipywidgets-8.1.5-py3-none-any.whl.metadata (2.3 kB)\n",
      "Requirement already satisfied: comm>=0.1.3 in /usr/local/lib/python3.12/dist-packages (from ipywidgets) (0.2.2)\n",
      "Requirement already satisfied: ipython>=6.1.0 in /usr/local/lib/python3.12/dist-packages (from ipywidgets) (8.31.0)\n",
      "Requirement already satisfied: traitlets>=4.3.1 in /usr/local/lib/python3.12/dist-packages (from ipywidgets) (5.14.3)\n",
      "Collecting widgetsnbextension~=4.0.12 (from ipywidgets)\n",
      "  Downloading widgetsnbextension-4.0.13-py3-none-any.whl.metadata (1.6 kB)\n",
      "Collecting jupyterlab-widgets~=3.0.12 (from ipywidgets)\n",
      "  Downloading jupyterlab_widgets-3.0.13-py3-none-any.whl.metadata (4.1 kB)\n",
      "Requirement already satisfied: decorator in /usr/local/lib/python3.12/dist-packages (from ipython>=6.1.0->ipywidgets) (5.1.1)\n",
      "Requirement already satisfied: jedi>=0.16 in /usr/local/lib/python3.12/dist-packages (from ipython>=6.1.0->ipywidgets) (0.19.2)\n",
      "Requirement already satisfied: matplotlib-inline in /usr/local/lib/python3.12/dist-packages (from ipython>=6.1.0->ipywidgets) (0.1.7)\n",
      "Requirement already satisfied: pexpect>4.3 in /usr/local/lib/python3.12/dist-packages (from ipython>=6.1.0->ipywidgets) (4.9.0)\n",
      "Requirement already satisfied: prompt_toolkit<3.1.0,>=3.0.41 in /usr/local/lib/python3.12/dist-packages (from ipython>=6.1.0->ipywidgets) (3.0.48)\n",
      "Requirement already satisfied: pygments>=2.4.0 in /usr/local/lib/python3.12/dist-packages (from ipython>=6.1.0->ipywidgets) (2.18.0)\n",
      "Requirement already satisfied: stack_data in /usr/local/lib/python3.12/dist-packages (from ipython>=6.1.0->ipywidgets) (0.6.3)\n",
      "Requirement already satisfied: parso<0.9.0,>=0.8.4 in /usr/local/lib/python3.12/dist-packages (from jedi>=0.16->ipython>=6.1.0->ipywidgets) (0.8.4)\n",
      "Requirement already satisfied: ptyprocess>=0.5 in /usr/local/lib/python3.12/dist-packages (from pexpect>4.3->ipython>=6.1.0->ipywidgets) (0.7.0)\n",
      "Requirement already satisfied: wcwidth in /usr/local/lib/python3.12/dist-packages (from prompt_toolkit<3.1.0,>=3.0.41->ipython>=6.1.0->ipywidgets) (0.2.13)\n",
      "Requirement already satisfied: executing>=1.2.0 in /usr/local/lib/python3.12/dist-packages (from stack_data->ipython>=6.1.0->ipywidgets) (2.1.0)\n",
      "Requirement already satisfied: asttokens>=2.1.0 in /usr/local/lib/python3.12/dist-packages (from stack_data->ipython>=6.1.0->ipywidgets) (3.0.0)\n",
      "Requirement already satisfied: pure-eval in /usr/local/lib/python3.12/dist-packages (from stack_data->ipython>=6.1.0->ipywidgets) (0.2.3)\n",
      "Downloading ipywidgets-8.1.5-py3-none-any.whl (139 kB)\n",
      "Downloading jupyterlab_widgets-3.0.13-py3-none-any.whl (214 kB)\n",
      "Downloading widgetsnbextension-4.0.13-py3-none-any.whl (2.3 MB)\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m2.3/2.3 MB\u001b[0m \u001b[31m123.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: widgetsnbextension, jupyterlab-widgets, ipywidgets\n",
      "Successfully installed ipywidgets-8.1.5 jupyterlab-widgets-3.0.13 widgetsnbextension-4.0.13\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager, possibly rendering your system unusable.It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. Use the --root-user-action option if you know what you are doing and want to suppress this warning.\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.0.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install ipywidgets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bd97a648",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import ipywidgets as widgets\n",
    "from IPython.display import display, HTML\n",
    "\n",
    "class DataFrameComparer:\n",
    "    \"\"\"\n",
    "    A memory-efficient UI for comparing two pandas DataFrames side by side\n",
    "    with NVIDIA-themed styling, optimized for large datasets.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, df1, df2, name1=\"DataFrame 1\", name2=\"DataFrame 2\", id_column=\"id\"):\n",
    "        \"\"\"\n",
    "        Initialize with two pandas DataFrames to compare\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        df1 : pandas.DataFrame\n",
    "            The first DataFrame to display\n",
    "        df2 : pandas.DataFrame\n",
    "            The second DataFrame to display\n",
    "        name1 : str\n",
    "            Label for the first DataFrame\n",
    "        name2 : str\n",
    "            Label for the second DataFrame\n",
    "        id_column : str\n",
    "            Name of the ID column to use for row selection (default: \"id\")\n",
    "        \"\"\"\n",
    "        # Store only references, not copies\n",
    "        self.df1 = df1\n",
    "        self.df2 = df2\n",
    "        self.name1 = name1\n",
    "        self.name2 = name2\n",
    "        self.id_column = id_column\n",
    "        \n",
    "        # Validate that the ID column exists in both DataFrames\n",
    "        if self.id_column not in self.df1.columns:\n",
    "            raise ValueError(f\"ID column '{self.id_column}' not found in first DataFrame\")\n",
    "        if self.id_column not in self.df2.columns:\n",
    "            raise ValueError(f\"ID column '{self.id_column}' not found in second DataFrame\")\n",
    "        \n",
    "        # For efficiency with large datasets, create a set of IDs for faster lookup\n",
    "        # and only get a sample for the dropdown to avoid memory issues\n",
    "        self.df1_id_set = set(self.df1[self.id_column])\n",
    "        self.df2_id_set = set(self.df2[self.id_column])\n",
    "        \n",
    "        # For the dropdown, we'll use a sample of IDs that exist in either dataframe\n",
    "        # Prioritize IDs that exist in both dataframes\n",
    "        common_ids = list(self.df1_id_set.intersection(self.df2_id_set))\n",
    "        df1_only_ids = list(self.df1_id_set - self.df2_id_set)\n",
    "        df2_only_ids = list(self.df2_id_set - self.df1_id_set)\n",
    "        \n",
    "        # Take a sample of at most 1000 IDs for the dropdown to prevent memory issues\n",
    "        max_sample_size = 1000\n",
    "        common_sample_size = min(len(common_ids), max_sample_size // 2)\n",
    "        remaining_slots = max_sample_size - common_sample_size\n",
    "        \n",
    "        # Prioritize common IDs, then distribute remaining slots\n",
    "        if common_sample_size > 0:\n",
    "            # Sort before slicing to ensure deterministic selection\n",
    "            common_ids.sort()\n",
    "            id_sample = common_ids[:common_sample_size]\n",
    "        else:\n",
    "            id_sample = []\n",
    "            \n",
    "        # Distribute remaining slots between df1_only and df2_only IDs\n",
    "        if remaining_slots > 0 and (df1_only_ids or df2_only_ids):\n",
    "            df1_sample_size = min(len(df1_only_ids), remaining_slots // 2)\n",
    "            df2_sample_size = min(len(df2_only_ids), remaining_slots - df1_sample_size)\n",
    "            \n",
    "            if df1_sample_size > 0:\n",
    "                df1_only_ids.sort()\n",
    "                id_sample.extend(df1_only_ids[:df1_sample_size])\n",
    "            \n",
    "            if df2_sample_size > 0:\n",
    "                df2_only_ids.sort()\n",
    "                id_sample.extend(df2_only_ids[:df2_sample_size])\n",
    "        \n",
    "        id_sample.sort()\n",
    "        self.id_sample = id_sample\n",
    "        \n",
    "        self._create_widgets()\n",
    "        \n",
    "    def _create_widgets(self):\n",
    "        \"\"\"Create widgets with NVIDIA-themed styling\"\"\"\n",
    "        # Get all column names from both DataFrames (this doesn't create a copy)\n",
    "        self.columns1 = list(self.df1.columns)\n",
    "        self.columns2 = list(self.df2.columns)\n",
    "        \n",
    "        # Custom widget styling\n",
    "        dropdown_layout = widgets.Layout(width='200px')\n",
    "        \n",
    "        # ID selection dropdown - uses sampled IDs to prevent memory issues\n",
    "        id_options = [('All IDs', 'all'), ('Enter ID...', 'enter')]\n",
    "        id_options.extend([(str(id_val), id_val) for id_val in self.id_sample])\n",
    "        \n",
    "        self.select_id = widgets.Dropdown(\n",
    "            options=id_options,\n",
    "            value='all',\n",
    "            description='Select ID:',\n",
    "            style={'description_width': 'initial'},\n",
    "            layout=dropdown_layout\n",
    "        )\n",
    "        \n",
    "        # Text input for entering a specific ID\n",
    "        self.id_input = widgets.Text(\n",
    "            value='',\n",
    "            placeholder='Type an ID value',\n",
    "            description='ID:',\n",
    "            style={'description_width': 'initial'},\n",
    "            layout=widgets.Layout(width='200px', display='none')\n",
    "        )\n",
    "        \n",
    "        # Column selection for DataFrame 1\n",
    "        self.select_cols1 = widgets.Dropdown(\n",
    "            options=[('All Columns', 'all')] + [(col, col) for col in self.columns1],\n",
    "            value='all',\n",
    "            description=f'{self.name1}:',\n",
    "            style={'description_width': 'initial'},\n",
    "            layout=dropdown_layout\n",
    "        )\n",
    "        \n",
    "        # Column selection for DataFrame 2\n",
    "        self.select_cols2 = widgets.Dropdown(\n",
    "            options=[('All Columns', 'all')] + [(col, col) for col in self.columns2],\n",
    "            value='all',\n",
    "            description=f'{self.name2}:',\n",
    "            style={'description_width': 'initial'},\n",
    "            layout=dropdown_layout\n",
    "        )\n",
    "        \n",
    "        # Page controls for navigating large datasets\n",
    "        self.page_size = widgets.Dropdown(\n",
    "            options=[5, 10, 20, 50],\n",
    "            value=10,\n",
    "            description='Page Size:',\n",
    "            style={'description_width': 'initial'},\n",
    "            layout=widgets.Layout(width='150px')\n",
    "        )\n",
    "        \n",
    "        self.page_num = widgets.BoundedIntText(\n",
    "            value=1,\n",
    "            min=1,\n",
    "            max=9999,\n",
    "            step=1,\n",
    "            description='Page:',\n",
    "            style={'description_width': 'initial'},\n",
    "            layout=widgets.Layout(width='120px')\n",
    "        )\n",
    "        \n",
    "        # Apply button with NVIDIA styling\n",
    "        self.apply_button = widgets.Button(\n",
    "            description='Compare',\n",
    "            disabled=False,\n",
    "            button_style='success',\n",
    "            tooltip='Compare DataFrames',\n",
    "            layout=widgets.Layout(width='120px')\n",
    "        )\n",
    "        \n",
    "        # Output area for the DataFrames\n",
    "        self.output = widgets.Output()\n",
    "        \n",
    "        # Connect event handlers\n",
    "        self.apply_button.on_click(self._handle_compare)\n",
    "        self.select_id.observe(self._handle_id_selection_change, names='value')\n",
    "        \n",
    "        # Layout with NVIDIA styling\n",
    "        title = widgets.HTML(\n",
    "            \"<div style='background-color: #1a1a1a; padding: 12px; \" \n",
    "            \"border-radius: 8px; margin-bottom: 15px; font-family: \\\"Segoe UI\\\", sans-serif;'>\"\n",
    "            \"<h4 style='margin: 0; color: #76B900; font-weight: 500;'>DataFrame Comparison</h4></div>\"\n",
    "        )\n",
    "        \n",
    "        # Row for ID selection\n",
    "        id_row = widgets.HBox([\n",
    "            self.select_id,\n",
    "            self.id_input\n",
    "        ], layout=widgets.Layout(justify_content='flex-start', margin='10px 0'))\n",
    "        \n",
    "        # Row for column selections\n",
    "        col_row = widgets.HBox([\n",
    "            self.select_cols1, \n",
    "            self.select_cols2\n",
    "        ], layout=widgets.Layout(justify_content='space-around', margin='10px 0'))\n",
    "        \n",
    "        # Row for pagination controls\n",
    "        page_row = widgets.HBox([\n",
    "            self.page_size,\n",
    "            self.page_num,\n",
    "            self.apply_button\n",
    "        ], layout=widgets.Layout(justify_content='flex-start', margin='0 0 15px 0'))\n",
    "        \n",
    "        self.main_layout = widgets.VBox([\n",
    "            title,\n",
    "            id_row,\n",
    "            col_row, \n",
    "            page_row,\n",
    "            self.output\n",
    "        ], layout=widgets.Layout(margin='10px'))\n",
    "    \n",
    "    def _handle_id_selection_change(self, change):\n",
    "        \"\"\"Handle changes to the ID selection dropdown\"\"\"\n",
    "        if change['new'] == 'enter':\n",
    "            self.id_input.layout.display = 'block'\n",
    "        else:\n",
    "            self.id_input.layout.display = 'none'\n",
    "    \n",
    "    def _render_table_html(self, df_subset, name, has_data=True):\n",
    "        \"\"\"Render a single table with NVIDIA styling\"\"\"\n",
    "        if not has_data:\n",
    "            return f\"\"\"\n",
    "            <div style=\"background-color: #1a1a1a; padding: 10px 15px; border-radius: 6px; margin-bottom: 10px; border-left: 4px solid #76B900;\">\n",
    "                <p style=\"margin: 0; font-weight: 500; color: #76B900;\">{name}</p>\n",
    "            </div>\n",
    "            <div style=\"border-radius: 6px; padding: 20px; text-align: center; background-color: #2d2d2d; color: #aaa;\">\n",
    "                No matching data found\n",
    "            </div>\n",
    "            \"\"\"\n",
    "        \n",
    "        # Use to_html with memory-efficient settings to generate the table HTML\n",
    "        # minimal=True reduces HTML output size\n",
    "        html_table = df_subset.to_html(\n",
    "            index=True, \n",
    "            classes=\"dataframe nvidia-table\",\n",
    "            na_rep=\"\",  # Better for memory\n",
    "            float_format=lambda x: f\"{x:.3f}\",  # Format floats to reduce size\n",
    "            border=0,\n",
    "            justify='left'\n",
    "        )\n",
    "        \n",
    "        return f\"\"\"\n",
    "        <div style=\"background-color: #1a1a1a; padding: 10px 15px; border-radius: 6px; margin-bottom: 10px; border-left: 4px solid #76B900;\">\n",
    "            <p style=\"margin: 0; font-weight: 500; color: #76B900;\">{name}</p>\n",
    "        </div>\n",
    "        <div style=\"border-radius: 6px; overflow: hidden; box-shadow: 0 3px 6px rgba(0,0,0,0.2);\">\n",
    "            {html_table}\n",
    "        </div>\n",
    "        \"\"\"\n",
    "    \n",
    "    def _handle_compare(self, _):\n",
    "        \"\"\"Compare button click handler optimized for memory efficiency\"\"\"\n",
    "        with self.output:\n",
    "            self.output.clear_output()\n",
    "            \n",
    "            # Determine which columns to display - don't create copies here\n",
    "            if self.select_cols1.value == 'all':\n",
    "                cols1 = self.columns1\n",
    "            else:\n",
    "                cols1 = [self.select_cols1.value]\n",
    "                \n",
    "            if self.select_cols2.value == 'all':\n",
    "                cols2 = self.columns2\n",
    "            else:\n",
    "                cols2 = [self.select_cols2.value]\n",
    "            \n",
    "            # Get the page size and current page\n",
    "            page_size = self.page_size.value\n",
    "            page = self.page_num.value\n",
    "            \n",
    "            # Memory-efficient filtering for ID or paged data\n",
    "            if self.select_id.value == 'enter':\n",
    "                # Get ID from text input\n",
    "                try:\n",
    "                    id_value = type(next(iter(self.df1_id_set)))(self.id_input.value)\n",
    "                except (ValueError, StopIteration):\n",
    "                    display(HTML(f\"<div style='color: #76B900; padding: 10px; background-color: #1a1a1a; border-radius: 4px;'>Invalid ID format: {self.id_input.value}</div>\"))\n",
    "                    return\n",
    "                    \n",
    "                # Check if ID exists in either DataFrame\n",
    "                in_df1 = id_value in self.df1_id_set\n",
    "                in_df2 = id_value in self.df2_id_set\n",
    "                \n",
    "                if not in_df1 and not in_df2:\n",
    "                    display(HTML(f\"<div style='color: #76B900; padding: 10px; background-color: #1a1a1a; border-radius: 4px;'>ID {id_value} not found in either DataFrame</div>\"))\n",
    "                    return\n",
    "                \n",
    "                # Memory-efficient filtering\n",
    "                if in_df1:\n",
    "                    # Use .loc for more efficient row selection without copying the entire DataFrame\n",
    "                    mask1 = self.df1[self.id_column] == id_value\n",
    "                    idx1 = mask1.idxmax() if mask1.any() else None\n",
    "                    df1_display = self.df1.loc[[idx1], cols1] if idx1 is not None else pd.DataFrame(columns=cols1)\n",
    "                else:\n",
    "                    df1_display = pd.DataFrame(columns=cols1)\n",
    "                \n",
    "                if in_df2:\n",
    "                    # Use .loc for more efficient row selection without copying the entire DataFrame\n",
    "                    mask2 = self.df2[self.id_column] == id_value\n",
    "                    idx2 = mask2.idxmax() if mask2.any() else None\n",
    "                    df2_display = self.df2.loc[[idx2], cols2] if idx2 is not None else pd.DataFrame(columns=cols2)\n",
    "                else:\n",
    "                    df2_display = pd.DataFrame(columns=cols2)\n",
    "                \n",
    "                # Status messages\n",
    "                id_status_html = \"\"\n",
    "                if not in_df1:\n",
    "                    id_status_html += f\"<div style='color: #76B900; padding: 5px; background-color: #1a1a1a; border-radius: 4px;'>Note: ID {id_value} not found in {self.name1}</div>\"\n",
    "                if not in_df2:\n",
    "                    id_status_html += f\"<div style='color: #76B900; padding: 5px; background-color: #1a1a1a; border-radius: 4px;'>Note: ID {id_value} not found in {self.name2}</div>\"\n",
    "                \n",
    "            elif self.select_id.value != 'all':\n",
    "                # Use selected ID from dropdown\n",
    "                id_value = self.select_id.value\n",
    "                \n",
    "                # Check if ID exists in either DataFrame\n",
    "                in_df1 = id_value in self.df1_id_set\n",
    "                in_df2 = id_value in self.df2_id_set\n",
    "                \n",
    "                # Memory-efficient filtering\n",
    "                if in_df1:\n",
    "                    # Use .loc for more efficient row selection without copying the entire DataFrame\n",
    "                    mask1 = self.df1[self.id_column] == id_value\n",
    "                    idx1 = mask1.idxmax() if mask1.any() else None\n",
    "                    df1_display = self.df1.loc[[idx1], cols1] if idx1 is not None else pd.DataFrame(columns=cols1)\n",
    "                else:\n",
    "                    df1_display = pd.DataFrame(columns=cols1)\n",
    "                \n",
    "                if in_df2:\n",
    "                    # Use .loc for more efficient row selection without copying the entire DataFrame\n",
    "                    mask2 = self.df2[self.id_column] == id_value\n",
    "                    idx2 = mask2.idxmax() if mask2.any() else None\n",
    "                    df2_display = self.df2.loc[[idx2], cols2] if idx2 is not None else pd.DataFrame(columns=cols2)\n",
    "                else:\n",
    "                    df2_display = pd.DataFrame(columns=cols2)\n",
    "                \n",
    "                # Status messages\n",
    "                id_status_html = \"\"\n",
    "                if not in_df1:\n",
    "                    id_status_html += f\"<div style='color: #76B900; padding: 5px; background-color: #1a1a1a; border-radius: 4px;'>Note: ID {id_value} not found in {self.name1}</div>\"\n",
    "                if not in_df2:\n",
    "                    id_status_html += f\"<div style='color: #76B900; padding: 5px; background-color: #1a1a1a; border-radius: 4px;'>Note: ID {id_value} not found in {self.name2}</div>\"\n",
    "                \n",
    "            else:\n",
    "                # Show paged data - calculate indices for current page\n",
    "                start_idx = (page - 1) * page_size\n",
    "                end_idx = start_idx + page_size\n",
    "                \n",
    "                # Check if we're past the end of the data\n",
    "                if start_idx >= len(self.df1):\n",
    "                    df1_display = pd.DataFrame(columns=cols1)\n",
    "                    has_df1_data = False\n",
    "                else:\n",
    "                    # Use efficient row indexing without copying the entire DataFrame\n",
    "                    # iloc is memory-efficient for sequential row access\n",
    "                    end_df1 = min(end_idx, len(self.df1))\n",
    "                    df1_display = self.df1.iloc[start_idx:end_df1][cols1]\n",
    "                    has_df1_data = True\n",
    "                \n",
    "                if start_idx >= len(self.df2):\n",
    "                    df2_display = pd.DataFrame(columns=cols2)\n",
    "                    has_df2_data = False\n",
    "                else:\n",
    "                    # Use efficient row indexing without copying the entire DataFrame\n",
    "                    end_df2 = min(end_idx, len(self.df2))\n",
    "                    df2_display = self.df2.iloc[start_idx:end_df2][cols2]\n",
    "                    has_df2_data = True\n",
    "                \n",
    "                # Pagination info\n",
    "                total_pages1 = (len(self.df1) + page_size - 1) // page_size\n",
    "                total_pages2 = (len(self.df2) + page_size - 1) // page_size\n",
    "                max_pages = max(total_pages1, total_pages2)\n",
    "                \n",
    "                id_status_html = f\"\"\"\n",
    "                <div style='color: #76B900; padding: 5px; background-color: #1a1a1a; border-radius: 4px; margin-top: 10px;'>\n",
    "                    Page {page} of {max_pages} | Showing rows {start_idx+1}-{end_idx} | \n",
    "                    Total: {len(self.df1)} rows in {self.name1}, {len(self.df2)} rows in {self.name2}\n",
    "                </div>\n",
    "                \"\"\"\n",
    "            \n",
    "            # Render tables - process each DataFrame independently\n",
    "            table1_html = self._render_table_html(\n",
    "                df1_display, \n",
    "                self.name1, \n",
    "                has_data=(not df1_display.empty)\n",
    "            )\n",
    "            \n",
    "            table2_html = self._render_table_html(\n",
    "                df2_display, \n",
    "                self.name2, \n",
    "                has_data=(not df2_display.empty)\n",
    "            )\n",
    "            \n",
    "            # NVIDIA-themed styling for better visual appeal\n",
    "            html = f\"\"\"\n",
    "            <div style=\"display: flex; width: 100%; font-family: 'Segoe UI', Tahoma, Geneva, Verdana, sans-serif;\">\n",
    "                <div style=\"flex: 1; padding-right: 10px;\">\n",
    "                    {table1_html}\n",
    "                </div>\n",
    "                <div style=\"flex: 1; padding-left: 10px;\">\n",
    "                    {table2_html}\n",
    "                </div>\n",
    "            </div>\n",
    "            {id_status_html}\n",
    "            <style>\n",
    "            .nvidia-table {{\n",
    "                border-collapse: collapse;\n",
    "                width: 100%;\n",
    "                font-size: 13px;\n",
    "                border: none;\n",
    "            }}\n",
    "            .nvidia-table th {{\n",
    "                background-color: #2d2d2d;\n",
    "                color: #76B900;\n",
    "                padding: 10px 12px;\n",
    "                text-align: left;\n",
    "                border: 1px solid #444;\n",
    "                font-weight: 500;\n",
    "            }}\n",
    "            .nvidia-table td {{\n",
    "                padding: 8px 12px;\n",
    "                border: 1px solid #444;\n",
    "                background-color: #1a1a1a;\n",
    "                color: #f0f0f0;\n",
    "            }}\n",
    "            .nvidia-table tr:nth-child(even) td {{\n",
    "                background-color: #262626;\n",
    "            }}\n",
    "            .nvidia-table tr:hover td {{\n",
    "                background-color: #333;\n",
    "            }}\n",
    "            </style>\n",
    "            \"\"\"\n",
    "            display(HTML(html))\n",
    "    \n",
    "    def display(self):\n",
    "        \"\"\"Display the UI and initial comparison\"\"\"\n",
    "        display(self.main_layout)\n",
    "        self._handle_compare(None)\n",
    "\n",
    "\n",
    "# Memory-efficient function to compare by ID\n",
    "def memory_efficient_compare_by_id(df1, df2, id_value, id_column=\"id\", name1=\"DataFrame 1\", name2=\"DataFrame 2\"):\n",
    "    \"\"\"\n",
    "    Compare a specific row from two DataFrames based on ID value\n",
    "    with optimizations for memory efficiency\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    df1 : pandas.DataFrame\n",
    "        The first DataFrame\n",
    "    df2 : pandas.DataFrame\n",
    "        The second DataFrame\n",
    "    id_value : any\n",
    "        The ID value to look up\n",
    "    id_column : str\n",
    "        The column name containing the ID (default: \"id\")\n",
    "    name1 : str\n",
    "        Label for the first DataFrame\n",
    "    name2 : str\n",
    "        Label for the second DataFrame\n",
    "    \"\"\"\n",
    "    # Check if ID exists in either DataFrame\n",
    "    in_df1 = id_value in set(df1[id_column])\n",
    "    in_df2 = id_value in set(df2[id_column])\n",
    "    \n",
    "    if not in_df1 and not in_df2:\n",
    "        return HTML(f\"<div style='color: #76B900; padding: 10px; background-color: #1a1a1a; border-radius: 4px;'>ID {id_value} not found in either DataFrame</div>\")\n",
    "    \n",
    "    # Memory-efficient filtering\n",
    "    if in_df1:\n",
    "        # Find the index of the row with this ID\n",
    "        mask1 = df1[id_column] == id_value\n",
    "        if mask1.any():\n",
    "            idx1 = mask1.idxmax()\n",
    "            df1_html = df1.loc[[idx1]].to_html(index=True, classes=\"dataframe nvidia-table\")\n",
    "        else:\n",
    "            df1_html = \"<p>No data found</p>\"\n",
    "    else:\n",
    "        df1_html = \"<p>ID not found in this DataFrame</p>\"\n",
    "    \n",
    "    if in_df2:\n",
    "        # Find the index of the row with this ID\n",
    "        mask2 = df2[id_column] == id_value\n",
    "        if mask2.any():\n",
    "            idx2 = mask2.idxmax()\n",
    "            df2_html = df2.loc[[idx2]].to_html(index=True, classes=\"dataframe nvidia-table\")\n",
    "        else:\n",
    "            df2_html = \"<p>No data found</p>\"\n",
    "    else:\n",
    "        df2_html = \"<p>ID not found in this DataFrame</p>\"\n",
    "    \n",
    "    # Status messages\n",
    "    id_status_html = \"\"\n",
    "    if not in_df1:\n",
    "        id_status_html += f\"<div style='color: #76B900; padding: 5px; background-color: #1a1a1a; border-radius: 4px; margin-top: 10px;'>Note: ID {id_value} not found in {name1}</div>\"\n",
    "    if not in_df2:\n",
    "        id_status_html += f\"<div style='color: #76B900; padding: 5px; background-color: #1a1a1a; border-radius: 4px; margin-top: 10px;'>Note: ID {id_value} not found in {name2}</div>\"\n",
    "    \n",
    "    # Create the HTML with NVIDIA styling\n",
    "    html = f\"\"\"\n",
    "    <div style=\"display: flex; width: 100%; font-family: 'Segoe UI', Tahoma, Geneva, Verdana, sans-serif; margin-top: 15px;\">\n",
    "        <div style=\"flex: 1; padding-right: 15px;\">\n",
    "            <div style=\"background-color: #1a1a1a; padding: 10px 15px; border-radius: 8px; margin-bottom: 10px; border-left: 4px solid #76B900;\">\n",
    "                <p style=\"margin: 0; font-weight: 500; color: #76B900; font-size: 16px;\">{name1}</p>\n",
    "            </div>\n",
    "            <div style=\"border-radius: 8px; overflow: hidden; box-shadow: 0 3px 6px rgba(0,0,0,0.2);\">\n",
    "                {df1_html}\n",
    "            </div>\n",
    "        </div>\n",
    "        <div style=\"flex: 1; padding-left: 15px;\">\n",
    "            <div style=\"background-color: #1a1a1a; padding: 10px 15px; border-radius: 8px; margin-bottom: 10px; border-left: 4px solid #76B900;\">\n",
    "                <p style=\"margin: 0; font-weight: 500; color: #76B900; font-size: 16px;\">{name2}</p>\n",
    "            </div>\n",
    "            <div style=\"border-radius: 8px; overflow: hidden; box-shadow: 0 3px 6px rgba(0,0,0,0.2);\">\n",
    "                {df2_html}\n",
    "            </div>\n",
    "        </div>\n",
    "    </div>\n",
    "    {id_status_html}\n",
    "    <style>\n",
    "    .nvidia-table {{\n",
    "        border-collapse: collapse;\n",
    "        width: 100%;\n",
    "        font-size: 13px;\n",
    "        border: none;\n",
    "    }}\n",
    "    .nvidia-table th {{\n",
    "        background-color: #2d2d2d;\n",
    "        color: #76B900;\n",
    "        padding: 10px 12px;\n",
    "        text-align: left;\n",
    "        border: 1px solid #444;\n",
    "        font-weight: 500;\n",
    "    }}\n",
    "    .nvidia-table td {{\n",
    "        padding: 8px 12px;\n",
    "        border: 1px solid #444;\n",
    "        background-color: #1a1a1a;\n",
    "        color: #f0f0f0;\n",
    "    }}\n",
    "    .nvidia-table tr:nth-child(even) td {{\n",
    "        background-color: #262626;\n",
    "    }}\n",
    "    .nvidia-table tr:hover td {{\n",
    "        background-color: #333;\n",
    "    }}\n",
    "    </style>\n",
    "    \"\"\"\n",
    "    return HTML(html)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee60881f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a48e07d370d64f3288a9a770d66f3b76",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='<div style=\\'background-color: #1a1a1a; padding: 12px; border-radius: 8px; margin-bâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    # Sample data\n",
    "    import numpy as np\n",
    "    \n",
    "    # Create two sample DataFrames with ID column\n",
    "    np.random.seed(42)\n",
    "    \n",
    "    # First DataFrame with IDs from 1 to 20000\n",
    "    df1 = pd.DataFrame({\n",
    "        'id': range(1, 20001),\n",
    "        'value_A': np.random.randn(20000),\n",
    "        'category': np.random.choice(['X', 'Y', 'Z'], 20000),\n",
    "    })\n",
    "    \n",
    "    # Second DataFrame with IDs from 1 to 20000 but with slightly different values\n",
    "    df2 = pd.DataFrame({\n",
    "        'id': range(1, 20001),\n",
    "        'value_B': np.random.randn(20000) * 1.2,\n",
    "        'category': np.random.choice(['X', 'Y', 'Z'], 20000),\n",
    "    })\n",
    "    \n",
    "    # Option 1: Use the memory-efficient UI with ID selection\n",
    "    comparer = DataFrameComparer(df1, df2, \"Original\", \"Modified\", id_column=\"id\")\n",
    "    comparer.display()\n",
    "    \n",
    "    # Option 2: Use the memory-efficient function to compare a specific ID\n",
    "    # display(memory_efficient_compare_by_id(df1, df2, 5, id_column=\"id\", name1=\"Original\", name2=\"Modified\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1882d2c-e2c1-4a59-9f9c-c12a76e9e04c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Silence Warnings (HuggingFace internal warnings)\n",
    "\n",
    "%env PYTHONWARNINGS=ignore\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9ef3ef29-ab79-4fea-9050-017b9e9203dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import fasttext\n",
    "import pandas as pd\n",
    "import cudf\n",
    "import dask_cudf\n",
    "import numpy as np\n",
    "import cupy as cp\n",
    "from pathlib import Path\n",
    "from typing import Optional, Tuple, Any, Dict, List\n",
    "from huggingface_hub import hf_hub_download\n",
    "\n",
    "from nemo_curator import get_client\n",
    "from nemo_curator.classifiers import FineWebNemotronEduClassifier, FineWebMixtralEduClassifier\n",
    "from nemo_curator.datasets import DocumentDataset\n",
    "from nemo_curator.utils.distributed_utils import load_object_on_worker\n",
    "from nemo_curator.utils.distributed_utils import get_device_total_memory"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "325f2af0-c7a2-488b-8fb6-d35623159f06",
   "metadata": {},
   "source": [
    "### Initializing NeMo Curator Client\n",
    "This step initializes the NeMo Curator client to enable distributed classification using GPU-based processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b6ea132",
   "metadata": {},
   "outputs": [],
   "source": [
    "%env CUDA_VISIBLE_DEVICES 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "374004c9-fd63-490f-bc81-875fc2f15ae9",
   "metadata": {},
   "outputs": [],
   "source": [
    "client = get_client(cluster_type=\"gpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab00c794-3655-44ee-be33-108958c01f43",
   "metadata": {},
   "source": [
    "### Setting Output File Paths\n",
    "Defines the paths where classification results, threshold values, and final bucketed results will be stored."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15d6977b-885a-4029-a868-bc6d336085ed",
   "metadata": {},
   "source": [
    "# Preparing Text Data for Classification\n",
    "- We create a sample dataset with diverse topics.\n",
    "- Optionally, users can provide a directory containing JSONL files for classification."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56b43d1a-7954-48b0-9c39-fe07c3ca06dc",
   "metadata": {},
   "source": [
    "# Step 1: Run the Classifiers\n",
    "\n",
    "1. Compute the floating-point classification score for each classifier.\n",
    "\n",
    "**Note:** Dask operations are lazy, meaning the classifiers wonâ€™t execute until an eager operation like `to_json`, `compute`, or `persist` is called."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16962500-d2a4-4a40-8804-e7accd44abf5",
   "metadata": {},
   "source": [
    "### FastText Quality Classifier\n",
    "\n",
    "The **FastText Quality Classifier** uses the [`fasttext-oh-eli5`](https://huggingface.co/mlfoundations/fasttext-oh-eli5) model from Hugging Face to assess text quality. It distinguishes **high-quality** (`__label__hq`) responses from lower-quality ones (`__label__cc`).  \n",
    "\n",
    "NeMo Curator allows users to define custom modules like this, enabling seamless integration of specialized models.  \n",
    "\n",
    "- **Model:** [`mlfoundations/fasttext-oh-eli5`](https://huggingface.co/mlfoundations/fasttext-oh-eli5)  \n",
    "- **Training Data:** Reddit ELI5 vs. Wikipedia (200k examples)  \n",
    "- **Output:** Confidence score + optional binary classification (where 1 represents high quality text and 0 represents low quality text)  \n",
    "\n",
    "ðŸ”— **More details:** [Hugging Face Model Card](https://huggingface.co/mlfoundations/fasttext-oh-eli5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c7e28c3-8e25-417a-a1c7-7f5b237d18a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define classifier score mapping\n",
    "classifier_scores = {\n",
    "    \"nemotron-score\": {\n",
    "        \"int_score\": \"fineweb-nemotron-edu-score-int\",\n",
    "        \"float_score\": \"fineweb-nemotron-edu-score\"\n",
    "    },\n",
    "    \"mixtral-score\": {\n",
    "        \"int_score\": \"fineweb-mixtral-edu-score-int\",\n",
    "        \"float_score\": \"fineweb-mixtral-edu-score\"\n",
    "    },\n",
    "    \"fasttext-score\": {\n",
    "        \"int_score\": \"fasttext-quality-score-int\",\n",
    "        \"float_score\": \"fasttext-quality-score\"\n",
    "    }\n",
    "}\n",
    "\n",
    "\n",
    "\n",
    "# Initialize classifiers\n",
    "classifiers = [\n",
    "    FineWebNemotronEduClassifier(batch_size=1024,\n",
    "                         pred_column=classifier_scores[\"nemotron-score\"][\"float_score\"],\n",
    "                         int_column=classifier_scores[\"nemotron-score\"][\"int_score\"]),\n",
    "    FineWebMixtralEduClassifier(batch_size=1024,\n",
    "                         pred_column=classifier_scores[\"mixtral-score\"][\"float_score\"],\n",
    "                         int_column=classifier_scores[\"mixtral-score\"][\"int_score\"]),\n",
    "    FastTextQualityClassifier(pred_column=classifier_scores[\"fasttext-score\"][\"float_score\"],\n",
    "                         int_column=classifier_scores[\"fasttext-score\"][\"int_score\"])\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0ed6bb33",
   "metadata": {},
   "outputs": [],
   "source": [
    "client.cluster.close()\n",
    "client.shutdown()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a672e5d8-bb1e-4fe4-bdd7-f9859a449158",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_dataset = input_dataset\n",
    "for classifier in classifiers:\n",
    "    output_dataset = classifier(dataset=output_dataset)\n",
    "\n",
    "# Dropping int columns\n",
    "# As we add new based on a threshold (in the following columns)\n",
    "output_dataset = output_dataset.df.drop(columns=[v[\"int_score\"] for v in classifier_scores.values()])\n",
    "output_dataset.to_parquet(path=OUTPUT_CLASSIFICATION_RESULTS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ec5cca63-ad01-4481-b910-8bcc735ece3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "del classifiers, output_dataset, input_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1aa51ac8-373c-4318-9bf5-f063321cb3e0",
   "metadata": {},
   "source": [
    "### Read Back in the scored Data Frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "229d2466-9064-4e2e-957e-07e949d2ae1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "scored_data = DocumentDataset.read_parquet(OUTPUT_CLASSIFICATION_RESULTS, backend=\"cudf\")\n",
    "scored_data.df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7ef568a-6c17-4b7c-b201-627f33df26fa",
   "metadata": {},
   "source": [
    "# Step 2: Compute Score Thresholds\n",
    "\n",
    "### Why Compute Thresholds?\n",
    "- To categorize classification scores into percentile-based bins.\n",
    "- Ensures results are comparable across different classifiers.\n",
    "\n",
    "### Approach:\n",
    "1. **Extract classifier scores** from the sampled dataset.\n",
    "2. **Compute weighted percentiles** for each classifier.\n",
    "3. **Save percentile thresholds** for later use in mapping scores.\n",
    "\n",
    "> **Note:** The percentile calculation is weighted by token count so that longer texts (with more tokens) have a greater impact on the thresholds. This ensures that the bins accurately reflect the distribution of content, giving a more meaningful categorization of the scores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "26a34e8a-c893-454c-8f93-d09cd60d99ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "def weighted_percentile(data, percentiles, weights):\n",
    "    \"\"\"\n",
    "    Compute weighted percentiles with the \"inverted_cdf\" method.\n",
    "\n",
    "    Parameters:\n",
    "      data : array-like, the data values.\n",
    "      percentiles : scalar or array-like, percentiles in [0, 100].\n",
    "      weights : array-like, the weights for each data value.\n",
    "    \n",
    "    Returns:\n",
    "      The weighted percentile values.\n",
    "    \"\"\"\n",
    "    data = np.asarray(data)\n",
    "    weights = np.asarray(weights)\n",
    "    \n",
    "    # Sort data and associated weights\n",
    "    sorter = np.argsort(data)\n",
    "    data_sorted = data[sorter]\n",
    "    weights_sorted = weights[sorter]\n",
    "    \n",
    "    # Compute the cumulative sum of weights and normalize it to [0, 1]\n",
    "    cum_weights = np.cumsum(weights_sorted)\n",
    "    total_weight = cum_weights[-1]\n",
    "    normalized_cum_weights = cum_weights / total_weight\n",
    "\n",
    "    # For each desired percentile, find the first data value where\n",
    "    # the normalized cumulative weight is >= (percentile / 100).\n",
    "    percentiles = np.atleast_1d(percentiles)\n",
    "    results = []\n",
    "    for p in percentiles:\n",
    "        # np.searchsorted returns the index where (p/100) should be inserted \n",
    "        # to maintain order.\n",
    "        idx = np.searchsorted(normalized_cum_weights, p / 100.0, side='left')\n",
    "        results.append(data_sorted[idx])\n",
    "    \n",
    "    return np.array(results)\n",
    "\n",
    "\n",
    "def compute_thresholds(score_ar: np.ndarray, token_ar: np.ndarray) -> Dict[str, float]:\n",
    "    \"\"\"\n",
    "    Compute percentile-based thresholds for a given score column using weighted percentiles.\n",
    "\n",
    "    Args:\n",
    "        score_ar (np.ndarray): Array containing the scores.\n",
    "        token_ar (np.ndarray): Array containing token counts for weighting.\n",
    "\n",
    "    Returns:\n",
    "        Dict[str, float]: Dictionary containing percentile thresholds.\n",
    "    \"\"\"\n",
    "    percentiles = np.arange(5, 100, 5)\n",
    "    # NumPy < 2.0 does not support the \"inverted_cdf\" method for computing percentiles \n",
    "    # with weights directly via np.percentile (see commented-out equivalent code below).\n",
    "    # To achieve the same result, we manually implement the weighted percentile computation\n",
    "    # using NumPy primitives.\n",
    "    # thresholds = np.percentile(cc_df_score, percentiles, weights=cc_df_tokens, method='inverted_cdf')\n",
    "    thresholds = weighted_percentile(score_ar, percentiles, weights=token_ar)\n",
    "    return {int(percentile): float(thresh) for percentile, thresh in zip(percentiles, thresholds)}\n",
    "\n",
    "\n",
    "def compute_thresholds_for_score_columns(\n",
    "    df: cudf.DataFrame, text_col_name: str, score_col_names: List[str]\n",
    ") -> Dict[str, Dict[str, float]]:\n",
    "    \"\"\"\n",
    "    Compute percentile-based thresholds for all specified score columns in a DataFrame.\n",
    "\n",
    "    Args:\n",
    "        df (cudf.DataFrame): The DataFrame containing the score columns and text column.\n",
    "        text_col_name (str): The name of the text column used to derive token counts.\n",
    "        score_col_names (List[str]): List of column names for which thresholds should be computed.\n",
    "\n",
    "    Returns:\n",
    "        Dict[str, Dict[str, float]]: A dictionary mapping each score column to its percentile thresholds.\n",
    "    \"\"\"\n",
    "    threshold_dict = {}\n",
    "    token_series = df[text_col_name].str.byte_count()\n",
    "\n",
    "    for score_col in score_col_names:\n",
    "        threshold_dict[score_col] = compute_thresholds(df[score_col].values.get(), token_series.values.get())\n",
    "\n",
    "    return threshold_dict\n",
    "\n",
    "\n",
    "def save_thresholds(threshold_dict: Dict[str, Dict[str, float]],  file_name) -> None:\n",
    "    \"\"\"\n",
    "    Save computed thresholds to a JSON file.\n",
    "\n",
    "    Args:\n",
    "        threshold_dict (Dict[str, Dict[str, float]]): The dictionary containing computed thresholds.\n",
    "        file_name (str, optional): The name of the output JSON file. Defaults to \"thresholds.json\".\n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "    with open(file_name, 'w') as fout:\n",
    "        json.dump(file_name, fout, indent=4)\n",
    "    print(f\"Thresholds saved to {file_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8b0a650-6290-4e60-b388-43950e1f7357",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adjust fraction based on how much can fit in a single GPU (1/2 ish)\n",
    "gpu_memory_available = get_device_total_memory()/2\n",
    "frac = max(1, scored_data.df.memory_usage(deep=True).sum().compute()/gpu_memory_available)\n",
    "sampled_data =  scored_data.df.sample(frac=frac).repartition(npartitions=1)\n",
    "\n",
    "score_col_names = [v[\"float_score\"] for v in classifier_scores.values()]\n",
    "threshold_dict = sampled_data.map_partitions(compute_thresholds_for_score_columns, text_col_name=\"text\", score_col_names=score_col_names).compute().iloc[0]\n",
    "save_thresholds(threshold_dict, OUTPUT_CLASSIFIER_THRESHOLDS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83696e60-be44-434f-acab-ef275253732a",
   "metadata": {},
   "outputs": [],
   "source": [
    "threshold_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "790a9c41-80ee-4885-8c7e-2b34b4e8117c",
   "metadata": {},
   "source": [
    "# Step 3: Convert Floating-Point Scores to Integer Scores\n",
    "\n",
    "### Why Convert?\n",
    "- Floating-point scores are mapped to integer categories (0-19) for easier comparison.\n",
    "- Integer scores are computed using **percentile-based thresholds**.\n",
    "\n",
    "### Process:\n",
    "1. **Retrieve percentile thresholds** from saved JSON.\n",
    "2. **Apply the thresholds to map scores to integer bins**.\n",
    "3. **Store integer scores in the dataset** for final ensemble computation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4d94e75a-0a78-4554-bb39-087b009db1c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def map_scores(df, score_col_name: str, score_int_name: str, bins: List[float]):\n",
    "    \"\"\"\n",
    "    Given a DataFrame df and a column of original scores, \n",
    "    use cp.digitize to map them into integer bins using the given thresholds.\n",
    "    \"\"\"\n",
    "    pred_orig_score = cp.array(df[score_col_name])\n",
    "    pred_int_score = cp.digitize(pred_orig_score, bins)\n",
    "    df[score_int_name] = pred_int_score\n",
    "    return df\n",
    "\n",
    "def map_score_columns(df: cudf.DataFrame, score_col_names: List[str], threshold_dict: Dict[str, dict]):\n",
    "    \"\"\"\n",
    "    For each score column in score_col_names, this function:\n",
    "      1. Creates a new column name by appending '-int'\n",
    "      2. Retrieves the corresponding thresholds from threshold_dict,\n",
    "         sorts them (using the keys which are assumed to be strings of numbers),\n",
    "      3. Passes the bins to map_scores to create the integer score column.\n",
    "    \"\"\"\n",
    "    for score_col_name in score_col_names:\n",
    "        # Build the new integer score column name.\n",
    "        score_int_name = score_col_name + \"-int\"\n",
    "        thresholds = threshold_dict.get(score_col_name)\n",
    "        if thresholds is None:\n",
    "            raise ValueError(f\"No thresholds found for score column '{score_col_name}'\")\n",
    "        \n",
    "        sorted_keys = sorted(thresholds.keys(), key=lambda x: int(x))\n",
    "        # Use cp.array to create a CuPy array from the list of threshold values.\n",
    "        bins = cp.array([thresholds[k] for k in sorted_keys])\n",
    "        \n",
    "        # Map the original score column to the new integer score column.\n",
    "        df = map_scores(df, score_col_name, score_int_name, bins)\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70e6a00e-5e42-493a-9dcb-682df8eead0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "scored_data.df = scored_data.df.map_partitions(map_score_columns, score_col_names, threshold_dict)\n",
    "scored_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cf526dd-363f-4199-bb9d-2ea9b8897fae",
   "metadata": {},
   "source": [
    "# Step 4: Compute the Final Ensembled Score\n",
    "\n",
    "### Purpose:\n",
    "- To combine the predictions from multiple classifiers into a **single representative score**.\n",
    "- The ensemble score is computed as the **maximum of all integer scores** across classifiers.\n",
    "\n",
    "### Approach:\n",
    "1. **Extract integer scores from each classifier.**\n",
    "2. **Compute the max integer score for each data point.**\n",
    "3. **Store the final ensemble score in the dataset.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "380f453d-4d6e-43cc-9d33-3d4d64b854d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "int_column_names = [f'{v[\"float_score\"]}-int' for v in classifier_scores.values()]\n",
    "scored_data.df['ensemble-max-int'] = scored_data.df[int_column_names].max(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "469cafbe-f8d2-466d-9e80-2522c59a0a1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "scored_data.df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35ba68b8-8566-401a-882b-eb2ae0414138",
   "metadata": {},
   "source": [
    "# Step 5: Write Results to Partitioned Buckets\n",
    "\n",
    "\n",
    "### Purpose:\n",
    "- Organize and store classified results in a **structured, partitioned format** to facilitate **annealing-based training** for downstream **LLM fine-tuning** and optimization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b6bfcc8-5fef-41df-9e04-c50c35538ff3",
   "metadata": {},
   "outputs": [],
   "source": [
    "scored_data.to_parquet(OUTPUT_BUCKETED_RESULTS, partition_on=\"ensemble-max-int\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8052be9b-6889-4254-bf21-ef1c8b41b82f",
   "metadata": {},
   "source": [
    "# Verify Results\n",
    "\n",
    "### Process:\n",
    "1. **List available partitions** (each corresponds to a score bucket).\n",
    "2. **Read a sample partition** and validate data integrity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67f0fc7b-eca6-4326-9a58-54d27daaf06a",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_buckets = sorted(os.listdir(OUTPUT_BUCKETED_RESULTS))\n",
    "print(all_buckets)\n",
    "first_bucket= DocumentDataset.read_parquet(os.path.join(OUTPUT_BUCKETED_RESULTS, all_buckets[0]))\n",
    "first_bucket.head()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
