{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9bd01afc",
   "metadata": {},
   "source": [
    "# Nemo Curator Pipeline Example\n",
    "\n",
    "## NeMo Curator Introduction\n",
    "The NeMo Curator is a Python library that consists of a collection of scalable data-mining modules for curating natural language processing (NLP) data for training large language models (LLMs). The modules within the NeMo Data Curator enable NLP researchers to mine high-quality text at scale from massive uncurated web corpora. \n",
    "\n",
    "NeMo Curator includes the following modules to perform data curation:\n",
    "- Data download and Extraction\n",
    "- Language identification and separation\n",
    "- Text reformatting and cleaning\n",
    "- Quality filtering\n",
    "- Document-level deduplication\n",
    "- Multilingual downstream-task decontamination\n",
    "- Distributed Data Classification\n",
    "- Personal identifiable information (PII) redaction\n",
    "\n",
    "NeMo Curator team has perform ablation experiments using Common Crawl dataset to train a 357M GPT-style model to assess the effect of different curation stage on model performance. \n",
    "\n",
    "![alt text](./image/zeroshot_ablations.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b1808ea",
   "metadata": {},
   "source": [
    "## About this notebook\n",
    "\n",
    "\n",
    "This notebook will use **Thai Wikipedia dataset** as example to demonstrate a typical data curation pipeline using NeMo Curator. After running through this script, user will be able to know how to use NDC to download wikipedia data, perform language separation using fasttext, perform GPU based exact deduplication and fuzzy deduplication and use CPU based heuristic filtering. \n",
    "\n",
    "Step description:\n",
    "1. Download and extract data\n",
    "2. Language detection and separation\n",
    "3. GPU based deduplication\n",
    "    1. Exact deduplication\n",
    "    2. Fuzzy deduplication\n",
    "4. Heuristic filtering\n",
    "\n",
    "What is not included:\n",
    "1. Customized downloading\n",
    "2. Classifier filtering\n",
    "3. Downstream-task decontamination\n",
    "4. Distributed data classification with PyTorch models\n",
    "5. Personal identifiable information (PII) redaction \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78537bd7",
   "metadata": {},
   "source": [
    "## Prerequisites\n",
    "\n",
    "### System Requirements\n",
    "Here is the hardware setting for this notebook\n",
    "\n",
    "**GPU**: NVIDIA A10 24G. \n",
    "\n",
    "**CUDA & Nvidia Drivers**: CUDA 12.2 with Driver 535.154.05\n",
    "\n",
    "**OS**: ubuntu 22.04\n",
    "\n",
    "### Getting NeMo Framework Training Container\n",
    "- Get access to the container via https://developer.nvidia.com/nemo-framework\n",
    "- Set your docker credentials \n",
    "    ```bash\n",
    "    docker login nvcr.io\n",
    "\n",
    "    Username: $oauthtoken\n",
    "    Password: <Your NGC Key>\n",
    "- Get NeMo NeMo Framework Training Container\n",
    "    ```bash\n",
    "    docker pull nvcr.io/nvidia/nemo:dev\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "062b5423",
   "metadata": {},
   "source": [
    "## 0. Env Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8add9bbd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!pip install jsonlines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97079227-d9c3-40d2-b939-64b221b86990",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%env CUDA_VISIBLE_DEVICES 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9940c70d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "from nemo_curator.utils.distributed_utils import get_client, get_num_workers\n",
    "from nemo_curator.utils.file_utils import get_all_files_paths_under, separate_by_metadata\n",
    "from nemo_curator.utils.distributed_utils import read_data, write_to_disk\n",
    "from nemo_curator.datasets import DocumentDataset\n",
    "\n",
    "import pandas as pd\n",
    "import time\n",
    "import cudf\n",
    "import dask_cudf\n",
    "import dask\n",
    "import numpy as np\n",
    "from dask.distributed import Client, LocalCluster\n",
    "import jsonlines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd8a381d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def pre_imports():\n",
    "    import cudf \n",
    "\n",
    "def check_jsonl_file(file_dir):\n",
    "    for file in os.listdir(file_dir):\n",
    "        if 'jsonl' not in file:\n",
    "            continue\n",
    "        with open(os.path.join(file_dir,file), 'r', encoding='utf-8') as f:\n",
    "            first_line = f.readline()\n",
    "            print(first_line)\n",
    "        break\n",
    "\n",
    "def extract_lines_with_id(file_path,target_list):\n",
    "    with jsonlines.open(file_path) as reader:\n",
    "        for obj in reader:\n",
    "            if obj.get('id') in target_list:\n",
    "                yield obj\n",
    "\n",
    "def get_base_dataset_file_name(download_folder):\n",
    "    files = os.listdir(download_folder)\n",
    "    for file in files:\n",
    "        if file.startswith('thwiki') and file.endswith(''):\n",
    "            return file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "589ff257",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "cur_dir = os.getcwd()\n",
    "print(cur_dir)\n",
    "data_dir = f\"{cur_dir}/workspace/\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "662d505f",
   "metadata": {},
   "source": [
    "## 1. Download\n",
    "In this example, Thai wikipedia data will be downloaded.\n",
    "\n",
    "Here is what happens when function `download_wikipedia()` is called:\n",
    "1. Run `get_wikipedia_urls()` to obtain a list of urls to download .bz2 files for Thai wikipedia data. In this module, we use the base link and the language from user input to formulate a repo links for downloadable wikipedia .bz2 dump files. The formulated link will be `https://dumps.wikimedia.org/<language>wiki`. All the links will be stored in a .txt file. Argument for this function includes:\n",
    "    - `dump_dates`: A date in the string format of 'YYYYMMDD'. It determines which wikipedia snapshot will be downloaded. If not specified, the `latest` snapshot will be downloaded\n",
    "    - `language`: language code of the desired language in lower case. Default value is `en`\n",
    "\n",
    "2. \n",
    "    Run `download_and_extract()` to download and extract contents based on the url list obtained from `get_wikipedia_urls`. User will need to define `downloader`, `extractor` and `iterator` for the dataset. \n",
    "    In this case, `WikipediaDownloader`,`WikipediaIterator` and `WikipediaExtractor` are used.\n",
    "    - `WikipediaDownloader`: Downloads wikipedia dumps file to local folder.\n",
    "    - `WikipediaIterator`: Extracts the .bz2 files and useful content from the base html content.\n",
    "    -  `WikipediaExtractor`: Performs further task specific html content cleaning such as removing media files, removing references/tables etc. and finally yield pure text data which will be store in .jsonl format. \n",
    "    Please refer to `./NeMo-Curator/nemo_curator/download/wikipedia.py` for  detail implementation.\n",
    "    \n",
    "    Argument for this function includes:\n",
    "    - `output_path`: Output path for downloaded and extracted dataset\n",
    "    - `output_type`: Type of output file. Default is .jsonl. User might choose other types such as parquet. In this example, .jsonl will be used\n",
    "    - `language`: See above\n",
    "    - `dump_date`: See above\n",
    "    - `raw_download_dir`: Output path for intermediate downloaded .bz2 file. If not specified, will be downloaded to `output_path`\n",
    "    - `keep_raw_download`: Whether to keep downloaded .bz2 files after extraction. Default is not to keep.\n",
    "    - `force_download`: Whether to restart downloading process if the target .bz2 files are detected under the `raw_download_dir` \n",
    "    - `url_limit`: Number of .bz2 files to be downloaded.\n",
    "\n",
    "The resultant .jsonl for Thai wikipedia will contain the following keys:\n",
    "1. text\n",
    "2. title\n",
    "3. id\n",
    "4. url\n",
    "5. language\n",
    "6. source_id\n",
    "7. file_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adb59379",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from nemo_curator.download import download_wikipedia"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b56f12a",
   "metadata": {},
   "source": [
    " Start a CPU based Dask cluster. Please modify `n_workers` and `memory_limit` according to your hardware specification. To process TH wikipedia data, it's advised to have `memory_limit` greater than 12GB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e822b5ac",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "client = get_client(cluster_type=\"cpu\", n_workers=10, processes=True, memory_limit='16GiB')\n",
    "client"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e90cc8b1",
   "metadata": {},
   "source": [
    "Define parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a03b463",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Output\n",
    "download_base_directory= os.path.join(data_dir,\"wiki_downloads\")\n",
    "download_output_directory = os.path.join(download_base_directory,\"data\")\n",
    "\n",
    "#Relevant parameters\n",
    "language = 'th'\n",
    "url_limit = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f41734a1",
   "metadata": {},
   "source": [
    "Download TH wikipedia data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a45965a7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "res = download_wikipedia(download_output_directory,\n",
    "                   language=language, \n",
    "                   url_limit=url_limit).df.compute()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22b7d5b3",
   "metadata": {},
   "source": [
    "**[Optional]** Verify result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45a69041",
   "metadata": {},
   "outputs": [],
   "source": [
    "# List all the file in the output directory.\n",
    "# ! ls {download_output_directory}\n",
    "\n",
    "# Please replace your dataset file name accordingly.\n",
    "# ! wc -l  {download_output_directory}/{YOUR DATASET FILE NAME}.jsonl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53bdccfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "check_jsonl_file(download_output_directory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "024d7a31-20a3-4d2a-9b89-2f85638fa0da",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!rm -r {download_output_directory}/downloads"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5f58643",
   "metadata": {},
   "source": [
    "**[Optional]** Close the Dask cluster.You might encounter error such as `Caught signal 11`.It's OK, just rerun the cell again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0669a830",
   "metadata": {},
   "outputs": [],
   "source": [
    "# client.cluster.close()\n",
    "# client.shutdown()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43334988",
   "metadata": {},
   "source": [
    "## 2.Language separation and unicode fixing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86ccdc1f",
   "metadata": {},
   "source": [
    "In this section, we will be using a language classification model by fasttext to separate the TH wikipedia dataset based on the document major languages, and we will also fix the unicode in the documents. Detailed steps are:\n",
    "\n",
    "1. Download fasttext model for text language detection\n",
    "2. Construct a filter which uses the downloaded fasttext model to produce a language label to each document. \n",
    "3. Separate each document by the language label. This will create sub-folders for each languages under the output path and the documents under the same language will be output to a .jsonl file in the corresponding sub-folder.\n",
    "4. Load .jsonl file in the folder of desirable language. In this example, `TH` folder will be loaded.\n",
    "5. Apply `UnicodeReformatter` to the data and output the result in .jsonl format. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e9198e8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from nemo_curator import ScoreFilter, Modify\n",
    "from nemo_curator.filters import FastTextLangId\n",
    "from nemo_curator.modifiers import UnicodeReformatter"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76e46d2a",
   "metadata": {},
   "source": [
    "**[Optional]** Start a cpu based Dask cluster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da3aed8a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# client = get_client(cluster_type=\"cpu\", n_workers=10, processes=True, memory_limit='16GiB')\n",
    "# client"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a72479c",
   "metadata": {},
   "source": [
    "Define parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13b9d2b1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Input path\n",
    "multilingual_data_path = download_output_directory\n",
    "\n",
    "# Output path\n",
    "language_base_output_path = os.path.join(data_dir,\"language_sep\")\n",
    "language_data_output_path = os.path.join(language_base_output_path,\"data\")\n",
    "language_separated_output_path = os.path.join(language_data_output_path,\"language\")\n",
    "lang_sep_cleaned_data_output_path = os.path.join(language_data_output_path,\"cleaned\")\n",
    "\n",
    "# Fasttext model path\n",
    "model_path = language_base_output_path\n",
    "\n",
    "# Define desired language\n",
    "target_language = \"TH\"\n",
    "\n",
    "# Define key in output .jsonl files to store the language information\n",
    "language_field = \"language\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8df0322a",
   "metadata": {},
   "source": [
    "Download fasttext model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2666727d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!wget https://dl.fbaipublicfiles.com/fasttext/supervised-models/lid.176.bin -P {model_path}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58452516",
   "metadata": {},
   "source": [
    "Apply fasttext model to separate documents by their languages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8b8c491",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "t0 = time.time()\n",
    "\n",
    "# Load dataset \n",
    "multilingual_dataset = DocumentDataset.read_json(multilingual_data_path, blocksize=\"64MiB\", add_filename=True)\n",
    "\n",
    "#Define Language separation pipeline\n",
    "lang_filter = FastTextLangId(os.path.join(model_path,'lid.176.bin'))\n",
    "language_id_pipeline = ScoreFilter(lang_filter, score_field=language_field, score_type='object')\n",
    "filtered_dataset = language_id_pipeline(multilingual_dataset)\n",
    "\n",
    "# The language separation pipeline will produce a result looks like ['EN',0.96873], we only want to keep the 'EN' label and drop the detailed classifier score\n",
    "filtered_dataset.df[language_field] = filtered_dataset.df[language_field].apply(lambda score: score[1],meta = (language_field, 'object'))\n",
    "\n",
    "# Split the dataset to corresponding language sub-folders\n",
    "language_stats = separate_by_metadata(filtered_dataset.df, language_separated_output_path, metadata_field=language_field).compute()\n",
    "\n",
    "print(f\"Time taken for splitting language:{time.time()-t0}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d443a5d1",
   "metadata": {},
   "source": [
    "Load `UnicodeReformatter` to reformat any unicode appeared in the desired language dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "272a5f67",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "t0 = time.time()\n",
    "\n",
    "# Read the language specific data and fix the unicode in it\n",
    "lang_data_path = os.path.join(language_separated_output_path, target_language)\n",
    "lang_data = DocumentDataset.read_json(lang_data_path, blocksize=\"64MiB\", add_filename=True)\n",
    "\n",
    "cleaner = Modify(UnicodeReformatter())\n",
    "cleaned_data = cleaner(lang_data)\n",
    "\n",
    "# Write the cleaned_data\n",
    "cleaned_data.to_json(lang_sep_cleaned_data_output_path, write_to_filename=True)\n",
    "\n",
    "print(f\"Time taken for fixing unicode:{time.time()-t0}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bd57a53",
   "metadata": {},
   "source": [
    "**[Optional]** Verify the result. We can see that some documents has been removed from TH wikipedia dataset since the number of lines in this output file is less than the original file "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3329c83",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# List all the file in the output directory.\n",
    "# ! ls {lang_sep_cleaned_data_output_path}\n",
    "\n",
    "# Please replace your dataset file name accordingly.\n",
    "# ! wc -l  {lang_sep_cleaned_data_output_path}/{YOUR DATASET FILE NAME}.jsonl"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b6cbc26",
   "metadata": {},
   "source": [
    "Furthur verify by loading documents that has been identified as other language, such as 'EN'. We can see from output that the removed document is indeed in English and contains very little or even no Thai."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "050d944c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "check_jsonl_file(os.path.join(language_separated_output_path,'EN'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d17f010",
   "metadata": {},
   "source": [
    "**[Optional]** Close the Dask cluster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e64cc35",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# client.cluster.close()\n",
    "# client.shutdown()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d46cece",
   "metadata": {},
   "source": [
    "## 3.Add ID\n",
    "TH wikipedia data do have `id` field, but the `id` field contains number only. It will be better if we unified the `id` field and transform it to the format of `<prefix>_<id>`. In this way, when handling multiple dataset, we will be able to know which document from which dataset has been removed. This `id` will be useful when we are running deduplication and heuristic filtering. The function we will be using is `AddID()`. Arguments for this function include:\n",
    "- `id_field`: fields will be added to input .json file. If the key already exists in the .jsonl, it's value will be replaced.\n",
    "- `id_prefix`: prefix used in ID. Default is 'doc_id'\n",
    "- `start_index`: starting index in ID. Default is None. When set to None, an unordered ID scheme will be used for fast calculation. In this notebook, it's set to 0 for easier reference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f788b91",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from nemo_curator import AddId"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd17be33",
   "metadata": {},
   "source": [
    "**[Optional]** If there is no running Dask cluster, start CPU based Dask cluster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ba1d54a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# cluster = LocalCluster(n_workers=10, processes=True, memory_limit='16GB')\n",
    "# client = Client(cluster)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12f59d5e",
   "metadata": {},
   "source": [
    "Define relevant parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "843eba7f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Input\n",
    "add_id_input_data_dir = lang_sep_cleaned_data_output_path\n",
    "\n",
    "#Output\n",
    "added_id_output_path = os.path.join(data_dir,\"add_id/cleaned\")\n",
    "\n",
    "#Format of output ID will be <prefix>_<id>, Define prefix here\n",
    "add_ID_id_prefix=\"TH_wiki\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7a8307c",
   "metadata": {},
   "source": [
    "Adding ID to dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7a91bf1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "t0 = time.time()\n",
    "# Read input files\n",
    "dataset = DocumentDataset.read_json(add_id_input_data_dir,add_filename=True)\n",
    "\n",
    "# Run AddID() on the input dataset\n",
    "add_id = AddId(id_field='id',id_prefix=add_ID_id_prefix,start_index=0)\n",
    "id_dataset = add_id(dataset)\n",
    "\n",
    "#Output files\n",
    "id_dataset.to_json(added_id_output_path, write_to_filename=True)\n",
    "\n",
    "print(f\"Time taken for add ID:{time.time()-t0}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e92b5dab",
   "metadata": {},
   "source": [
    "Verify the result. From the output, we can see that the `id` value has been changed to `TH_wiki-0000000000` "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e585cedd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "check_jsonl_file(added_id_output_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cbddf6e",
   "metadata": {},
   "source": [
    "Close Dask cluster. This cell needs to be run as we are starting a new GPU Dask cluster in the following task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4daa1f2a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "client.cluster.close()\n",
    "client.shutdown()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1baf027e",
   "metadata": {},
   "source": [
    "## 4.Exact Deduplication\n",
    "\n",
    "In exact deduplication, the document text is hashed into unique string using certain hashing algorithm, such as 'md5'. The documents with exact hashed values are having identical text. We will output the `ID` of duplicated documents for removal later. The function used is `ExactDuplicates()`. Arguments for this function include:\n",
    "- `id_field`: Key in input file for identifying document ID\n",
    "- `text_field`: Key in input file which contains document text.\n",
    "- `hash_method`: Hashing algorithm used. Default is `md5`\n",
    "- `cache_dir`: If specified, the duplicated document IDs will be output to the `cache_dir`. Otherwise, the IDs will not be saved\n",
    "\n",
    "Also, we are going to use GPU dask cluster to accelerate computation for deduplication (both exact and fuzzy)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f7ba34c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from nemo_curator.modules import ExactDuplicates"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e268cfca",
   "metadata": {},
   "source": [
    "Start a GPU based Dask cluster. Since GPU based Dask cluster involves setting several arguments, we will use the `get_client()` wrapper function to quickly set up. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b73e5f9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "client = get_client(cluster_type = 'gpu', set_torch_to_use_rmm=False)\n",
    "print(f\"Number of dask worker:{get_num_workers(client)}\")\n",
    "client.run(pre_imports)\n",
    "client"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fc99440",
   "metadata": {},
   "source": [
    "If you encounter the following error\n",
    "`get_client() missing 1 required positional argument: 'args'`:\n",
    "\n",
    "This is probably because the `nemo_curator` library is not updated to the newer version. Please run the following line in the terminal, following instruction in our [GitHub](https://github.com/nicoleeeluo/NeMo-Curator/tree/main) repo, and restart the notebook. Intermediate result of the previous section has been saved to local, you can start from this section after updating."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a590c78a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#pip install --extra-index-url https://pypi.nvidia.com \".[cuda12x]\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0151abe0",
   "metadata": {},
   "source": [
    "Define parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54b627a4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Input\n",
    "exact_dedup_input_dataset_dir = added_id_output_path\n",
    "\n",
    "#Output\n",
    "exact_dedup_base_output_path = os.path.join(data_dir,\"exact_dedup\")\n",
    "exact_dedup_log_dir = os.path.join(exact_dedup_base_output_path,'log')\n",
    "exact_dedup_output_dir = os.path.join(exact_dedup_base_output_path,'data')\n",
    "\n",
    "#Parameters for ExactDuplicates()\n",
    "exact_dedup_dataset_id_field = \"id\"\n",
    "exact_dedup_dataset_text_field = \"text\" \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ede2e41",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!mkdir -p {exact_dedup_log_dir}\n",
    "!mkdir -p {exact_dedup_output_dir}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1882204a",
   "metadata": {},
   "source": [
    "Apply exact deduplication"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfaaa765",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "t0 = time.time()\n",
    "# Read input dataset\n",
    "input_dataset = DocumentDataset.read_json(exact_dedup_input_dataset_dir, backend='cudf')\n",
    "\n",
    "#Run exact deduplication to the input\n",
    "exact_dup = ExactDuplicates(\n",
    "    logger=exact_dedup_log_dir,\n",
    "    id_field=exact_dedup_dataset_id_field,\n",
    "    text_field=exact_dedup_dataset_text_field,\n",
    "    hash_method=\"md5\",\n",
    "    cache_dir=exact_dedup_output_dir #Duplicated document ID list is output to the cache_dir\n",
    ")\n",
    "duplicates = exact_dup(dataset=input_dataset)\n",
    "\n",
    "print(f\"Number of exact duplicated file:{len(duplicates)}\")\n",
    "\n",
    "print(f\"Time taken for exact duplicate:{time.time()-t0}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e68f0399",
   "metadata": {},
   "source": [
    "**[Optional]** Verify the output duplicated ID. We can group by the `_hashes` to get the list of duplicated documents having the same _hashes and use `extract_lines_with_id()` to verify that those documents are indeed exact duplicates. Please note that the `id` might changes, therefore, please replace the `target_list` when necessary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28d8bb0b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "exact_dedup_res = pd.read_parquet(os.path.join(exact_dedup_output_dir,\"_exact_duplicates.parquet\"))\n",
    "print(f\"Number of exact duplicated document:{len(exact_dedup_res)}\")\n",
    "exact_dedup_res.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fca41870",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "duplicated_list = exact_dedup_res.groupby('_hashes')['id'].agg(list).reset_index().head()\n",
    "duplicated_list"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "597d04a4-0b82-43f9-9f61-61729e768ab1",
   "metadata": {},
   "source": [
    "Using the duplicated id shown above, check the content to see if it's exact duplicates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c9624ac",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# example_duplicates = duplicated_list[\"id\"].to_list()[0][0:4]\n",
    "\n",
    "# for line in extract_lines_with_id(os.path.join(exact_dedup_input_dataset_dir,'{YOUR DATASET FILE NAME}'),example_duplicates):\n",
    "#     print(line)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4013203c",
   "metadata": {},
   "source": [
    "**[Optional]** You might choose to close Dask cluster here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ef2f05e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# client.cluster.close()\n",
    "# client.shutdown()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a2feadc",
   "metadata": {},
   "source": [
    "## 5. Fuzzy Deduplication\n",
    "Fuzzy deduplication involves 3 to 5 intermediate steps to generate duplicates. Refer to https://docs.nvidia.com/nemo-framework/user-guide/latest/datacuration/gpudeduplication.html for details\n",
    "\n",
    "Fuzzy deduplication in this example is a GPU implementation of MinhashLSH algorithm. This algorithm measures similarity based on statistics but not semantic meanings of text. There are a few concepts to be introduced before heading into fuzzy deduplication.\n",
    "1. Jaccard similarity: Jaccard similarity is often used as a metric to calculate the similarity between two sets. It's calculated by dividing the number of common elements in the two sets (Intersection) by the number of total unique elements in the two sets (Union). In the case of text documents, we transform a document into a set of n-grams. If two documents share a large amount of n-grams, most likely the documents are similar. \n",
    "\n",
    "    ![alt text](./image/jaccard.png )\n",
    "\n",
    "2. Complexity of the problem: To find all the similar document pairs in a dataset, we need to compute pair-wise Jaccard similarity across the dataset. Hence, making the complexity $O(N^2)$\n",
    "\n",
    "The MinhashLSH algorithm is a technique for quickly estimating the similarity between sets, such as the similarity between documents represented as sets of shingles (n-grams). It's able to find out Jaccard similar pair in the corpus but in a much computational efficient way. This algorithm has following steps in a high-level:\n",
    "1. Compute minhash for each document.\n",
    "2. Run Locality Sensitive Hashing (LSH) based on the minhash which further assign buckets to each document. Each document will be assigned to multiple buckets. Documents within the same bucket are deemed to be similar.\n",
    "3. **[Optional]**: Run pair-wise Jaccard similarity within documents in each bucket to remove false positive cases within the buckets.\n",
    "4. Based on the Buckets and jaccard values between documents (if computed), transform documents across buckets (deemed similar) into a graph and run the connected components algorithm. For a group of connected components in the graph, they are the final similar document groups and the IDs within each groups will be output for duplicate removal.\n",
    "More detailed explanation please refer to https://docs.nvidia.com/nemo-framework/user-guide/latest/datacuration/gpudeduplication.html#fuzzy-deduplication.\n",
    "\n",
    "For implementation of MinhashLSH on GPU, there are 3 to 5 steps:\n",
    "1. Minhash computation\n",
    "2. Bucket computation (LSH)\n",
    "3. **[Optional]**: Jaccard shuffle for load balancing in a distributed system\n",
    "4. **[Optional]**: Jaccard similarity computation\n",
    "5. Connected component \n",
    "\n",
    "In this section, we will firstly provide an example using the fuzzy deduplication wrapper for both cases where we skip the false positive check and where we want to compute false positives. This will be followed by examples of the sub-steps (non false positive check) for users to have a better understanding on what is going on under the hood."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffca14ad",
   "metadata": {},
   "source": [
    "**If there is not running Dask cluster, start a GPU Dask cluster here**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e00ba2fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# client = get_client(cluster_type = 'gpu', set_torch_to_use_rmm=False)\n",
    "# print(f\"Number of dask worker:{get_num_workers(client)}\")\n",
    "# client.run(pre_imports)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f36436f3",
   "metadata": {},
   "source": [
    "### 5.1 Fuzzy deduplication wrapper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb52ec06",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from nemo_curator import FuzzyDuplicates, FuzzyDuplicatesConfig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "625c1828",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Input\n",
    "fuzzy_dedup_data_path = added_id_output_path\n",
    "#Output\n",
    "fuzzy_dedup_base_output_path = os.path.join(data_dir,\"fuzzy_wrapper\")\n",
    "fuzzy_dedup_log_dir = os.path.join(fuzzy_dedup_base_output_path,'log')\n",
    "fuzzy_dedup_no_false_positive_cache_dir = os.path.join(fuzzy_dedup_base_output_path,'cache_nofp')\n",
    "fuzzy_dedup_false_positive_cache_dir = os.path.join(fuzzy_dedup_base_output_path,'cache_fp')\n",
    "fuzzy_dedup_output_dir = os.path.join(fuzzy_dedup_base_output_path,'data')\n",
    "#Specify dataset name\n",
    "dataset_name = 'TH_wikipedia'\n",
    "\n",
    "#Relevant parameters\n",
    "id_field = 'id'\n",
    "text_field = 'text'\n",
    "filetype = \"parquet\"\n",
    "\n",
    "!mkdir -p {fuzzy_dedup_base_output_path}\n",
    "!mkdir -p {fuzzy_dedup_log_dir}\n",
    "!mkdir -p {fuzzy_dedup_no_false_positive_cache_dir}\n",
    "!mkdir -p {fuzzy_dedup_false_positive_cache_dir}\n",
    "!mkdir -p {fuzzy_dedup_output_dir}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb76d8e5",
   "metadata": {},
   "source": [
    "**[Optional]** If the cache folder is not empty, please CLEAR the folder before proceeding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7fb4c4c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# !rm -r {fuzzy_dedup_no_false_positive_cache_dir}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1543c517-5f70-4bd3-a122-16ae8937fd29",
   "metadata": {},
   "source": [
    "#### 5.1.1 Skipping the False positive check (Recommended)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2368443f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "t0 = time.time()\n",
    "\n",
    "input_dataset = DocumentDataset.read_json(fuzzy_dedup_data_path, backend='cudf')\n",
    "\n",
    "fuzzy_dedup_config = FuzzyDuplicatesConfig(\n",
    "    cache_dir=fuzzy_dedup_no_false_positive_cache_dir,\n",
    "    id_field=id_field,\n",
    "    text_field=text_field,\n",
    "    seed=10,\n",
    "    char_ngrams=24,\n",
    "    num_buckets=20,\n",
    "    hashes_per_bucket=13,\n",
    "    use_64_bit_hash=False,\n",
    "    buckets_per_shuffle=5,\n",
    "    false_positive_check=False,\n",
    ")\n",
    "\n",
    "fuzzy_dup = FuzzyDuplicates(logger=fuzzy_dedup_log_dir, config=fuzzy_dedup_config)\n",
    "duplicates = fuzzy_dup(dataset=input_dataset)\n",
    "\n",
    "duplicates.to_parquet(fuzzy_dedup_output_dir, write_to_filename=False)\n",
    "\n",
    "print(f\"Time taken for Fuzzy Deduplication (No False Positive Check): {time.time()-t0} s\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14bfe3bc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "fuzzy_dedup_res = pd.read_parquet(fuzzy_dedup_output_dir)\n",
    "fuzzy_dedup_res.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f30b8b4-bf8d-4d5b-9f13-b0e1f54f723d",
   "metadata": {},
   "source": [
    "#### 5.1.2 Running the False positive check (Computationally Expensive)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "387f58db",
   "metadata": {},
   "source": [
    "**[Optional]** If the cache folder is not empty, please CLEAR the folder before proceeding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a115732f-7590-428c-bd52-b8e73fcadb81",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!rm -r {fuzzy_dedup_false_positive_cache_dir}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ab47746-f011-47bc-aa2a-f1ec80e4ae4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "t0 = time.time()\n",
    "\n",
    "input_dataset = DocumentDataset.read_json(fuzzy_dedup_data_path, backend='cudf')\n",
    "\n",
    "fuzzy_dedup_config = FuzzyDuplicatesConfig(\n",
    "    cache_dir=fuzzy_dedup_false_positive_cache_dir,\n",
    "    id_field=id_field,\n",
    "    text_field=text_field,\n",
    "    seed=10,\n",
    "    char_ngrams=24,\n",
    "    num_buckets=20,\n",
    "    hashes_per_bucket=13,\n",
    "    use_64_bit_hash=False,\n",
    "    buckets_per_shuffle=5,\n",
    "    false_positive_check=True,\n",
    "    jaccard_threshold=0.8, # Documents with similarity less than this value in a bucket will not be considered duplicates.\n",
    ")\n",
    "\n",
    "fuzzy_dup = FuzzyDuplicates(logger=fuzzy_dedup_log_dir, config=fuzzy_dedup_config)\n",
    "duplicates = fuzzy_dup(dataset=input_dataset)\n",
    "\n",
    "duplicates.to_parquet(fuzzy_dedup_output_dir, write_to_filename=False)\n",
    "\n",
    "print(f\"Time taken for Fuzzy Deduplication (False Positive Check): {time.time()-t0} s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9429530f-f5f7-49ed-9044-eb5759fe0f65",
   "metadata": {},
   "outputs": [],
   "source": [
    "fuzzy_dedup_res = pd.read_parquet(fuzzy_dedup_output_dir)\n",
    "fuzzy_dedup_res.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5df73743",
   "metadata": {},
   "source": [
    "### 5.2 Minhash\n",
    "\n",
    "Run `MinHash()` for this section. The output of a minhash is a parquet file which contains document ID and hashed value which is an array contains 260 32-bit integer data. To obtain such hashed values we need to go through the following steps:\n",
    "1. Generate a set of n-gram components of a document. For example, doc = `Nemo Curator is a data curation tool`, a 3-gram set of this document will be `['Nemo Curator is','Curator is a','is a data','a data curation','data curation tool']`\n",
    "2. Hashed each n-gram into numerical values\n",
    "3. Generate a random hash function $H_1()$ which will hash each numeric n-gram into a 32-bit integer and take the minimum integer to use as minhash value for $H_1()$\n",
    "4. Repeat step 2 and 3 with hash function $H_x()$ until desired minhash length is reached. Minhash value of each iteration will be append together to form the final minhash array. \n",
    "\n",
    "Arguments include:\n",
    "- `seed`:Random seed used for initializing the hash functions used to compute the MinHashes. It's advised to keep this value the same for different experiment for reproducibility\n",
    "- `num_hashes`:Length of each minhash array. Default is 260. Longer minhash length will have better estimate of actual Jaccard similarity, but require more computational power\n",
    "- `char_ngrams`:n-gram length. Assuming an average of 4.5 chars per word it's recommended to use `char_ngrams>=24` to use ~5 word ngrams or greater.\n",
    "- `use_64bit_hash`:Whether to use 64bit or 32bit hash function\n",
    "- `id_field`: Key in input file for identifying document ID\n",
    "- `text_field`: Key in input file which contains document text.\n",
    "- `cache_dir`: If specified, the intermediate result will be output to the `cache_dir`. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fc5bff3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from nemo_curator import MinHash"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bf9cc8d",
   "metadata": {},
   "source": [
    "Define parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d600d1b8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Input\n",
    "minhash_data_path = added_id_output_path\n",
    "#Output\n",
    "minhash_base_output_path = os.path.join(data_dir,\"fuzzy/minhash\")\n",
    "minhash_log_dir = os.path.join(minhash_base_output_path,'log')\n",
    "minhash_output_dir = os.path.join(minhash_base_output_path,'data')\n",
    "#Specify dataset name\n",
    "dataset_name = 'TH_wikipedia'\n",
    "\n",
    "#Relevant parameters\n",
    "minhash_id_field = 'id'\n",
    "minhash_text_field = 'text'\n",
    "seed = 10 # Using the same value as the wrapper above for consistency\n",
    "minhash_length = 260\n",
    "char_ngram = 24\n",
    "use_64bit_hash = False\n",
    "files_per_partition = 2\n",
    "\n",
    "!mkdir -p {minhash_log_dir}\n",
    "!mkdir -p {minhash_output_dir}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c31ddf4",
   "metadata": {},
   "source": [
    "Run MinHash"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88540950",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "t0 = time.time()\n",
    "print(f\"Computing minhashes for {minhash_data_path}\")\n",
    "\n",
    "# Load data. Only the [minhash_id_field, text_field] columns are needed\n",
    "files = get_all_files_paths_under(\n",
    "    root=minhash_data_path, recurse_subdirectories=False, keep_extensions=\"jsonl\"\n",
    ")\n",
    "df = read_data(\n",
    "    files,\n",
    "    file_type=\"jsonl\",\n",
    "    backend=\"cudf\",\n",
    "    files_per_partition=files_per_partition,\n",
    "    add_filename=False,\n",
    ")[[minhash_id_field, minhash_text_field]]\n",
    "\n",
    "# Run MinHash() on input data\n",
    "minhasher = MinHash(\n",
    "    seed=seed,\n",
    "    num_hashes=minhash_length,\n",
    "    char_ngrams=char_ngram,\n",
    "    use_64bit_hash=use_64bit_hash,\n",
    "    logger=minhash_log_dir,\n",
    "    id_field=minhash_id_field,\n",
    "    text_field=minhash_text_field,\n",
    "    cache_dir=minhash_output_dir\n",
    ")\n",
    "res = minhasher(DocumentDataset(df)).df\n",
    "\n",
    "print(f\"Time taken for MinHash:{time.time()-t0}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "158bf3ab",
   "metadata": {},
   "source": [
    "**[Optional]** Verify result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10b5eb55",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# minhash_res = pd.read_parquet(os.path.join(minhash_output_dir, \"_minhashes.parquet\"))\n",
    "# minhash_res.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bce0f80",
   "metadata": {},
   "source": [
    "### 5.3 LSH\n",
    "`LSH()` implements LSH algorithm which includes the following steps:\n",
    "1. Divide the minhash array into `X` different portions. \n",
    "2. For each portions, hash the minhash values into buckets. One document will be assigned to `X` buckets.\n",
    "3. Documents within the same bucket will be deemed similar. Since every document will be assigned `X` buckets and as long as two documents share 1 or more buckets they are deemed similar.\n",
    "\n",
    "Arguments include:\n",
    "- `minhash_length`:Length of minhash signature. Must be consistent with `MinHash()`\n",
    "- `num_buckets`: Number of buckets\n",
    "- `buckets_per_shuffle`: Number of buckets to shuffle concurrently\n",
    "- `id_field`: Key in input file for identifying document ID\n",
    "- `minhash_field`: Key in input file for identifying document MinHash signature \n",
    "- `cache_dir`:If specified, the intermediate result will be output to the `cache_dir`.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "645b8a53",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from nemo_curator import LSH"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "110db216",
   "metadata": {},
   "source": [
    "Define parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "738ab265",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Input\n",
    "lsh_input_data_path = minhash_output_dir\n",
    "\n",
    "#Output\n",
    "lsh_base_output_path = os.path.join(data_dir,\"fuzzy/lsh\")\n",
    "lsh_log_dir = os.path.join(lsh_base_output_path,'log')\n",
    "lsh_output_dir = os.path.join(lsh_base_output_path,'data')\n",
    "\n",
    "#Relevant parameters\n",
    "lsh_id_field = 'id'\n",
    "minhash_field = '_minhash_signature'\n",
    "minhash_length=260\n",
    "num_bands=20\n",
    "buckets_per_shuffle=1\n",
    "\n",
    "!mkdir -p {lsh_log_dir}\n",
    "!mkdir -p {lsh_output_dir}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5250a2a",
   "metadata": {},
   "source": [
    "Run LSH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ef61e2b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "t0 = time.time()\n",
    "\n",
    "#Load MinHash output\n",
    "df = dask_cudf.read_parquet(lsh_input_data_path, blocksize=\"2GB\", aggregate_files=True, backend = \"cudf\")\n",
    "\n",
    "#Run LSH()\n",
    "lsh = LSH(\n",
    "    cache_dir=lsh_output_dir,\n",
    "    num_hashes=minhash_length,\n",
    "    num_buckets=num_bands,\n",
    "    buckets_per_shuffle=buckets_per_shuffle,\n",
    "    id_fields=[\"id\"],\n",
    "    minhash_field=minhash_field,\n",
    "    logger=lsh_log_dir,\n",
    ")\n",
    "res = lsh(DocumentDataset(df))\n",
    "\n",
    "t1 = time.time()\n",
    "print(f\"Time taken for LSH:{time.time()-t0}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad2e3b60",
   "metadata": {},
   "source": [
    "**[Optional]** Verify result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d0449c6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# lsh_res = pd.read_parquet(os.path.join(lsh_output_dir, \"_buckets.parquet\"))\n",
    "# lsh_res.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f952f074",
   "metadata": {},
   "source": [
    "### 5.4 Buckets to Edges\n",
    "In this section, we will be using `BucketsToEdges()`\n",
    "\n",
    "`BucketsToEdges()` is designed to take the bucket information which is output of LSH, and create an edgelist dataset where documents with the same `_bucket_id` are connected with an edge between them. This edgelist can then be passed on the connected components to identify groups of similar documents across buckets. Since the false positive check is skipped all documents within a bucket are considered to be duplicates of each other and assigned a jaccard similarity of 1.0 to avoid edge removal during the next step.\n",
    "\n",
    "- `id_field`: Key in input .jsonl file for identifying document ID\n",
    "- `bucket_field`: Key in input _buckets.parquet which contains `bucket_id`\n",
    "- `cache_dir`: If specified, the intermediate result will be output to the `cache_dir`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "707ea54d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from nemo_curator import BucketsToEdges"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f2e321d",
   "metadata": {},
   "source": [
    "Define parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70e2dff9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Input\n",
    "input_bucket_path = lsh_output_dir\n",
    "\n",
    "#Output\n",
    "buckets_to_edges_base_output_path = os.path.join(data_dir,\"fuzzy/buckets_to_edges\")\n",
    "edgelist_output_dir = os.path.join(buckets_to_edges_base_output_path, \"data\")\n",
    "buckets_to_edges_log_path = os.path.join(buckets_to_edges_base_output_path,\"log\")\n",
    "\n",
    "#Relevant parameters for BucketsToEdges()\n",
    "input_id_field = 'id'\n",
    "\n",
    "\n",
    "!mkdir -p {edgelist_output_dir}\n",
    "!mkdir -p {buckets_to_edges_log_path}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0f19efa",
   "metadata": {},
   "source": [
    "Run Jaccard map bucket"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2850b0a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "t0 = time.time()\n",
    "\n",
    "# Read \"_buckets.parquet\"\n",
    "ddf_bk = DocumentDataset.read_parquet(input_bucket_path, backend=\"cudf\")\n",
    "\n",
    "#Run _MapBuckets()\n",
    "buckets_to_edges = BucketsToEdges(cache_dir=edgelist_output_dir, id_fields=input_id_field, logger=buckets_to_edges_log_path)\n",
    "res = buckets_to_edges(ddf_bk)\n",
    "\n",
    "print(f\"Time taken for Bucket->Edgelist:{time.time()-t0} s\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1533a15",
   "metadata": {},
   "source": [
    "**[Optional]** Verify result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d74012c3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# edgelist_res = pd.read_parquet(os.path.join(edgelist_output_dir, \"_edges.parquet\"))\n",
    "# edgelist_res.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a505402e",
   "metadata": {},
   "source": [
    "### 5.5 Connected Components\n",
    "This section uses `ConnectedComponents()`.This section takes a dataset consisting of document pairs and their corresponding jaccard similarity to construct a non-directed graph. A edge will be formed between documents whose Jaccard similarity is higher than the threshold. It will then identify the connected components in this graph. Documents within the same connected components are deemed duplicated.\n",
    "\n",
    "Arguments include:\n",
    "- `cache_dir`: Output path for intermediate results\n",
    "- `jaccard_pairs_path`: Input path for `jaccard_similarity_results.parquet`\n",
    "- `id_column`: prefix of ID column in `jaccard_similarity_results.parquet`\n",
    "- `jaccard_threshold`: Threshold to determine if an edge exists between two documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bff521b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from nemo_curator import ConnectedComponents"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8afed6a",
   "metadata": {},
   "source": [
    "Define parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b40735dd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Input\n",
    "jaccard_pairs_path = edgelist_output_dir\n",
    "\n",
    "#Output\n",
    "connected_component_base_output_path = os.path.join(data_dir,\"fuzzy/cc\")\n",
    "connected_component_output_path = os.path.join(connected_component_base_output_path, \"connected_components.parquet\")\n",
    "connected_component_cache_dir = os.path.join(connected_component_base_output_path, \"cache\")\n",
    "connected_component_log_path = os.path.join(connected_component_base_output_path,\"log\")\n",
    "\n",
    "#Relevant parameters\n",
    "input_id_field = 'id'\n",
    "\n",
    "!mkdir -p {connected_component_base_output_path}\n",
    "!mkdir -p {connected_component_log_path}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33d8957f",
   "metadata": {},
   "source": [
    "Run Connected Component"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe62dd51",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "t0 = time.time()\n",
    "    \n",
    "components_stage = ConnectedComponents(\n",
    "    cache_dir=connected_component_cache_dir,\n",
    "    jaccard_pairs_path=jaccard_pairs_path,\n",
    "    id_column=input_id_field,\n",
    "    logger=connected_component_log_path,\n",
    ")\n",
    "\n",
    "#Load and run connected component\n",
    "components_stage(output_path=connected_component_output_path)\n",
    "print(f\"Time taken for Connected Component: {time.time()-t0} s\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "669495ee",
   "metadata": {},
   "source": [
    "**[Optional]** Run the following cells to verify the result of `Connected Components`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efbd6973",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# cc_compute_res = pd.read_parquet(connected_component_output_path)\n",
    "# cc_compute_res.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c3e2bdc",
   "metadata": {},
   "source": [
    "Let's check if the output fuzzy duplicated documents within the same group are similar. Please note that the `group` id in your output might be different from the notebook output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8fa1e8e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# cc_compute_res.groupby('group')[input_id_field].agg(list).reset_index()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f34b8140",
   "metadata": {},
   "source": [
    "Change the `group` number if necessary. By running the code below, we can obtain a list of near duplicated documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd01f5fe",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Replace ??? with the group number you want to check\n",
    "# cc_compute_res[cc_compute_res['group']==???].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99a8d732",
   "metadata": {},
   "source": [
    "Print the text of near duplicated document. Please replace the `id` if necessary, `id` should be in the format of `<dataset_id>_<doc_id>`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "68883f58",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['ประเทศสวิตเซอร์แลนด์ ได้เข้าร่วมแข่งขันกีฬาโอลิมปิกเยาวชนฤดูหนาว ครั้งที่ 3 ค.ศ. 2020 (พ.ศ. 2563) ณ เมืองโลซาน ประเทศสวิตเซอร์แลนด์ ระหว่างวันที่ 9 - 22 มกราคม พ.ศ. 2563 คณะกรรมการโอลิมปิกแห่งชาติสวิตเซอร์แลนด์ได้ส่งทีมนักกีฬาเข้าแข่งขันทั้งหมด 56 คน แบ่งเป็นเป็นชาย 32 คนและหญิง 56 คน เข้าร่วมการแข่งขันใน 15 ชนิดกีฬา\\n\\nจำนวนผู้เข้าแข่งขัน\\n\\nผลการแข่งขัน\\n\\nสเกตลีลา\\n\\nสเกตความเร็ว\\n\\nสเกตความเร็วระยะสั้น\\n\\nฮอกกี้น้ำแข็ง\\n\\nเคอร์ลิง\\n\\nสกีลงเขา\\n\\nสกีข้ามทุ่ง\\n\\nสกีกระโดดไกล\\n\\nสกีนอร์ดิกผสม\\n\\nสกีลีลา\\n\\nสกีปีนเขา\\n\\nสโนว์บอร์ด\\n\\nทวิกีฬาฤดูหนาว\\n\\nบอบสเล\\n\\nสเกเลตัน\\n\\nอ้างอิง\\n\\nแหล่งข้อมูลอื่น \\n เว็บไซต์อย่างเป็นทางการ \\n\\nประเทศสวิตเซอร์แลนด์ในโอลิมปิกเยาวชน\\nประเทศที่เข้าร่วมแข่งขันโอลิมปิกเยาวชนฤดูหนาว 2020',\n",
       "       'ประเทศบัลแกเรีย ได้เข้าร่วมแข่งขันกีฬาโอลิมปิกเยาวชนฤดูหนาว ครั้งที่ 3 ค.ศ. 2020 (พ.ศ. 2563) ณ เมืองโลซาน ประเทศสวิตเซอร์แลนด์ ระหว่างวันที่ 9 - 22 มกราคม พ.ศ. 2563 คณะกรรมการโอลิมปิกแห่งชาติบัลแกเรียได้ส่งทีมนักกีฬาเข้าแข่งขันทั้งหมด 18 คน แบ่งเป็นเป็นชาย 11 คนและหญิง 7 คน เข้าร่วมการแข่งขันใน 8 ชนิดกีฬา\\n\\nจำนวนผู้เข้าแข่งขัน\\n\\nผลการแข่งขัน\\n\\nสเกตลีลา\\n\\nสเกตความเร็ว\\n\\nสเกตความเร็วระยะสั้น\\n\\nฮอกกี้น้ำแข็ง\\n\\nเคอร์ลิง\\n\\nสกีลงเขา\\n\\nสกีข้ามทุ่ง\\n\\nสกีกระโดดไกล\\n\\nสกีนอร์ดิกผสม\\n\\nสกีลีลา\\n\\nสกีปีนเขา\\n\\nสโนว์บอร์ด\\n\\nทวิกีฬาฤดูหนาว\\n\\nลูช\\n\\nบอบสเล\\n\\nสเกเลตัน\\n\\nอ้างอิง\\n\\nแหล่งข้อมูลอื่น \\n เว็บไซต์อย่างเป็นทางการ \\n\\nประเทศบัลแกเรียในโอลิมปิกเยาวชน\\nประเทศที่เข้าร่วมแข่งขันโอลิมปิกเยาวชนฤดูหนาว 2020'],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Replace 'ID1' and 'ID2' with IDs you want to check\n",
    "#The output is an example of fuzzy duplicates \n",
    "# df = input_dataset.df.compute()\n",
    "# df[df['id'].isin(['ID1','ID2'])]['text'].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b6578b4",
   "metadata": {},
   "source": [
    "Below is the English translation of the output above. We can see that the two documents are indeed very similar to each other.\n",
    "- `Text 1`:\n",
    "```\n",
    "Switzerland participated in the 3rd Youth Olympic Winter Games in 2020 (B.E. 2563) in Lausanne, Switzerland from January 9 - 22, 2563. The Swiss Olympic Committee sent a total of 56 athletes, consisting of 32 men and 56 women, to compete in 15 sports.\n",
    "Number of Competitors:\n",
    "Competition Results:\n",
    "Figure Skating\n",
    "Speed Skating\n",
    "Short Track Speed Skating\n",
    "Ice Hockey\n",
    "Curling\n",
    "Alpine Skiing\n",
    "Cross-Country Skiing\n",
    "Ski Jumping\n",
    "Nordic Combined\n",
    "Freestyle Skiing\n",
    "Ski Mountaineering\n",
    "Snowboard\n",
    "Biathlon\n",
    "Bobsleigh\n",
    "Skeleton\n",
    "References:\n",
    "Other Resources:\n",
    "Official Website\n",
    "Switzerland at the Youth Olympics\n",
    "Countries at the 2020 Youth Winter Olympics\n",
    "```\n",
    "- `Text 2`:\n",
    "```\n",
    "Bulgaria participated in the 3rd Youth Olympic Winter Games in 2020 (B.E. 2563) in Lausanne, Switzerland from January 9 - 22, 2563. The Bulgarian Olympic Committee sent a total of 18 athletes, consisting of 11 men and 7 women, to compete in 8 sports.\n",
    "Number of Competitors:\n",
    "Competition Results:\n",
    "Figure Skating\n",
    "Speed Skating\n",
    "Short Track Speed Skating\n",
    "Ice Hockey\n",
    "Curling\n",
    "Alpine Skiing\n",
    "Cross-Country Skiing\n",
    "Ski Jumping\n",
    "Nordic Combined\n",
    "Freestyle Skiing\n",
    "Ski Mountaineering\n",
    "Snowboard\n",
    "Biathlon\n",
    "Luge\n",
    "Bobsleigh\n",
    "Skeleton\n",
    "References:\n",
    "Other Resources:\n",
    "Official Website\n",
    "Bulgaria at the Youth Olympics\n",
    "Countries at the 2020 Youth Winter Olympics\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2726cf9",
   "metadata": {},
   "source": [
    "## 6. Remove duplicates\n",
    "\n",
    "Now we have duplicated document IDs output by both exact deduplication and fuzzy deduplication. We will run this section to remove those documents. This is done be loading the output .parquet files and the unicode fixed input dataset in .jsonl as DataFrame. Then use DataFrame operation to remove the duplicated documents."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4dd78db",
   "metadata": {},
   "source": [
    "Define parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0027c8d2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Input\n",
    "dataset_dir = added_id_output_path\n",
    "\n",
    "#Output\n",
    "dudped_output_dir = os.path.join(data_dir,\"remove_duplicate/result.parquet\")\n",
    "\n",
    "#Relevant parameters\n",
    "input_id_field = 'id'\n",
    "id_prefix = add_ID_id_prefix\n",
    "\n",
    "!mkdir -p {dudped_output_dir}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a373860d",
   "metadata": {},
   "source": [
    "We will first process the result of exact deduplication. Since result of exact deduplication contains original ID used in input dataset, it is more straightforward to deal with."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f59e92c3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Load .jsonl dataset\n",
    "input_dataset = DocumentDataset.read_json(dataset_dir, backend='cudf')\n",
    "\n",
    "#Load exact deduplicate result and extract list of duplicated document ID\n",
    "exact_duplicates = DocumentDataset.read_parquet(os.path.join(exact_dedup_output_dir,\"_exact_duplicates.parquet\"), backend='cudf')\n",
    "exact_docs_to_remove = exact_duplicates.df.map_partitions(\n",
    "    lambda x: x[x._hashes.duplicated(keep=\"first\")]\n",
    ")\n",
    "\n",
    "#Remove the duplicated document from input dataset\n",
    "result = input_dataset.df[\n",
    "    ~input_dataset.df[input_id_field].isin(exact_docs_to_remove[input_id_field].compute())\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "746d3673",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Loads result from fuzzy dedup wrapper\n",
    "fuzzy_duplicates = pd.read_parquet(fuzzy_dedup_output_dir)\n",
    "\n",
    "#Generate list of near duplicate document ID\n",
    "fuzzy_docs_to_remove = fuzzy_duplicates[fuzzy_duplicates.duplicated(subset=['group'], keep='first')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62b34838",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Remove near duplicates\n",
    "result = result[~result[input_id_field].isin(fuzzy_docs_to_remove[input_id_field])]\n",
    "\n",
    "#Save final result to local\n",
    "result.to_parquet(dudped_output_dir, write_to_filename=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edfa52ce",
   "metadata": {},
   "source": [
    "Verify the result of duplicate removal. We can see that the number of document in resultant document is less than the original dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78eee9b3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "res = pd.read_parquet(dudped_output_dir)\n",
    "print(f\"Length of duplicate removed dataset:{len(res)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15e07a32",
   "metadata": {},
   "source": [
    "Close the GPU Dask Cluster.You might encounter error such as `Caught signal 11`.It's OK, just rerun the cell again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e807bd7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "client.cluster.close()\n",
    "client.shutdown()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a416a293",
   "metadata": {},
   "source": [
    "## 7. Heuristic Fitlering\n",
    "\n",
    "In this section, we will apply multiple heuristic filters to the dataset, record the heuristic score for documents and documents removed for each filter. For each heuristic filter, the filter calculates a quality scores based on user defined heuristics/algorithms and classifies documents into high quality documents or low quality documents if the quality score is above the user defined threshold.\n",
    "\n",
    "Sample lists of heuristic filters can be found in `./config/`\n",
    "- `heuristic_filter_en.yaml`: Sample heuristic filter list for English dataset\n",
    "- `heuristic_filter_non-en.yaml`:Sample heuristic filter list for Non-English dataset\n",
    "- `heuristic_filter_code.yaml`:Sample heuristic filter list for Code language dataset\n",
    "Please adjust the sample list e.g. remove/add filters or change filter threshold based on your own use case. In this example, `heuristic_filter_non-en.yaml` will be used.\n",
    "\n",
    "For detailed implementation and description of each heuristic filter, please refer to `./NeMo-Curator/nemo-curator/filters/heuristics_filter.py`. For customized heuristic filter implementation, user shall follow the sample implementations, write customized filters and update the .yaml files accordingly.\n",
    "\n",
    "For analysis of impact of each filters on the dataset, user should set `log-score` to true for the filters in the corresponding config .yaml file. This will output quality score for all filters in separate .txt files for each individual filter. With the quality score and filter threshold, use can calculate quality score distribution and other analysis to assess the effectiveness of each filter.\n",
    "\n",
    "In this example, in order to get a comprehensive output of each filter, we are iterating through ever filter using a for loop and saving the intermediate result. This process will involve extensive I/O operations and is less effective. Alternatively, after loading input dataset and filter pipeline, user can simply call `filter_pipeline(dataset)` to obtain the final filtered result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b988ad1e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from nemo_curator.utils.config_utils import build_filter_pipeline\n",
    "from nemo_curator import Score, ScoreFilter\n",
    "from nemo_curator.utils.file_utils import expand_outdir_and_mkdir"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "097a1b48",
   "metadata": {},
   "source": [
    "**[Optional]** The following cell is to remove warning from dask."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44552288",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "\n",
    "# Disable the metadata warning\n",
    "warnings.filterwarnings(\"ignore\", module=\"dask.dataframe.core\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a59699d",
   "metadata": {},
   "source": [
    "Create a CPU Dask Cluster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8f80ab3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "client = get_client(cluster_type=\"cpu\", n_workers=10, processes=True, memory_limit='16GiB')\n",
    "client"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7702918",
   "metadata": {},
   "source": [
    "Define some helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f2e7523",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_dataframe_complement(original_df, filtered_df):\n",
    "    def partition_complement(part_original_df, partition_info=None):\n",
    "        if not partition_info:\n",
    "            return part_original_df\n",
    "        part_filtered_df = filtered_df.get_partition(partition_info[\"number\"])\n",
    "        complement_mask = ~part_original_df.index.isin(part_filtered_df.index.persist())\n",
    "        complement_df = part_original_df[complement_mask]\n",
    "        return complement_df\n",
    "\n",
    "    return original_df.map_partitions(partition_complement)\n",
    "\n",
    "def write_scores(df, output_dir):\n",
    "    for column in df.columns:\n",
    "        output_path = os.path.join(output_dir, f\"{column}.txt\")\n",
    "        df[column].to_csv(output_path, single_file=True, encoding=\"utf-8\", header=False, index=False, mode=\"a\")\n",
    "\n",
    "def get_score_fields(pipeline):\n",
    "    score_fields = []\n",
    "    for nc_module in pipeline.modules:\n",
    "        if isinstance(nc_module, Score) or isinstance(nc_module, ScoreFilter):\n",
    "            if nc_module.score_field:\n",
    "                score_fields.append(nc_module.score_field)\n",
    "    return score_fields"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "227fa8b0",
   "metadata": {},
   "source": [
    "Define parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a894f90f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Input\n",
    "HF_input_data_dir = dudped_output_dir\n",
    "input_file_type = 'parquet'\n",
    "batch_size = 1\n",
    "\n",
    "#Output\n",
    "HF_base_output_path = os.path.join(data_dir,'heuristic_filtering')\n",
    "kept_document_dir =  os.path.join(HF_base_output_path,'data','hq.parquet')\n",
    "removed_document_dir =  os.path.join(HF_base_output_path,'data','lq.parquet')\n",
    "output_document_score_dir =  os.path.join(HF_base_output_path,'data','score')\n",
    "output_file_type = 'parquet'\n",
    "\n",
    "#Relevant parameters\n",
    "filter_config_file = './config/heuristic_filter_non-en.yaml'\n",
    "input_id_field = 'id'\n",
    "\n",
    "#Set to False if do not want to save intermediate results\n",
    "is_cache = True\n",
    "\n",
    "!mkdir -p {kept_document_dir}\n",
    "!mkdir -p {removed_document_dir}\n",
    "!mkdir -p {output_document_score_dir}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccea406e",
   "metadata": {},
   "source": [
    "Run heuristic filtering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03b3da27",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "t0 = time.time()\n",
    "\n",
    "#Load filters from config\n",
    "filter_pipeline = build_filter_pipeline(filter_config_file)\n",
    "score_fields = get_score_fields(filter_pipeline)\n",
    "\n",
    "# Load dataset\n",
    "dataset = DocumentDataset.read_parquet(HF_input_data_dir, files_per_partition=1, blocksize=None, backend='pandas', add_filename=True)\n",
    "\n",
    "\n",
    "# Iterate through filters. For each filter, the low quality document will be removed from the dataset and output to corresponding folder for analysis\n",
    "# Output of previous filter will be input of the next filter\n",
    "if is_cache:\n",
    "    curr_dataset = prev_dataset = dataset\n",
    "    for filter_module in filter_pipeline.modules:\n",
    "        #Apply filter\n",
    "        curr_dataset = filter_module(curr_dataset).persist()\n",
    "\n",
    "        #Output filtered document\n",
    "        print(f\"Saving data for {filter_module.filter_obj._name}\")\n",
    "        removed_df = get_dataframe_complement(prev_dataset.df, curr_dataset.df)\n",
    "        removed_filter_dir = os.path.join(removed_document_dir, filter_module.filter_obj._name)\n",
    "        expand_outdir_and_mkdir(removed_filter_dir)\n",
    "        write_to_disk(removed_df, removed_filter_dir, write_to_filename=True, output_type=output_file_type)\n",
    "        prev_dataset = curr_dataset\n",
    "    filtered_dataset = curr_dataset\n",
    "else:\n",
    "    filtered_dataset = filter_pipeline(dataset)\n",
    "\n",
    "# Write scores of retained doucment to separate directory\n",
    "output_df = filtered_dataset.df[[input_id_field, *score_fields]]\n",
    "write_scores(output_df, output_document_score_dir)\n",
    "\n",
    "# Remove scores from dataset df\n",
    "filtered_dataset = DocumentDataset(filtered_dataset.df.drop(columns=score_fields))\n",
    "\n",
    "# Output filtered dataset\n",
    "filtered_dataset.to_parquet(kept_document_dir, write_to_filename=True)\n",
    "\n",
    "print(f\"Time taken for Heuristic filtering: {time.time()-t0} s\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a53b04e9",
   "metadata": {},
   "source": [
    "**[Optional]** Verify the result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07475373",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# res = pd.read_parquet(kept_document_dir)\n",
    "# print(f\"Dataset size after heuristic filtering:{len(res)}\")\n",
    "# res.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24e8b173",
   "metadata": {},
   "source": [
    "Close the CPU Dask Cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12508f5e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "client.cluster.close()\n",
    "client.shutdown()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "181bb75c-88fb-4c18-a010-b5df36c6c781",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
