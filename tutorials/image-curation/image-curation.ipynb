{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Image Curation in NeMo Curator\n",
    "\n",
    "In the following notebook, we'll be exploring all of the functionality that NeMo Curator has for image dataset curation.\n",
    "NeMo Curator has a few built-in modules for:\n",
    "\n",
    "1. CLIP Image Embedding Creation\n",
    "1. Aesthetic Classification\n",
    "1. Semantic Deduplication\n",
    "1. Not Safe for Work (NSFW) Classification\n",
    "\n",
    "We'll cover the first three modules in this tutorial notebook. First, we'll need to install NeMo Curator!\n",
    "\n",
    "NOTE: Please ensure you meet the [requirements](https://github.com/NVIDIA/NeMo-Curator/tree/main?tab=readme-ov-file#install-nemo-curator) before proceeding!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Table of Contents\n",
    "1. [Download a Sample Dataset](#Download-a-Sample-Dataset)\n",
    "2. [Install NeMo Curator](#Install-NeMo-Curator)\n",
    "3. [Create CLIP Image Embeddings](#Create-CLIP-Image-Embeddings)\n",
    "4. [Aesthetic Classifier](#Aesthetic-Classifier)\n",
    "5. [Semantic Deduplication](#Semantic-Deduplication)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Install NeMo Curator\n",
    "\n",
    "If you have not already, please install NeMo Curator by following the [README](https://github.com/NVIDIA/NeMo-Curator?tab=readme-ov-file#nemo-curator); you should install either `nemo-curator[all]` or `nemo-curator[image]` for this tutorial. If you are using the NeMo Framework Container, then NeMo Curator is already installed and no action is needed.\n",
    "\n",
    "We also need to install some additional libraries for helper functions in the notebook:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "!pip install ipywidgets aiofiles"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download a Sample Dataset\n",
    "If you already have a dataset in webdataset format, great! You can skip to the [next section](#create-clip-embeddings).\n",
    "In order to have a sample dataset to play with, we are going to download a subset of the [Microsoft Common Objects in Context (mscoco)](https://cocodataset.org/#home) dataset.\n",
    "MSCOCO is a dataset of 600,000 image-text pairs (around 76GB) that takes around 20 minutes to download.\n",
    "For the sake of this tutorial, we are only going to download a subset of the dataset.\n",
    "We will download 20,000 image-text pairs (around 3GB).\n",
    "\n",
    "To download the dataset, we are going to use a helper function in `helper.py`. For large scale downloading, you might use something like [img2dataset](https://github.com/rom1504/img2dataset)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we need to get a list of URLs that identify where all the images are hosted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2024-09-19 16:50:18--  https://huggingface.co/datasets/ChristophSchuhmann/MS_COCO_2017_URL_TEXT/resolve/main/mscoco.parquet\n",
      "Resolving huggingface.co (huggingface.co)... 3.168.86.96, 3.168.86.52, 3.168.86.111, ...\n",
      "Connecting to huggingface.co (huggingface.co)|3.168.86.96|:443... connected.\n",
      "HTTP request sent, awaiting response... 302 Found\n",
      "Location: https://cdn-lfs.huggingface.co/datasets/ChristophSchuhmann/MS_COCO_2017_URL_TEXT/2c39f6d88edde6942b67e09eaa53b4f7b387b509f5affcc4f5a5da38081580e2?response-content-disposition=inline%3B+filename*%3DUTF-8%27%27mscoco.parquet%3B+filename%3D%22mscoco.parquet%22%3B&Expires=1727023818&Policy=eyJTdGF0ZW1lbnQiOlt7IkNvbmRpdGlvbiI6eyJEYXRlTGVzc1RoYW4iOnsiQVdTOkVwb2NoVGltZSI6MTcyNzAyMzgxOH19LCJSZXNvdXJjZSI6Imh0dHBzOi8vY2RuLWxmcy5odWdnaW5nZmFjZS5jby9kYXRhc2V0cy9DaHJpc3RvcGhTY2h1aG1hbm4vTVNfQ09DT18yMDE3X1VSTF9URVhULzJjMzlmNmQ4OGVkZGU2OTQyYjY3ZTA5ZWFhNTNiNGY3YjM4N2I1MDlmNWFmZmNjNGY1YTVkYTM4MDgxNTgwZTI%7EcmVzcG9uc2UtY29udGVudC1kaXNwb3NpdGlvbj0qIn1dfQ__&Signature=frn2DWTHSihsgSDh6jsnsGkJD6OxJ3vmvfKdCXF1JTIpGCOvGu1zGT1JBm4yxa0kdW6Ke8ieNKyyBDoY8E5uR7YXJt8av%7Er2xcire3Fb35uArzUt1GVAO-rZ9I3zjLG0VWhTQjPcw4bebAJn4MNdoIwaBTwIOv49awtIvlzGkO4IaxoRxG96KPTwnFCmJlf920W%7EmHAgjHZJeoWc6kupwhDrjzvKXmiO03P2wNSVNJyFz89s3pFQIj9IOl2YdCQFyM8KTGLsZyip%7EWXVFUVv-XuS5x1JzKqaN9WUaXsJzTi8Qhscf7Ad2f-hGrT57EgfgpijtVmvzi0sDGg-O01xDg__&Key-Pair-Id=K3ESJI6DHPFC7 [following]\n",
      "--2024-09-19 16:50:18--  https://cdn-lfs.huggingface.co/datasets/ChristophSchuhmann/MS_COCO_2017_URL_TEXT/2c39f6d88edde6942b67e09eaa53b4f7b387b509f5affcc4f5a5da38081580e2?response-content-disposition=inline%3B+filename*%3DUTF-8%27%27mscoco.parquet%3B+filename%3D%22mscoco.parquet%22%3B&Expires=1727023818&Policy=eyJTdGF0ZW1lbnQiOlt7IkNvbmRpdGlvbiI6eyJEYXRlTGVzc1RoYW4iOnsiQVdTOkVwb2NoVGltZSI6MTcyNzAyMzgxOH19LCJSZXNvdXJjZSI6Imh0dHBzOi8vY2RuLWxmcy5odWdnaW5nZmFjZS5jby9kYXRhc2V0cy9DaHJpc3RvcGhTY2h1aG1hbm4vTVNfQ09DT18yMDE3X1VSTF9URVhULzJjMzlmNmQ4OGVkZGU2OTQyYjY3ZTA5ZWFhNTNiNGY3YjM4N2I1MDlmNWFmZmNjNGY1YTVkYTM4MDgxNTgwZTI%7EcmVzcG9uc2UtY29udGVudC1kaXNwb3NpdGlvbj0qIn1dfQ__&Signature=frn2DWTHSihsgSDh6jsnsGkJD6OxJ3vmvfKdCXF1JTIpGCOvGu1zGT1JBm4yxa0kdW6Ke8ieNKyyBDoY8E5uR7YXJt8av%7Er2xcire3Fb35uArzUt1GVAO-rZ9I3zjLG0VWhTQjPcw4bebAJn4MNdoIwaBTwIOv49awtIvlzGkO4IaxoRxG96KPTwnFCmJlf920W%7EmHAgjHZJeoWc6kupwhDrjzvKXmiO03P2wNSVNJyFz89s3pFQIj9IOl2YdCQFyM8KTGLsZyip%7EWXVFUVv-XuS5x1JzKqaN9WUaXsJzTi8Qhscf7Ad2f-hGrT57EgfgpijtVmvzi0sDGg-O01xDg__&Key-Pair-Id=K3ESJI6DHPFC7\n",
      "Resolving cdn-lfs.huggingface.co (cdn-lfs.huggingface.co)... 3.167.212.45, 3.167.212.67, 3.167.212.53, ...\n",
      "Connecting to cdn-lfs.huggingface.co (cdn-lfs.huggingface.co)|3.167.212.45|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 18269978 (17M) [binary/octet-stream]\n",
      "Saving to: ‘mscoco.parquet’\n",
      "\n",
      "mscoco.parquet      100%[===================>]  17.42M  23.4MB/s    in 0.7s    \n",
      "\n",
      "2024-09-19 16:50:19 (23.4 MB/s) - ‘mscoco.parquet’ saved [18269978/18269978]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!wget https://huggingface.co/datasets/ChristophSchuhmann/MS_COCO_2017_URL_TEXT/resolve/main/mscoco.parquet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We truncate this list of URLs so we don't download the whole dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "NUM_URLS = 20_000\n",
    "urls = pd.read_parquet(\"mscoco.parquet\")\n",
    "deduplicated_urls = urls[~urls[\"URL\"].duplicated()]\n",
    "truncated_urls = deduplicated_urls[:NUM_URLS]\n",
    "truncated_urls.to_parquet(\"truncated_mscoco.parquet\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's start the download."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from helper import download_webdataset\n",
    "\n",
    "download_webdataset(\"truncated_mscoco.parquet\", \"./mscoco\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create CLIP Image Embeddings\n",
    "\n",
    "NeMo Curator's image classifiers take as input the embeddings generated from CLIP models' vision encoders.\n",
    "Semantic deduplication also operates on image embeddings.\n",
    "Therefore, the first step in image curation pipelines involves generating and storing the embeddings so they can be used for the downstream curation stages.\n",
    "\n",
    "### Start the Dask Cluster\n",
    "NeMo Curator runs on Dask and Dask-cuDF to distribute computation. You can read more about it [in the documentation](https://docs.nvidia.com/nemo-framework/user-guide/latest/datacuration/cpuvsgpu.html). All of the image curation modules are GPU-based, so we need to start a GPU-based local Dask cluster before we can use them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nemo_curator import get_client\n",
    "\n",
    "client = get_client(cluster_type=\"gpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the Dataset\n",
    "Instead of operating on the images directly, most features in NeMo Curator take embeddings as inputs. So, as the first stage in the pipeline, we are going to generate embeddings for all the images in the dataset. To begin, let's load the dataset using NeMo Curator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change the dataset path if you have your own dataset\n",
    "dataset_path = \"./mscoco\"\n",
    "# Change the unique identifier depending on your dataset\n",
    "id_field = \"key\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nemo_curator.datasets import ImageTextPairDataset\n",
    "\n",
    "dataset = ImageTextPairDataset.from_webdataset(dataset_path, id_field)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Choose the Embedder\n",
    "We can now define the embedding creation step in our pipeline. NeMo Curator has support for all [timm](https://pypi.org/project/timm/) models. NeMo Curator's aesthetic classifier is trained on embeddings from `vit_large_patch14_clip_quickgelu_224.openai`, so we will use that. This model is the weights of OpenAI's original [ViT-L/14 CLIP variant](https://huggingface.co/openai/clip-vit-large-patch14).\n",
    "\n",
    "The cell below will do the following:\n",
    "1. Download the model `vit_large_patch14_clip_quickgelu_224.openai`.\n",
    "1. Automatically convert the image preprocessing transformations of `vit_large_patch14_clip_quickgelu_224.openai` from their PyTorch form to DALI."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b4bb53a629ef443f8cae29d9af782804",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pytorch_model.bin:   0%|          | 0.00/1.71G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from nemo_curator.image.embedders import TimmImageEmbedder\n",
    "\n",
    "embedding_model = TimmImageEmbedder(\n",
    "    \"vit_large_patch14_clip_quickgelu_224.openai\",\n",
    "    pretrained=True,\n",
    "    batch_size=1024,\n",
    "    num_threads_per_worker=16,\n",
    "    normalize_embeddings=True,\n",
    "    autocast=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now create embeddings for the whole dataset. It's important to understand what is going on internally in NeMo Curator so you can modify parameters appropriately.\n",
    "\n",
    "Once the computation is triggered, the cell below will:\n",
    "1. Load a shard of metadata (a `.parquet` file) onto each GPU you have available using Dask-cuDF.\n",
    "1. Load a copy of `vit_large_patch14_clip_quickgelu_224.openai` onto each GPU.\n",
    "1. Repeatedly load images into batches of size `batch_size` onto each GPU with a given threads per worker (`num_threads_per_worker`) using DALI.\n",
    "1. The model is run on the batch (without `torch.autocast()` since `autocast=False`).\n",
    "1. The output embeddings of the model are normalized since `normalize_embeddings=True`.\n",
    "\n",
    "\n",
    "Since NeMo Curator uses Dask, the cell below will not cause the embeddings to be created. The computation will only begin once we inspect the output in the `dataset.metadata.head()` call or when we write to disk using `dataset.save_metadata()` or `dataset.to_webdataset()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dask is lazy, so this will not compute embeddings\n",
    "dataset = embedding_model(dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since embedding creation can take a long time, we save the embeddings right after they are generated. `dataset.save_metadata()` will add a new column for the image embeddings in the existing `.parquet` files in the dataset directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/app/NeMo-Curator/tutorials/image-curation/mscoco/00000.tar - Embedding creation with vit_large_patch14_clip_quickgelu_224.openai:   0%|          | 0/10000 [00:00<?, ?it/s]/opt/conda/envs/rapids/lib/python3.10/site-packages/nvidia/dali/backend.py:99: Warning: nvidia-dali-cuda120 is no longer shipped with CUDA runtime. You need to install it separately. NPP is typically provided with CUDA Toolkit installation or an appropriate wheel. Please check https://docs.nvidia.com/cuda/cuda-quick-start-guide/index.html#pip-wheels-installation-linux for the reference.\n",
      "  deprecation_warning(\n",
      "[/opt/dali/dali/operators/reader/loader/webdataset_loader.cc:380] Index file not provided, it may take some time to infer it from the tar file\n",
      "/app/NeMo-Curator/tutorials/image-curation/mscoco/00001.tar - Embedding creation with vit_large_patch14_clip_quickgelu_224.openai:   0%|          | 0/10000 [00:00<?, ?it/s]/opt/conda/envs/rapids/lib/python3.10/site-packages/nvidia/dali/backend.py:99: Warning: nvidia-dali-cuda120 is no longer shipped with CUDA runtime. You need to install it separately. NPP is typically provided with CUDA Toolkit installation or an appropriate wheel. Please check https://docs.nvidia.com/cuda/cuda-quick-start-guide/index.html#pip-wheels-installation-linux for the reference.\n",
      "  deprecation_warning(\n",
      "[/opt/dali/dali/operators/reader/loader/webdataset_loader.cc:380] Index file not provided, it may take some time to infer it from the tar file\n",
      "/app/NeMo-Curator/tutorials/image-curation/mscoco/00000.tar - Embedding creation with vit_large_patch14_clip_quickgelu_224.openai: 100%|██████████| 10000/10000 [01:31<00:00, 109.44it/s]\n",
      "/app/NeMo-Curator/tutorials/image-curation/mscoco/00001.tar - Embedding creation with vit_large_patch14_clip_quickgelu_224.openai: 100%|██████████| 10000/10000 [01:50<00:00, 90.79it/s]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>caption</th>\n",
       "      <th>url</th>\n",
       "      <th>key</th>\n",
       "      <th>status</th>\n",
       "      <th>error_message</th>\n",
       "      <th>width</th>\n",
       "      <th>height</th>\n",
       "      <th>original_width</th>\n",
       "      <th>original_height</th>\n",
       "      <th>exif</th>\n",
       "      <th>sha256</th>\n",
       "      <th>image_embedding</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>A man with a red helmet on a small moped on a ...</td>\n",
       "      <td>http://images.cocodataset.org/train2017/000000...</td>\n",
       "      <td>000000000</td>\n",
       "      <td>success</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>640</td>\n",
       "      <td>360</td>\n",
       "      <td>640</td>\n",
       "      <td>360</td>\n",
       "      <td>{}</td>\n",
       "      <td>d0d62a2eed6433c8d65bae1c8dca849eb678d35a8f415e...</td>\n",
       "      <td>[0.047896094620227814, -0.01185801811516285, -...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Man riding a motor bike on a dirt road on the ...</td>\n",
       "      <td>http://images.cocodataset.org/train2017/000000...</td>\n",
       "      <td>000000001</td>\n",
       "      <td>success</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>640</td>\n",
       "      <td>360</td>\n",
       "      <td>640</td>\n",
       "      <td>360</td>\n",
       "      <td>{}</td>\n",
       "      <td>d0d62a2eed6433c8d65bae1c8dca849eb678d35a8f415e...</td>\n",
       "      <td>[0.047896094620227814, -0.01185801811516285, -...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>A man riding on the back of a motorcycle.</td>\n",
       "      <td>http://images.cocodataset.org/train2017/000000...</td>\n",
       "      <td>000000002</td>\n",
       "      <td>success</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>640</td>\n",
       "      <td>360</td>\n",
       "      <td>640</td>\n",
       "      <td>360</td>\n",
       "      <td>{}</td>\n",
       "      <td>d0d62a2eed6433c8d65bae1c8dca849eb678d35a8f415e...</td>\n",
       "      <td>[0.047896094620227814, -0.01185801811516285, -...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>A dirt path with a young person on a motor bik...</td>\n",
       "      <td>http://images.cocodataset.org/train2017/000000...</td>\n",
       "      <td>000000003</td>\n",
       "      <td>success</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>640</td>\n",
       "      <td>360</td>\n",
       "      <td>640</td>\n",
       "      <td>360</td>\n",
       "      <td>{}</td>\n",
       "      <td>d0d62a2eed6433c8d65bae1c8dca849eb678d35a8f415e...</td>\n",
       "      <td>[0.047896094620227814, -0.01185801811516285, -...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>A man in a red shirt and a red hat is on a mot...</td>\n",
       "      <td>http://images.cocodataset.org/train2017/000000...</td>\n",
       "      <td>000000004</td>\n",
       "      <td>success</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>640</td>\n",
       "      <td>360</td>\n",
       "      <td>640</td>\n",
       "      <td>360</td>\n",
       "      <td>{}</td>\n",
       "      <td>d0d62a2eed6433c8d65bae1c8dca849eb678d35a8f415e...</td>\n",
       "      <td>[0.047896094620227814, -0.01185801811516285, -...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             caption  \\\n",
       "0  A man with a red helmet on a small moped on a ...   \n",
       "1  Man riding a motor bike on a dirt road on the ...   \n",
       "2          A man riding on the back of a motorcycle.   \n",
       "3  A dirt path with a young person on a motor bik...   \n",
       "4  A man in a red shirt and a red hat is on a mot...   \n",
       "\n",
       "                                                 url        key   status  \\\n",
       "0  http://images.cocodataset.org/train2017/000000...  000000000  success   \n",
       "1  http://images.cocodataset.org/train2017/000000...  000000001  success   \n",
       "2  http://images.cocodataset.org/train2017/000000...  000000002  success   \n",
       "3  http://images.cocodataset.org/train2017/000000...  000000003  success   \n",
       "4  http://images.cocodataset.org/train2017/000000...  000000004  success   \n",
       "\n",
       "  error_message  width  height  original_width  original_height exif  \\\n",
       "0          <NA>    640     360             640              360   {}   \n",
       "1          <NA>    640     360             640              360   {}   \n",
       "2          <NA>    640     360             640              360   {}   \n",
       "3          <NA>    640     360             640              360   {}   \n",
       "4          <NA>    640     360             640              360   {}   \n",
       "\n",
       "                                              sha256  \\\n",
       "0  d0d62a2eed6433c8d65bae1c8dca849eb678d35a8f415e...   \n",
       "1  d0d62a2eed6433c8d65bae1c8dca849eb678d35a8f415e...   \n",
       "2  d0d62a2eed6433c8d65bae1c8dca849eb678d35a8f415e...   \n",
       "3  d0d62a2eed6433c8d65bae1c8dca849eb678d35a8f415e...   \n",
       "4  d0d62a2eed6433c8d65bae1c8dca849eb678d35a8f415e...   \n",
       "\n",
       "                                     image_embedding  \n",
       "0  [0.047896094620227814, -0.01185801811516285, -...  \n",
       "1  [0.047896094620227814, -0.01185801811516285, -...  \n",
       "2  [0.047896094620227814, -0.01185801811516285, -...  \n",
       "3  [0.047896094620227814, -0.01185801811516285, -...  \n",
       "4  [0.047896094620227814, -0.01185801811516285, -...  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# This triggers the computation\n",
    "# You don't need to save and load normally, but we do it here so\n",
    "# the embeddings aren't recomputed every time they are needed.\n",
    "dataset.save_metadata()\n",
    "dataset = ImageTextPairDataset.from_webdataset(dataset_path, id_field)\n",
    "dataset.metadata.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Aesthetic Classifier\n",
    "With the embeddings now created, we can use the aesthetic classifier. This classifier assigns a score from 0 to 10 that corresponds to how aesthetically pleasing the image is. A score of 0 means that the image is not pleasant to look at, while a score of 10 is pleasant to look at. The exact classifier used is the `LAION-Aesthetics_Predictor V2`. More information on the model can be found here: https://laion.ai/blog/laion-aesthetics/.\n",
    "\n",
    "The following cell will download the model to your local storage at `NEMO_CURATOR_HOME` (`/home/user/.nemo_curator`). The model is only 3.6MB."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nemo_curator.image.classifiers import AestheticClassifier\n",
    "\n",
    "aesthetic_dataset_path = \"./aesthetic_dataset\"\n",
    "aesthetic_classifier = AestheticClassifier()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = aesthetic_classifier(dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After annotating with the aesthetic score, we can filter by the aesthetic score. Finally, we save the resulting filter dataset to a new webdataset.\n",
    "\n",
    "Unlike `dataset.save_metadata()`, `dataset.to_webdataset()` will modify the tar files as well as the `.parquet` files. It will cause the tar files to be resharded and any entries that have an aesthetic score less than or equal to 6 will be removed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/rapids/lib/python3.10/site-packages/nemo_curator/image/classifiers/aesthetic.py:90: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  weights = torch.load(self.model_path, map_location=torch.device(\"cpu\"))\n",
      "/opt/conda/envs/rapids/lib/python3.10/site-packages/nemo_curator/image/classifiers/aesthetic.py:90: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  weights = torch.load(self.model_path, map_location=torch.device(\"cpu\"))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished writing shard 00000.tar with parquet length 110 and tar length 330\n"
     ]
    }
   ],
   "source": [
    "dataset.metadata[\"passes_aesthetic_check\"] = dataset.metadata[\"aesthetic_score\"] > 6\n",
    "dataset.to_webdataset(aesthetic_dataset_path, filter_column=\"passes_aesthetic_check\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize Results\n",
    "Now that we have filtered our dataset based on aesthetic score, we can see what kinds of images are remaining."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tarfile\n",
    "import io\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "\n",
    "def display_image_from_tar(tar_file_path, image_file_name):\n",
    "    # Open the tar file\n",
    "    with tarfile.open(tar_file_path, 'r') as tar:\n",
    "        # Extract the specified image file\n",
    "        image_file = tar.extractfile(image_file_name)\n",
    "        \n",
    "        if image_file is not None:\n",
    "            # Read the image data\n",
    "            image_data = image_file.read()\n",
    "            \n",
    "            # Create a PIL Image object from the image data\n",
    "            image = Image.open(io.BytesIO(image_data))\n",
    "            \n",
    "            # Display the image using matplotlib\n",
    "            plt.figure(figsize=(10, 8))\n",
    "            plt.imshow(image)\n",
    "            plt.axis('off')  # Hide axes\n",
    "            plt.title(f\"Image: {image_file_name}\")\n",
    "            plt.show()\n",
    "        else:\n",
    "            print(f\"Image file '{image_file_name}' not found in the tar archive.\")\n",
    "    \n",
    "\n",
    "output_shard = os.path.join(aesthetic_dataset_path, \"00000.tar\")\n",
    "image_file_name = '000000000.jpg'\n",
    "display_image_from_tar(output_shard, image_file_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Semantic Deduplication\n",
    "\n",
    "NeMo Curator provides an easy module for semantically deduplicating images. Semantic duplicates are images that contain almost the same information content, but are perceptually different. Two images of the same dog taken from slightly different angles would be considered semantic duplicates. NeMo Curator' semantic deduplication approach is based on the paper [SemDeDup: Data-efficient learning at web-scale through semantic deduplication](https://arxiv.org/pdf/2303.09540) by Abbas et al which has demonstrated that deduplicating your data can lead to the same downstream performance in *half* the number of training iterations. For more information on the algorithm, you can check out the [documentation page](https://docs.nvidia.com/nemo-framework/user-guide/latest/datacuration/semdedup.html#data-curator-semdedup).\n",
    "\n",
    "### Parameter Choice\n",
    "There are many options to configure how semantic deduplication behaves. The values below are appropriate for our dataset size, but it's worth mentioning how one might tweak them to scale to larger datasets. We'll focus on parameters that affect the k-means clustering step of semantic deduplication.\n",
    "- `max_iter=10` defines the number of iterations of adjusting the cluster centroids. This value should be ~100 for datasets in the size of $10^6$ images.\n",
    "- `n_clusters=1` defines the number of clusters. Duplicates are only considered within each cluster, so increasing the number of clusters reduces the number of duplicate comparisons made. This will reduce the computational burden, but also allow for more duplicates to slip through. This value should be ~50,000 for datasets in the size of $10^6$ images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-09-19 16:56:38,855 | f62f18cf1933 | Rank 0 | KMeans starting fit\n",
      "2024-09-19 16:56:40,649 | f62f18cf1933 | Rank 0 | KMeans fit complete\n",
      "2024-09-19 16:56:40,652 | f62f18cf1933 | Rank 0 | Computing nearest centroids + distance to centers using kmeans.predict\n",
      "2024-09-19 16:56:40,744 | f62f18cf1933 | Rank 0 | Saving centroids complete\n",
      "2024-09-19 16:56:44,143 | f62f18cf1933 | Rank 0 | Saved embeddings by nearest center to ./semantic_deduplication/cluster_output/embs_by_nearest_center\n",
      "2024-09-19 16:56:44,146 | f62f18cf1933 | Rank 0 | Ranking...\n",
      "2024-09-19 16:56:48,805 | f62f18cf1933 | Rank 0 | Completed 1 clusters. Missing 0 clusters.\n",
      "2024-09-19 16:56:48,808 | f62f18cf1933 | Rank 0 | Time for ranking: 0.08 mins\n",
      "2024-09-19 16:56:48,810 | f62f18cf1933 | Rank 0 | DONE!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from nemo_curator.datasets import DocumentDataset\n",
    "from nemo_curator import ClusteringModel, SemanticClusterLevelDedup\n",
    "\n",
    "# Convert the dataset\n",
    "embeddings_dataset = DocumentDataset(dataset.metadata)\n",
    "\n",
    "semantic_dedup_outputs = \"./semantic_deduplication\"\n",
    "os.makedirs(semantic_dedup_outputs, exist_ok=True)\n",
    "\n",
    "# Run clustering\n",
    "clustering_output = os.path.join(semantic_dedup_outputs, \"cluster_output\")\n",
    "clustering_model = ClusteringModel(\n",
    "    id_field=id_field,\n",
    "    embedding_col=\"image_embedding\",\n",
    "    max_iter=10,\n",
    "    n_clusters=1,\n",
    "    clustering_output_dir=clustering_output,\n",
    ")\n",
    "clustered_dataset = clustering_model(embeddings_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-09-19 16:56:49,966 | f62f18cf1933 | Rank 0 | DONE saving 3995 out of 20000. Removed: 16005. Epsilon: 0.0100\n",
      "2024-09-19 16:56:49,966 | f62f18cf1933 | Rank 0 | DONE saving 3995 out of 20000. Removed: 16005. Epsilon: 0.0100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading 1 files\n"
     ]
    }
   ],
   "source": [
    "# Run cluster-level dedup\n",
    "emb_by_cluster_output = os.path.join(clustering_output, \"embs_by_nearest_center\")\n",
    "sorted_cluster_output = os.path.join(clustering_output, \"sorted\")\n",
    "duplicate_output = os.path.join(semantic_dedup_outputs, \"duplicates\")\n",
    "\n",
    "semantic_dedup = SemanticClusterLevelDedup(\n",
    "    n_clusters=1,\n",
    "    emb_by_clust_dir=emb_by_cluster_output,\n",
    "    sorted_clusters_dir=sorted_cluster_output,\n",
    "    id_field=id_field,\n",
    "    id_field_type=\"str\",\n",
    "    embedding_col=\"image_embedding\",\n",
    "    which_to_keep=\"hard\",\n",
    "    output_dir=duplicate_output,\n",
    ")\n",
    "semantic_dedup.compute_semantic_match_dfs([0.01, 0.001])\n",
    "deduplicated_dataset_ids = semantic_dedup.extract_dedup_data(eps_to_extract=0.01)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remove Duplicates\n",
    "\n",
    "We got a list of deduplicated image IDs. Now we can remove entries with those IDs from our dataset, and reshard the dataset to remove them from the tar files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished writing shard 00000.tar with parquet length 3995 and tar length 11985\n"
     ]
    }
   ],
   "source": [
    "# Remove duplicates\n",
    "deduplicated_dataset_path = \"./deduplicated_dataset\"\n",
    "dataset.metadata[\"is_unique\"] = dataset.metadata[\"key\"].isin(deduplicated_dataset_ids.df[\"key\"].compute())\n",
    "dataset.to_webdataset(deduplicated_dataset_path, \"is_unique\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize Duplicates\n",
    "\n",
    "To better understand what we did in semantic deduplication, let's examine the output of an intermediate step and visualize some of the duplicates NeMo Curator removed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>indices</th>\n",
       "      <th>id</th>\n",
       "      <th>max_id</th>\n",
       "      <th>cosine_sim_score</th>\n",
       "      <th>eps=0.01</th>\n",
       "      <th>eps=0.001</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>8558</th>\n",
       "      <td>8558</td>\n",
       "      <td>000003838</td>\n",
       "      <td>000003833</td>\n",
       "      <td>0.995424</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18909</th>\n",
       "      <td>18909</td>\n",
       "      <td>000008687</td>\n",
       "      <td>000008672</td>\n",
       "      <td>0.991162</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11065</th>\n",
       "      <td>11065</td>\n",
       "      <td>000000020</td>\n",
       "      <td>000000015</td>\n",
       "      <td>0.989709</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18059</th>\n",
       "      <td>18059</td>\n",
       "      <td>000008682</td>\n",
       "      <td>000008672</td>\n",
       "      <td>0.988220</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7667</th>\n",
       "      <td>7667</td>\n",
       "      <td>000008582</td>\n",
       "      <td>000008577</td>\n",
       "      <td>0.987734</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       indices         id     max_id  cosine_sim_score  eps=0.01  eps=0.001\n",
       "8558      8558  000003838  000003833          0.995424      True      False\n",
       "18909    18909  000008687  000008672          0.991162      True      False\n",
       "11065    11065  000000020  000000015          0.989709     False      False\n",
       "18059    18059  000008682  000008672          0.988220     False      False\n",
       "7667      7667  000008582  000008577          0.987734     False      False"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "cluster_path = os.path.join(duplicate_output, \"semdedup_pruning_tables\", \"cluster_0.parquet\")\n",
    "df = pd.read_parquet(cluster_path)\n",
    "df = df[~df[\"eps=0.001\"]]\n",
    "df = df.sort_values(\"cosine_sim_score\", ascending=False)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's examine what the columns mean.\n",
    "1. The `id` column represents the ID of the datapoint.\n",
    "1. `max_id` represents the ID of the image that `id` is the most similar to.\n",
    "1. `cosine_sim_score` is the cosine similarity, where `1` indicates the two images are exactly the same and `0` means the images are completely different.\n",
    "1. `eps=0.01` is `True` if `cosine_sim_score` is `>= 0.99`.\n",
    "1. `eps=0.001` is `True` if `cosine_sim_score` is `>= 0.999`.\n",
    "\n",
    "This dataset has a lot of exact duplicates, so those are caught by `eps=0.001`. We have filtered out the IDs so we don't see them here. There are still a few documents that are similar, but not exact duplicates. We can examine `000008687.jpg` and `000008672.jpg` using our helper function from earlier to see what they look like."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_shard = os.path.join(dataset_path, \"00000.tar\")\n",
    "\n",
    "display_image_from_tar(input_shard, \"000000004.jpg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display_image_from_tar(input_shard, \"000000003.jpg\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, we can see that the images are different but very similar. Despite the small differences, these images would still be considered semantic duplicates. Feel free to adjust the epsilon threshold and see what kinds of images are considered duplicates."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
