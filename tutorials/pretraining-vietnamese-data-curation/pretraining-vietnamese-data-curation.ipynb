{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Processing High-Quality Vietnamese Data: Viettel’s Success with NVIDIA NeMo Curator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Open-source [large language models (LLMs)](https://www.nvidia.com/en-us/glossary/large-language-models/) excel in English but struggle with other languages, especially in Southeast Asia. This is primarily due to a lack of training data in these languages, limited understanding of local cultures, and insufficient tokens to capture unique linguistic structures and expressions. To fully meet customer needs, enterprises in non-English-speaking countries must go beyond generic models and customize them to capture the nuances of their local languages, ensuring a seamless and impactful customer experience.\n",
    "\n",
    "In this tutorial, we will use NeMo Curator to process high-quality [Vietnamese data](https://huggingface.co/datasets/VTSNLP/vietnamese_curated_dataset). We will guide you through the data curation pipeline used and share sample code for each stage."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Table of Contents\n",
    "- **1. [Prerequisites and Environment setups](#prerequisites-and-environment-setups)**\n",
    "- **2. [Data Collecting](#data-collecting)**\n",
    "- **3. [Data Curation flow](#data-curation-flow)**\n",
    "    - a. [Unicode reformatting](#unicode-reformatting)\n",
    "    - b. [Adding Custom IDs to Documents](#adding-custom-ids-to-documents)\n",
    "    - c. [Exact deduplication](#exact-deduplication)\n",
    "    - d. [Heuristic Quality Filtering](#heuristic-quality-filtering)\n",
    "    - e. [Classifier-based Quality Filtering](#classifier-based-quality-filtering)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prerequisites and Environment setups"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Install NeMo Curator by following the instructions to install the CPU and CUDA-accelerated modules in the README file of the [NeMo Curator repository](https://github.com/NVIDIA/NeMo-Curator/tree/main). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, install these additional packages:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.org/simple, https://pypi.ngc.nvidia.com\n",
      "Requirement already satisfied: datasets in /usr/local/lib/python3.10/dist-packages (2.19.0)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from datasets) (3.13.1)\n",
      "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from datasets) (1.24.4)\n",
      "Requirement already satisfied: pyarrow>=12.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (12.0.1)\n",
      "Requirement already satisfied: pyarrow-hotfix in /usr/local/lib/python3.10/dist-packages (from datasets) (0.6)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.3.8)\n",
      "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets) (1.5.3)\n",
      "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (2.31.0)\n",
      "Requirement already satisfied: tqdm>=4.62.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (4.62.3)\n",
      "Requirement already satisfied: xxhash in /usr/local/lib/python3.10/dist-packages (from datasets) (3.4.1)\n",
      "Requirement already satisfied: multiprocess in /usr/local/lib/python3.10/dist-packages (from datasets) (0.70.16)\n",
      "Requirement already satisfied: fsspec<=2024.3.1,>=2023.1.0 in /usr/local/lib/python3.10/dist-packages (from fsspec[http]<=2024.3.1,>=2023.1.0->datasets) (2023.12.2)\n",
      "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets) (3.9.1)\n",
      "Requirement already satisfied: huggingface-hub>=0.21.2 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.22.2)\n",
      "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from datasets) (23.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (6.0.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (23.2.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (6.0.4)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.9.4)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.4.1)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.3.1)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (4.0.3)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.21.2->datasets) (4.11.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (3.6)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (1.26.18)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (2024.2.2)\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2023.3.post1)\n",
      "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.1->pandas->datasets) (1.16.0)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.0\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython -m pip install --upgrade pip\u001b[0m\n",
      "Looking in indexes: https://pypi.org/simple, https://pypi.ngc.nvidia.com\n",
      "Collecting jsonlines\n",
      "  Downloading jsonlines-4.0.0-py3-none-any.whl.metadata (1.6 kB)\n",
      "Requirement already satisfied: attrs>=19.2.0 in /usr/local/lib/python3.10/dist-packages (from jsonlines) (23.2.0)\n",
      "Downloading jsonlines-4.0.0-py3-none-any.whl (8.7 kB)\n",
      "Installing collected packages: jsonlines\n",
      "Successfully installed jsonlines-4.0.0\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.0\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython -m pip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install datasets\n",
    "!pip install jsonlines"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To proceed with data processing, we need to set up a Dask environment. Dask is a flexible, open-source library that enables parallel and distributed computing in Python, allowing us to scale computations across multiple cores or even clusters. By distributing tasks, Dask makes the data handling process significantly faster and more efficient."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note:** This notebook was run on a single DGX A100 GPU, with a 128-core CPU and 2TB of RAM to handle the dataset size. Depending on your dataset and computing resources, you may need to adjust the Dask worker configuration below accordingly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nemo_curator\n",
    "from dask.distributed import Client, LocalCluster\n",
    "\n",
    "# Start a Dask cluster with 12 workers, each limited at 64GB of memory. \n",
    "# You might need to adjust these numbers according to your computing resources.\n",
    "cluster = LocalCluster(n_workers=12, processes=True, memory_limit=\"64GB\")\n",
    "client = Client(cluster)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Collecting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each dataset is accessed and downloaded using the Hugging Face Hub, with additional steps required for OSCAR (the Vietnamese subset dataset, version 23.01, an aggregation of web-crawled data) due to its access restrictions. For OSCAR, you need to accept the conditions on the [dataset page](https://huggingface.co/datasets/oscar-corpus/OSCAR-2301) and use a [Hugging Face access token](https://huggingface.co/docs/hub/en/security-tokens) for downloading."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Download and Convert Datasets to Parquet**\n",
    "\n",
    "The conversion of dataset into Parquet format facilitates efficient handling and processing of large datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/minhnh/python_venv/nlp/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Downloading readme: 100%|██████████| 131k/131k [00:00<00:00, 894kB/s] \n",
      "Downloading data: 100%|██████████| 291M/291M [00:02<00:00, 105MB/s]  \n",
      "Downloading data: 100%|██████████| 71.0M/71.0M [00:00<00:00, 82.4MB/s]\n",
      "Downloading data: 100%|██████████| 50.9M/50.9M [00:00<00:00, 70.6MB/s]\n",
      "Downloading data: 100%|██████████| 316M/316M [00:03<00:00, 102MB/s]  \n",
      "Generating train split: 100%|██████████| 1288680/1288680 [00:04<00:00, 319004.20 examples/s]\n",
      "Creating parquet from Arrow format: 100%|██████████| 1289/1289 [00:04<00:00, 262.30ba/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1617830227"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "from datasets import load_dataset as load_hf_dataset\n",
    "from datasets import DownloadConfig \n",
    "\n",
    "data_dir = \"./datasets/\"\n",
    "download_config = DownloadConfig(num_proc=4)\n",
    "\n",
    "# Load and save Vietnamese Wikipedia dataset\n",
    "# In this experiment, we'll focus exclusively on the Wikipedia dataset to have a faster runtime and streamline the process.\n",
    "ds = load_hf_dataset(\"wikimedia/wikipedia\", \"20231101.vi\")\n",
    "ds[\"train\"].to_parquet(os.path.join(data_dir, \"wiki_vi_231101.parquet\"))\n",
    "\n",
    "# Load and save Vietnamese news corpus\n",
    "ds = load_hf_dataset(\"jetaudio/binhvq_news\")\n",
    "ds[\"train\"].to_parquet(os.path.join(data_dir, \"binhvq_news_train.parquet\"))\n",
    "\n",
    "# Load and save OSCAR dataset\n",
    "ds = load_hf_dataset(\"oscar-corpus/OSCAR-2301\", language=\"vi\", token=True, download_config=download_config, trust_remote_code=True)\n",
    "ds[\"train\"].to_parquet(os.path.join(data_dir, \"oscar_vi.parquet\"))\n",
    "\n",
    "# Load and save C4 dataset\n",
    "ds = load_hf_dataset(\"allenai/c4\", data_files=\"multilingual/c4-vi.*.json.gz\", download_config=download_config, trust_remote_code=True)\n",
    "ds[\"train\"].to_parquet(os.path.join(data_dir, \"c4_vi.parquet\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Combine and Standardize Format**\n",
    "\n",
    "We then combine them into a single dataset, keeping only the \"text\" column. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating train split: 1288680 examples [00:03, 332051.80 examples/s]\n"
     ]
    }
   ],
   "source": [
    "from datasets import concatenate_datasets\n",
    "\n",
    "# Combine datasets and standardize format\n",
    "datasets = [os.path.join(data_dir, file) for file in [\"wiki_vi_231101.parquet\", \"c4_vi.parquet\", \"oscar_vi.parquet\", \"binhvq_news_train.parquet\"]]\n",
    "\n",
    "data_files = {\"train\": datasets[0]}\n",
    "ds = load_hf_dataset(\"parquet\", data_files=data_files)\n",
    "ds = ds[\"train\"].remove_columns([col for col in ds[\"train\"].column_names if col != \"text\"])\n",
    "\n",
    "for d in datasets[1:]:\n",
    "    ds_ = load_hf_dataset(\"parquet\", data_files={\"train\": d})\n",
    "    ds_ = ds_[\"train\"].remove_columns([col for col in ds_[\"train\"].column_names if col != \"text\"])\n",
    "    ds = concatenate_datasets([ds, ds_])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Shard the Combined Dataset**\n",
    "\n",
    "The combined dataset is then sharded into smaller chunks. Sharding is performed to distribute the data evenly across multiple workers in the Dask cluster, facilitating efficient parallel processing during the data curation stages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Creating parquet from Arrow format: 100%|██████████| 6/6 [00:00<00:00, 97.09ba/s]\n",
      "Creating parquet from Arrow format: 100%|██████████| 6/6 [00:00<00:00, 116.71ba/s]\n",
      "Creating parquet from Arrow format: 100%|██████████| 6/6 [00:00<00:00, 112.64ba/s]\n",
      "Creating parquet from Arrow format: 100%|██████████| 6/6 [00:00<00:00, 107.82ba/s]\n",
      "Creating parquet from Arrow format: 100%|██████████| 6/6 [00:00<00:00, 115.92ba/s]\n",
      "Creating parquet from Arrow format: 100%|██████████| 6/6 [00:00<00:00, 117.47ba/s]\n",
      "Creating parquet from Arrow format: 100%|██████████| 6/6 [00:00<00:00, 114.83ba/s]\n",
      "Creating parquet from Arrow format: 100%|██████████| 6/6 [00:00<00:00, 109.00ba/s]\n",
      "Creating parquet from Arrow format: 100%|██████████| 6/6 [00:00<00:00, 112.05ba/s]\n",
      "Creating parquet from Arrow format: 100%|██████████| 6/6 [00:00<00:00, 114.88ba/s]\n",
      "Creating parquet from Arrow format: 100%|██████████| 6/6 [00:00<00:00, 112.04ba/s]\n",
      "Creating parquet from Arrow format: 100%|██████████| 6/6 [00:00<00:00, 113.70ba/s]\n",
      "Creating parquet from Arrow format: 100%|██████████| 6/6 [00:00<00:00, 108.56ba/s]\n",
      "Creating parquet from Arrow format: 100%|██████████| 6/6 [00:00<00:00, 116.50ba/s]\n",
      "Creating parquet from Arrow format: 100%|██████████| 6/6 [00:00<00:00, 112.95ba/s]\n",
      "Creating parquet from Arrow format: 100%|██████████| 6/6 [00:00<00:00, 52.21ba/s]\n",
      "Creating parquet from Arrow format: 100%|██████████| 6/6 [00:00<00:00, 115.49ba/s]\n",
      "Creating parquet from Arrow format: 100%|██████████| 6/6 [00:00<00:00, 115.18ba/s]\n",
      "Creating parquet from Arrow format: 100%|██████████| 6/6 [00:00<00:00, 114.03ba/s]\n",
      "Creating parquet from Arrow format: 100%|██████████| 6/6 [00:00<00:00, 114.23ba/s]\n",
      "Creating parquet from Arrow format: 100%|██████████| 6/6 [00:00<00:00, 115.30ba/s]\n",
      "Creating parquet from Arrow format: 100%|██████████| 6/6 [00:00<00:00, 115.10ba/s]\n",
      "Creating parquet from Arrow format: 100%|██████████| 6/6 [00:00<00:00, 112.70ba/s]\n",
      "Creating parquet from Arrow format: 100%|██████████| 6/6 [00:00<00:00, 114.95ba/s]\n",
      "Creating parquet from Arrow format: 100%|██████████| 6/6 [00:00<00:00, 113.47ba/s]\n",
      "Creating parquet from Arrow format: 100%|██████████| 6/6 [00:00<00:00, 114.77ba/s]\n",
      "Creating parquet from Arrow format: 100%|██████████| 6/6 [00:00<00:00, 113.08ba/s]\n",
      "Creating parquet from Arrow format: 100%|██████████| 6/6 [00:00<00:00, 115.01ba/s]\n",
      "Creating parquet from Arrow format: 100%|██████████| 6/6 [00:00<00:00, 112.83ba/s]\n",
      "Creating parquet from Arrow format: 100%|██████████| 6/6 [00:00<00:00, 113.90ba/s]\n",
      "Creating parquet from Arrow format: 100%|██████████| 6/6 [00:00<00:00, 111.08ba/s]\n",
      "Creating parquet from Arrow format: 100%|██████████| 6/6 [00:00<00:00, 114.52ba/s]\n",
      "Creating parquet from Arrow format: 100%|██████████| 6/6 [00:00<00:00, 114.39ba/s]\n",
      "Creating parquet from Arrow format: 100%|██████████| 6/6 [00:00<00:00, 111.72ba/s]\n",
      "Creating parquet from Arrow format: 100%|██████████| 6/6 [00:00<00:00, 112.77ba/s]\n",
      "Creating parquet from Arrow format: 100%|██████████| 6/6 [00:00<00:00, 113.32ba/s]\n",
      "Creating parquet from Arrow format: 100%|██████████| 6/6 [00:00<00:00, 113.49ba/s]\n",
      "Creating parquet from Arrow format: 100%|██████████| 6/6 [00:00<00:00, 115.26ba/s]\n",
      "Creating parquet from Arrow format: 100%|██████████| 6/6 [00:00<00:00, 116.80ba/s]\n",
      "Creating parquet from Arrow format: 100%|██████████| 6/6 [00:00<00:00, 113.10ba/s]\n",
      "Creating parquet from Arrow format: 100%|██████████| 6/6 [00:00<00:00, 113.27ba/s]\n",
      "Creating parquet from Arrow format: 100%|██████████| 6/6 [00:00<00:00, 113.36ba/s]\n",
      "Creating parquet from Arrow format: 100%|██████████| 6/6 [00:00<00:00, 114.04ba/s]\n",
      "Creating parquet from Arrow format: 100%|██████████| 6/6 [00:00<00:00, 114.98ba/s]\n",
      "Creating parquet from Arrow format: 100%|██████████| 6/6 [00:00<00:00, 115.54ba/s]\n",
      "Creating parquet from Arrow format: 100%|██████████| 6/6 [00:00<00:00, 115.37ba/s]\n",
      "Creating parquet from Arrow format: 100%|██████████| 6/6 [00:00<00:00, 114.88ba/s]\n",
      "Creating parquet from Arrow format: 100%|██████████| 6/6 [00:00<00:00, 113.92ba/s]\n",
      "Creating parquet from Arrow format: 100%|██████████| 6/6 [00:00<00:00, 114.30ba/s]\n",
      "Creating parquet from Arrow format: 100%|██████████| 6/6 [00:00<00:00, 112.03ba/s]\n",
      "Creating parquet from Arrow format: 100%|██████████| 6/6 [00:00<00:00, 114.27ba/s]\n",
      "Creating parquet from Arrow format: 100%|██████████| 6/6 [00:00<00:00, 113.22ba/s]\n",
      "Creating parquet from Arrow format: 100%|██████████| 6/6 [00:00<00:00, 115.45ba/s]\n",
      "Creating parquet from Arrow format: 100%|██████████| 6/6 [00:00<00:00, 112.58ba/s]\n",
      "Creating parquet from Arrow format: 100%|██████████| 6/6 [00:00<00:00, 116.87ba/s]\n",
      "Creating parquet from Arrow format: 100%|██████████| 6/6 [00:00<00:00, 112.07ba/s]\n",
      "Creating parquet from Arrow format: 100%|██████████| 6/6 [00:00<00:00, 114.79ba/s]\n",
      "Creating parquet from Arrow format: 100%|██████████| 6/6 [00:00<00:00, 111.42ba/s]\n",
      "Creating parquet from Arrow format: 100%|██████████| 6/6 [00:00<00:00, 114.41ba/s]\n",
      "Creating parquet from Arrow format: 100%|██████████| 6/6 [00:00<00:00, 112.62ba/s]\n",
      "Creating parquet from Arrow format: 100%|██████████| 6/6 [00:00<00:00, 114.01ba/s]\n",
      "Creating parquet from Arrow format: 100%|██████████| 6/6 [00:00<00:00, 112.91ba/s]\n",
      "Creating parquet from Arrow format: 100%|██████████| 6/6 [00:00<00:00, 112.88ba/s]\n",
      "Creating parquet from Arrow format: 100%|██████████| 6/6 [00:00<00:00, 114.67ba/s]\n",
      "Creating parquet from Arrow format: 100%|██████████| 6/6 [00:00<00:00, 112.68ba/s]\n",
      "Creating parquet from Arrow format: 100%|██████████| 6/6 [00:00<00:00, 115.72ba/s]\n",
      "Creating parquet from Arrow format: 100%|██████████| 6/6 [00:00<00:00, 114.64ba/s]\n",
      "Creating parquet from Arrow format: 100%|██████████| 6/6 [00:00<00:00, 114.75ba/s]\n",
      "Creating parquet from Arrow format: 100%|██████████| 6/6 [00:00<00:00, 111.91ba/s]\n",
      "Creating parquet from Arrow format: 100%|██████████| 6/6 [00:00<00:00, 115.22ba/s]\n",
      "Creating parquet from Arrow format: 100%|██████████| 6/6 [00:00<00:00, 112.71ba/s]\n",
      "Creating parquet from Arrow format: 100%|██████████| 6/6 [00:00<00:00, 112.84ba/s]\n",
      "Creating parquet from Arrow format: 100%|██████████| 6/6 [00:00<00:00, 109.26ba/s]\n",
      "Creating parquet from Arrow format: 100%|██████████| 6/6 [00:00<00:00, 114.71ba/s]\n",
      "Creating parquet from Arrow format: 100%|██████████| 6/6 [00:00<00:00, 109.96ba/s]\n",
      "Creating parquet from Arrow format: 100%|██████████| 6/6 [00:00<00:00, 113.44ba/s]\n",
      "Creating parquet from Arrow format: 100%|██████████| 6/6 [00:00<00:00, 113.84ba/s]\n",
      "Creating parquet from Arrow format: 100%|██████████| 6/6 [00:00<00:00, 115.85ba/s]\n",
      "Creating parquet from Arrow format: 100%|██████████| 6/6 [00:00<00:00, 115.44ba/s]\n",
      "Creating parquet from Arrow format: 100%|██████████| 6/6 [00:00<00:00, 112.42ba/s]\n",
      "Creating parquet from Arrow format: 100%|██████████| 6/6 [00:00<00:00, 111.00ba/s]\n",
      "Creating parquet from Arrow format: 100%|██████████| 6/6 [00:00<00:00, 111.89ba/s]\n",
      "Creating parquet from Arrow format: 100%|██████████| 6/6 [00:00<00:00, 111.36ba/s]\n",
      "Creating parquet from Arrow format: 100%|██████████| 6/6 [00:00<00:00, 113.11ba/s]\n",
      "Creating parquet from Arrow format: 100%|██████████| 6/6 [00:00<00:00, 115.24ba/s]\n",
      "Creating parquet from Arrow format: 100%|██████████| 6/6 [00:00<00:00, 115.32ba/s]\n",
      "Creating parquet from Arrow format: 100%|██████████| 6/6 [00:00<00:00, 115.33ba/s]\n",
      "Creating parquet from Arrow format: 100%|██████████| 6/6 [00:00<00:00, 110.57ba/s]\n",
      "Creating parquet from Arrow format: 100%|██████████| 6/6 [00:00<00:00, 110.41ba/s]\n",
      "Creating parquet from Arrow format: 100%|██████████| 6/6 [00:00<00:00, 112.08ba/s]\n",
      "Creating parquet from Arrow format: 100%|██████████| 6/6 [00:00<00:00, 117.14ba/s]\n",
      "Creating parquet from Arrow format: 100%|██████████| 6/6 [00:00<00:00, 110.52ba/s]\n",
      "Creating parquet from Arrow format: 100%|██████████| 6/6 [00:00<00:00, 113.06ba/s]\n",
      "Creating parquet from Arrow format: 100%|██████████| 6/6 [00:00<00:00, 113.68ba/s]\n",
      "Creating parquet from Arrow format: 100%|██████████| 6/6 [00:00<00:00, 110.73ba/s]\n",
      "Creating parquet from Arrow format: 100%|██████████| 6/6 [00:00<00:00, 112.11ba/s]\n",
      "Creating parquet from Arrow format: 100%|██████████| 6/6 [00:00<00:00, 110.03ba/s]\n",
      "Creating parquet from Arrow format: 100%|██████████| 6/6 [00:00<00:00, 114.50ba/s]\n",
      "Creating parquet from Arrow format: 100%|██████████| 6/6 [00:00<00:00, 110.18ba/s]\n",
      "Creating parquet from Arrow format: 100%|██████████| 6/6 [00:00<00:00, 112.95ba/s]\n",
      "Creating parquet from Arrow format: 100%|██████████| 6/6 [00:00<00:00, 115.13ba/s]\n",
      "Creating parquet from Arrow format: 100%|██████████| 6/6 [00:00<00:00, 115.17ba/s]\n",
      "Creating parquet from Arrow format: 100%|██████████| 6/6 [00:00<00:00, 112.24ba/s]\n",
      "Creating parquet from Arrow format: 100%|██████████| 6/6 [00:00<00:00, 111.62ba/s]\n",
      "Creating parquet from Arrow format: 100%|██████████| 6/6 [00:00<00:00, 114.59ba/s]\n",
      "Creating parquet from Arrow format: 100%|██████████| 6/6 [00:00<00:00, 113.29ba/s]\n",
      "Creating parquet from Arrow format: 100%|██████████| 6/6 [00:00<00:00, 110.60ba/s]\n",
      "Creating parquet from Arrow format: 100%|██████████| 6/6 [00:00<00:00, 113.74ba/s]\n",
      "Creating parquet from Arrow format: 100%|██████████| 6/6 [00:00<00:00, 113.48ba/s]\n",
      "Creating parquet from Arrow format: 100%|██████████| 6/6 [00:00<00:00, 111.49ba/s]\n",
      "Creating parquet from Arrow format: 100%|██████████| 6/6 [00:00<00:00, 113.85ba/s]\n",
      "Creating parquet from Arrow format: 100%|██████████| 6/6 [00:00<00:00, 114.40ba/s]\n",
      "Creating parquet from Arrow format: 100%|██████████| 6/6 [00:00<00:00, 114.61ba/s]\n",
      "Creating parquet from Arrow format: 100%|██████████| 6/6 [00:00<00:00, 115.37ba/s]\n",
      "Creating parquet from Arrow format: 100%|██████████| 6/6 [00:00<00:00, 116.13ba/s]\n",
      "Creating parquet from Arrow format: 100%|██████████| 6/6 [00:00<00:00, 114.38ba/s]\n",
      "Creating parquet from Arrow format: 100%|██████████| 6/6 [00:00<00:00, 114.06ba/s]\n",
      "Creating parquet from Arrow format: 100%|██████████| 6/6 [00:00<00:00, 113.21ba/s]\n",
      "Creating parquet from Arrow format: 100%|██████████| 6/6 [00:00<00:00, 112.15ba/s]\n",
      "Creating parquet from Arrow format: 100%|██████████| 6/6 [00:00<00:00, 110.29ba/s]\n",
      "Creating parquet from Arrow format: 100%|██████████| 6/6 [00:00<00:00, 116.81ba/s]\n",
      "Creating parquet from Arrow format: 100%|██████████| 6/6 [00:00<00:00, 113.69ba/s]\n",
      "Creating parquet from Arrow format: 100%|██████████| 6/6 [00:00<00:00, 116.85ba/s]\n",
      "Creating parquet from Arrow format: 100%|██████████| 6/6 [00:00<00:00, 111.08ba/s]\n",
      "Creating parquet from Arrow format: 100%|██████████| 6/6 [00:00<00:00, 113.02ba/s]\n",
      "Creating parquet from Arrow format: 100%|██████████| 6/6 [00:00<00:00, 114.35ba/s]\n",
      "Creating parquet from Arrow format: 100%|██████████| 6/6 [00:00<00:00, 112.29ba/s]\n",
      "Creating parquet from Arrow format: 100%|██████████| 6/6 [00:00<00:00, 54.24ba/s]\n",
      "Creating parquet from Arrow format: 100%|██████████| 6/6 [00:00<00:00, 111.51ba/s]\n",
      "Creating parquet from Arrow format: 100%|██████████| 6/6 [00:00<00:00, 113.41ba/s]\n",
      "Creating parquet from Arrow format: 100%|██████████| 6/6 [00:00<00:00, 114.16ba/s]\n",
      "Creating parquet from Arrow format: 100%|██████████| 6/6 [00:00<00:00, 111.27ba/s]\n",
      "Creating parquet from Arrow format: 100%|██████████| 6/6 [00:00<00:00, 116.61ba/s]\n",
      "Creating parquet from Arrow format: 100%|██████████| 6/6 [00:00<00:00, 116.42ba/s]\n",
      "Creating parquet from Arrow format: 100%|██████████| 6/6 [00:00<00:00, 113.81ba/s]\n",
      "Creating parquet from Arrow format: 100%|██████████| 6/6 [00:00<00:00, 110.69ba/s]\n",
      "Creating parquet from Arrow format: 100%|██████████| 6/6 [00:00<00:00, 109.03ba/s]\n",
      "Creating parquet from Arrow format: 100%|██████████| 6/6 [00:00<00:00, 111.98ba/s]\n",
      "Creating parquet from Arrow format: 100%|██████████| 6/6 [00:00<00:00, 114.92ba/s]\n",
      "Creating parquet from Arrow format: 100%|██████████| 6/6 [00:00<00:00, 115.09ba/s]\n",
      "Creating parquet from Arrow format: 100%|██████████| 6/6 [00:00<00:00, 113.37ba/s]\n",
      "Creating parquet from Arrow format: 100%|██████████| 6/6 [00:00<00:00, 112.10ba/s]\n",
      "Creating parquet from Arrow format: 100%|██████████| 6/6 [00:00<00:00, 114.35ba/s]\n",
      "Creating parquet from Arrow format: 100%|██████████| 6/6 [00:00<00:00, 115.57ba/s]\n",
      "Creating parquet from Arrow format: 100%|██████████| 6/6 [00:00<00:00, 114.83ba/s]\n",
      "Creating parquet from Arrow format: 100%|██████████| 6/6 [00:00<00:00, 114.02ba/s]\n",
      "Creating parquet from Arrow format: 100%|██████████| 6/6 [00:00<00:00, 115.69ba/s]\n",
      "Creating parquet from Arrow format: 100%|██████████| 6/6 [00:00<00:00, 108.27ba/s]\n",
      "Creating parquet from Arrow format: 100%|██████████| 6/6 [00:00<00:00, 109.10ba/s]\n",
      "Creating parquet from Arrow format: 100%|██████████| 6/6 [00:00<00:00, 114.32ba/s]\n",
      "Creating parquet from Arrow format: 100%|██████████| 6/6 [00:00<00:00, 115.83ba/s]\n",
      "Creating parquet from Arrow format: 100%|██████████| 6/6 [00:00<00:00, 110.94ba/s]\n",
      "Creating parquet from Arrow format: 100%|██████████| 6/6 [00:00<00:00, 112.97ba/s]\n",
      "Creating parquet from Arrow format: 100%|██████████| 6/6 [00:00<00:00, 112.93ba/s]\n",
      "Creating parquet from Arrow format: 100%|██████████| 6/6 [00:00<00:00, 111.58ba/s]\n",
      "Creating parquet from Arrow format: 100%|██████████| 6/6 [00:00<00:00, 114.19ba/s]\n",
      "Creating parquet from Arrow format: 100%|██████████| 6/6 [00:00<00:00, 113.29ba/s]\n",
      "Creating parquet from Arrow format: 100%|██████████| 6/6 [00:00<00:00, 57.04ba/s]\n",
      "Creating parquet from Arrow format: 100%|██████████| 6/6 [00:00<00:00, 113.58ba/s]\n",
      "Creating parquet from Arrow format: 100%|██████████| 6/6 [00:00<00:00, 113.71ba/s]\n",
      "Creating parquet from Arrow format: 100%|██████████| 6/6 [00:00<00:00, 112.46ba/s]\n",
      "Creating parquet from Arrow format: 100%|██████████| 6/6 [00:00<00:00, 110.48ba/s]\n",
      "Creating parquet from Arrow format: 100%|██████████| 6/6 [00:00<00:00, 115.75ba/s]\n",
      "Creating parquet from Arrow format: 100%|██████████| 6/6 [00:00<00:00, 114.04ba/s]\n",
      "Creating parquet from Arrow format: 100%|██████████| 6/6 [00:00<00:00, 113.48ba/s]\n",
      "Creating parquet from Arrow format: 100%|██████████| 6/6 [00:00<00:00, 111.39ba/s]\n",
      "Creating parquet from Arrow format: 100%|██████████| 6/6 [00:00<00:00, 112.21ba/s]\n",
      "Creating parquet from Arrow format: 100%|██████████| 6/6 [00:00<00:00, 110.93ba/s]\n",
      "Creating parquet from Arrow format: 100%|██████████| 6/6 [00:00<00:00, 111.15ba/s]\n",
      "Creating parquet from Arrow format: 100%|██████████| 6/6 [00:00<00:00, 112.19ba/s]\n",
      "Creating parquet from Arrow format: 100%|██████████| 6/6 [00:00<00:00, 112.76ba/s]\n",
      "Creating parquet from Arrow format: 100%|██████████| 6/6 [00:00<00:00, 113.13ba/s]\n",
      "Creating parquet from Arrow format: 100%|██████████| 6/6 [00:00<00:00, 114.49ba/s]\n",
      "Creating parquet from Arrow format: 100%|██████████| 6/6 [00:00<00:00, 116.28ba/s]\n",
      "Creating parquet from Arrow format: 100%|██████████| 6/6 [00:00<00:00, 113.16ba/s]\n",
      "Creating parquet from Arrow format: 100%|██████████| 6/6 [00:00<00:00, 111.92ba/s]\n",
      "Creating parquet from Arrow format: 100%|██████████| 6/6 [00:00<00:00, 113.26ba/s]\n",
      "Creating parquet from Arrow format: 100%|██████████| 6/6 [00:00<00:00, 112.12ba/s]\n",
      "Creating parquet from Arrow format: 100%|██████████| 6/6 [00:00<00:00, 111.68ba/s]\n",
      "Creating parquet from Arrow format: 100%|██████████| 6/6 [00:00<00:00, 112.04ba/s]\n",
      "Creating parquet from Arrow format: 100%|██████████| 6/6 [00:00<00:00, 112.42ba/s]\n",
      "Creating parquet from Arrow format: 100%|██████████| 6/6 [00:00<00:00, 115.48ba/s]\n",
      "Creating parquet from Arrow format: 100%|██████████| 6/6 [00:00<00:00, 111.74ba/s]\n",
      "Creating parquet from Arrow format: 100%|██████████| 6/6 [00:00<00:00, 116.59ba/s]\n",
      "Creating parquet from Arrow format: 100%|██████████| 6/6 [00:00<00:00, 106.84ba/s]\n",
      "Creating parquet from Arrow format: 100%|██████████| 6/6 [00:00<00:00, 111.81ba/s]\n",
      "Creating parquet from Arrow format: 100%|██████████| 6/6 [00:00<00:00, 115.45ba/s]\n",
      "Creating parquet from Arrow format: 100%|██████████| 6/6 [00:00<00:00, 53.03ba/s]\n",
      "Creating parquet from Arrow format: 100%|██████████| 6/6 [00:00<00:00, 114.81ba/s]\n",
      "Creating parquet from Arrow format: 100%|██████████| 6/6 [00:00<00:00, 115.91ba/s]\n",
      "Creating parquet from Arrow format: 100%|██████████| 6/6 [00:00<00:00, 115.89ba/s]\n",
      "Creating parquet from Arrow format: 100%|██████████| 6/6 [00:00<00:00, 114.34ba/s]\n",
      "Creating parquet from Arrow format: 100%|██████████| 6/6 [00:00<00:00, 108.60ba/s]\n",
      "Creating parquet from Arrow format: 100%|██████████| 6/6 [00:00<00:00, 111.49ba/s]\n",
      "Creating parquet from Arrow format: 100%|██████████| 6/6 [00:00<00:00, 110.83ba/s]\n",
      "Creating parquet from Arrow format: 100%|██████████| 6/6 [00:00<00:00, 113.94ba/s]\n",
      "Creating parquet from Arrow format: 100%|██████████| 6/6 [00:00<00:00, 115.04ba/s]\n",
      "Creating parquet from Arrow format: 100%|██████████| 6/6 [00:00<00:00, 113.09ba/s]\n",
      "Creating parquet from Arrow format: 100%|██████████| 6/6 [00:00<00:00, 112.08ba/s]\n",
      "Creating parquet from Arrow format: 100%|██████████| 6/6 [00:00<00:00, 117.43ba/s]\n",
      "Creating parquet from Arrow format: 100%|██████████| 6/6 [00:00<00:00, 113.98ba/s]\n",
      "Creating parquet from Arrow format: 100%|██████████| 6/6 [00:00<00:00, 112.51ba/s]\n",
      "Creating parquet from Arrow format: 100%|██████████| 6/6 [00:00<00:00, 115.36ba/s]\n",
      "Creating parquet from Arrow format: 100%|██████████| 6/6 [00:00<00:00, 116.98ba/s]\n",
      "Creating parquet from Arrow format: 100%|██████████| 6/6 [00:00<00:00, 113.66ba/s]\n",
      "Creating parquet from Arrow format: 100%|██████████| 6/6 [00:00<00:00, 112.98ba/s]\n",
      "Creating parquet from Arrow format: 100%|██████████| 6/6 [00:00<00:00, 111.97ba/s]\n",
      "Creating parquet from Arrow format: 100%|██████████| 6/6 [00:00<00:00, 112.81ba/s]\n",
      "Creating parquet from Arrow format: 100%|██████████| 6/6 [00:00<00:00, 112.61ba/s]\n",
      "Creating parquet from Arrow format: 100%|██████████| 6/6 [00:00<00:00, 114.27ba/s]\n",
      "Creating parquet from Arrow format: 100%|██████████| 6/6 [00:00<00:00, 114.22ba/s]\n",
      "Creating parquet from Arrow format: 100%|██████████| 6/6 [00:00<00:00, 114.66ba/s]\n",
      "Creating parquet from Arrow format: 100%|██████████| 6/6 [00:00<00:00, 112.04ba/s]\n",
      "Creating parquet from Arrow format: 100%|██████████| 6/6 [00:00<00:00, 96.42ba/s]\n",
      "Creating parquet from Arrow format: 100%|██████████| 6/6 [00:00<00:00, 116.12ba/s]\n",
      "Creating parquet from Arrow format: 100%|██████████| 6/6 [00:00<00:00, 116.05ba/s]\n",
      "Creating parquet from Arrow format: 100%|██████████| 6/6 [00:00<00:00, 115.10ba/s]\n",
      "Creating parquet from Arrow format: 100%|██████████| 6/6 [00:00<00:00, 115.75ba/s]\n",
      "Creating parquet from Arrow format: 100%|██████████| 6/6 [00:00<00:00, 116.41ba/s]\n",
      "Creating parquet from Arrow format: 100%|██████████| 6/6 [00:00<00:00, 113.10ba/s]\n",
      "Creating parquet from Arrow format: 100%|██████████| 6/6 [00:00<00:00, 112.80ba/s]\n",
      "Creating parquet from Arrow format: 100%|██████████| 6/6 [00:00<00:00, 112.49ba/s]\n",
      "Creating parquet from Arrow format: 100%|██████████| 6/6 [00:00<00:00, 113.13ba/s]\n",
      "Creating parquet from Arrow format: 100%|██████████| 6/6 [00:00<00:00, 114.02ba/s]\n",
      "Creating parquet from Arrow format: 100%|██████████| 6/6 [00:00<00:00, 114.18ba/s]\n",
      "Creating parquet from Arrow format: 100%|██████████| 6/6 [00:00<00:00, 112.84ba/s]\n",
      "Creating parquet from Arrow format: 100%|██████████| 6/6 [00:00<00:00, 114.07ba/s]\n",
      "Creating parquet from Arrow format: 100%|██████████| 6/6 [00:00<00:00, 116.74ba/s]\n",
      "Creating parquet from Arrow format: 100%|██████████| 6/6 [00:00<00:00, 111.53ba/s]\n",
      "Creating parquet from Arrow format: 100%|██████████| 6/6 [00:00<00:00, 114.91ba/s]\n",
      "Creating parquet from Arrow format: 100%|██████████| 6/6 [00:00<00:00, 114.25ba/s]\n",
      "Creating parquet from Arrow format: 100%|██████████| 6/6 [00:00<00:00, 111.04ba/s]\n",
      "Creating parquet from Arrow format: 100%|██████████| 6/6 [00:00<00:00, 114.62ba/s]\n",
      "Creating parquet from Arrow format: 100%|██████████| 6/6 [00:00<00:00, 111.60ba/s]\n",
      "Creating parquet from Arrow format: 100%|██████████| 6/6 [00:00<00:00, 111.17ba/s]\n",
      "Creating parquet from Arrow format: 100%|██████████| 6/6 [00:00<00:00, 111.62ba/s]\n",
      "Creating parquet from Arrow format: 100%|██████████| 6/6 [00:00<00:00, 113.22ba/s]\n",
      "Creating parquet from Arrow format: 100%|██████████| 6/6 [00:00<00:00, 58.40ba/s]\n",
      "Creating parquet from Arrow format: 100%|██████████| 6/6 [00:00<00:00, 115.48ba/s]\n",
      "Creating parquet from Arrow format: 100%|██████████| 6/6 [00:00<00:00, 115.49ba/s]\n",
      "Creating parquet from Arrow format: 100%|██████████| 6/6 [00:00<00:00, 111.13ba/s]\n",
      "Creating parquet from Arrow format: 100%|██████████| 6/6 [00:00<00:00, 114.91ba/s]\n",
      "Creating parquet from Arrow format: 100%|██████████| 6/6 [00:00<00:00, 115.65ba/s]\n",
      "Creating parquet from Arrow format: 100%|██████████| 6/6 [00:00<00:00, 114.43ba/s]\n",
      "Creating parquet from Arrow format: 100%|██████████| 6/6 [00:00<00:00, 116.22ba/s]\n",
      "Creating parquet from Arrow format: 100%|██████████| 6/6 [00:00<00:00, 114.21ba/s]\n",
      "Creating parquet from Arrow format: 100%|██████████| 6/6 [00:00<00:00, 113.22ba/s]\n",
      "Creating parquet from Arrow format: 100%|██████████| 6/6 [00:00<00:00, 110.18ba/s]\n",
      "Creating parquet from Arrow format: 100%|██████████| 6/6 [00:00<00:00, 111.98ba/s]\n",
      "Creating parquet from Arrow format: 100%|██████████| 6/6 [00:00<00:00, 109.91ba/s]\n",
      "Creating parquet from Arrow format: 100%|██████████| 6/6 [00:00<00:00, 107.83ba/s]\n",
      "Creating parquet from Arrow format: 100%|██████████| 6/6 [00:00<00:00, 111.56ba/s]\n",
      "Creating parquet from Arrow format: 100%|██████████| 6/6 [00:00<00:00, 111.74ba/s]\n",
      "Creating parquet from Arrow format: 100%|██████████| 6/6 [00:00<00:00, 109.96ba/s]\n",
      "Creating parquet from Arrow format: 100%|██████████| 6/6 [00:00<00:00, 113.69ba/s]\n",
      "Creating parquet from Arrow format: 100%|██████████| 6/6 [00:00<00:00, 119.30ba/s]\n"
     ]
    }
   ],
   "source": [
    "# Define paths for raw data\n",
    "raw_data_directory = os.path.join(data_dir, \"raw\")\n",
    "\n",
    "# Shard the dataset\n",
    "num_shards = 256\n",
    "for shard_idx in range(num_shards):\n",
    "    shard = ds.shard(index=shard_idx, num_shards=num_shards)\n",
    "    shard.to_parquet(os.path.join(raw_data_directory, f\"{shard_idx}.parquet\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Curation flow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Unicode reformatting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unicode reformatting is an essential preprocessing step to ensure that text data is standardized and free of encoding errors, which are common in web-crawled datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading 256 files\n",
      "Writing to disk complete for 256 partitions\n"
     ]
    }
   ],
   "source": [
    "from nemo_curator import Modify\n",
    "from nemo_curator.modifiers import UnicodeReformatter\n",
    "from nemo_curator.utils.distributed_utils import read_data, write_to_disk\n",
    "from nemo_curator.utils.file_utils import get_all_files_paths_under\n",
    "from nemo_curator.datasets import DocumentDataset\n",
    "\n",
    "# Define paths for Unicode formatted data\n",
    "unicode_formatted_output_path = os.path.join(data_dir, \"formatted\")\n",
    "\n",
    "# Load the raw data\n",
    "def load_dataset(input_data_dir, file_type=\"parquet\"):\n",
    "    files = list(get_all_files_paths_under(input_data_dir))\n",
    "    raw_data = read_data(files, file_type=file_type, backend=\"pandas\", add_filename=True)\n",
    "    dataset = DocumentDataset(raw_data)\n",
    "\n",
    "    return dataset\n",
    "\n",
    "raw_data = load_dataset(raw_data_directory, file_type=\"parquet\")\n",
    "\n",
    "# Initialize the Unicode reformatter\n",
    "cleaner = Modify(UnicodeReformatter())\n",
    "\n",
    "# Apply Unicode reformatting\n",
    "cleaned_data = cleaner(raw_data)\n",
    "\n",
    "# Save the cleaned data to disk\n",
    "write_to_disk(cleaned_data.df, unicode_formatted_output_path, write_to_filename=True, output_type=\"parquet\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adding Custom IDs to Documents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before proceeding with further curation steps, it is advisable to preprocess the dataset by adding a unique ID to each document. These IDs serve as trackers that help in identifying duplicate or low-quality documents throughout the curation process, ensuring that each document remains uniquely identifiable throughout processing. <br>\n",
    "\n",
    "NeMo Curator offers an `AddId` class, which allows users to insert custom IDs into documents using a specified prefix format, such as `<prefix>_<id>`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading 256 files\n",
      "Writing to disk complete for 256 partitions\n"
     ]
    }
   ],
   "source": [
    "from nemo_curator import AddId\n",
    "\n",
    "# Define paths for input data and output with added IDs\n",
    "add_id_input_data_dir = unicode_formatted_output_path\n",
    "added_id_output_path = os.path.join(data_dir, \"add_id\")\n",
    "add_ID_id_prefix = \"VI_\"\n",
    "\n",
    "# Load the formatted dataset\n",
    "dataset = DocumentDataset.read_parquet(add_id_input_data_dir)\n",
    "\n",
    "# Initialize the AddId class with a specified prefix and start index\n",
    "add_id = AddId(id_field=\"id\", id_prefix=add_ID_id_prefix, start_index=0)\n",
    "\n",
    "# Apply the ID addition to the dataset\n",
    "id_dataset = add_id(dataset)\n",
    "\n",
    "# Save the dataset with added IDs to disk\n",
    "write_to_disk(id_dataset.df, output_file_dir=added_id_output_path, write_to_filename=True, output_type=\"parquet\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exact deduplication"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Exact deduplication removes identical duplicates from the dataset. By eliminating exact duplicates, we ensure that each data point contributes uniquely to the training process, enhancing the diversity and overall quality of the dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this stage, we’ll leverage GPU acceleration by utilizing a Dask CUDA cluster. Since the current cluster is CPU-based, we need to shut it down and start a new one with GPU support.\n",
    "\n",
    "To close the existing cluster:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "client.cluster.close()\n",
    "client.shutdown()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, to initialize the GPU Dask cluster:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"DASK_DATAFRAME__QUERY_PLANNING\"] = \"False\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'tcp://127.0.0.1:38425': None}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nemo_curator.utils.distributed_utils import get_client\n",
    "import dask.dataframe\n",
    "\n",
    "def pre_imports():\n",
    "    import cudf \n",
    "\n",
    "client = get_client(cluster_type=\"gpu\", set_torch_to_use_rmm=False)\n",
    "client.run(pre_imports)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Below is the implementation for exact deduplication:**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Imports and directory preparation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from nemo_curator.modules import ExactDuplicates\n",
    "from nemo_curator.datasets import DocumentDataset\n",
    "\n",
    "# Define input and output paths\n",
    "exact_dedup_input_dataset_dir = added_id_output_path\n",
    "exact_dedup_base_output_path = os.path.join(data_dir, \"exact_dedup\")\n",
    "exact_dedup_log_dir = os.path.join(exact_dedup_base_output_path, \"log\")\n",
    "exact_dedup_output_dir = os.path.join(exact_dedup_base_output_path, \"data\")\n",
    "deduped_output_dir = os.path.join(data_dir, \"remove_duplicate\")\n",
    "\n",
    "# Create directories for logs and output\n",
    "!mkdir -p {exact_dedup_log_dir}\n",
    "!mkdir -p {exact_dedup_output_dir}\n",
    "!mkdir -p {deduped_output_dir}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set parameters and load dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading 256 files\n"
     ]
    }
   ],
   "source": [
    "# Parameters for ExactDuplicates\n",
    "exact_dedup_dataset_id_field = \"id\"\n",
    "exact_dedup_dataset_text_field = \"text\"\n",
    "\n",
    "# Load the input dataset\n",
    "input_dataset = DocumentDataset.read_parquet(exact_dedup_input_dataset_dir, backend=\"cudf\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initialize and run deduplication:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/nemo_curator/modules/exact_dedup.py:158: UserWarning: Output path f./datasets/exact_dedup/data/_exact_duplicates.parquet already exists and will be overwritten\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of exact duplicate files: 751\n"
     ]
    }
   ],
   "source": [
    "# Initialize and run exact deduplication\n",
    "exact_dup = ExactDuplicates(\n",
    "    logger=exact_dedup_log_dir,\n",
    "    id_field=exact_dedup_dataset_id_field,\n",
    "    text_field=exact_dedup_dataset_text_field,\n",
    "    hash_method=\"md5\",\n",
    "    cache_dir=exact_dedup_output_dir\n",
    ")\n",
    "duplicates = exact_dup(dataset=input_dataset)\n",
    "\n",
    "print(f\"Number of exact duplicate files: {len(duplicates)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remove duplicates and save final dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading 256 files\n",
      "Reading 1 files\n",
      "Writing to disk complete for 256 partitions\n"
     ]
    }
   ],
   "source": [
    "# Load the dataset and exact duplicates to identify and remove duplicate IDs\n",
    "input_dataset = DocumentDataset.read_parquet(added_id_output_path, backend=\"cudf\")\n",
    "exact_duplicates = DocumentDataset.read_parquet(\n",
    "    os.path.join(exact_dedup_output_dir, \"_exact_duplicates.parquet\"), backend=\"cudf\"\n",
    ")\n",
    "\n",
    "# Extract list of duplicate document IDs\n",
    "exact_docs_to_remove = exact_duplicates.df.map_partitions(\n",
    "    lambda x: x[x._hashes.duplicated(keep=\"first\")]\n",
    ")\n",
    "\n",
    "# Remove duplicated documents from the input dataset\n",
    "result = input_dataset.df[\n",
    "    ~input_dataset.df[exact_dedup_dataset_id_field].isin(exact_docs_to_remove[exact_dedup_dataset_id_field].compute())\n",
    "]\n",
    "\n",
    "# Save the final deduplicated dataset\n",
    "write_to_disk(result, output_file_dir=deduped_output_dir, write_to_filename=True, output_type=\"parquet\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Close the GPU Dask cluster:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "client.cluster.close()\n",
    "client.shutdown()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Heuristic Quality Filtering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Heuristic quality filtering is designed to enhance the quality of the dataset by removing low-quality content based on predefined heuristics. This approach involves applying a series of filters to the dataset to eliminate undesirable data characteristics such as excessive special characters, overly short or long texts, or other criteria that could negatively impact model performance.\n",
    "\n",
    "We use a YAML file to define the heuristic filters. The configuration can be found [here](https://raw.githubusercontent.com/NVIDIA/NeMo-Curator/main/config/heuristic_filter_non-en.yaml). This file lists the filtering criteria and settings used to build a filter pipeline. You can customize the filters or change thresholds based on your needs. The `filter_pipeline` helper reads the YAML settings and applies each filter to the dataset step by step.\\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recreate a CPU Dask cluster:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start a Dask cluster with 12 workers, each limited at 64GB of memory. \n",
    "# You might need to adjust these numbers according to your computing resources\n",
    "\n",
    "cluster = LocalCluster(n_workers=12, processes=True, memory_limit=\"64GB\")\n",
    "client = Client(cluster)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2024-10-24 09:56:51--  https://raw.githubusercontent.com/NVIDIA/NeMo-Curator/main/config/heuristic_filter_non-en.yaml\n",
      "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.111.133, 185.199.108.133, 185.199.110.133, ...\n",
      "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.111.133|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 3911 (3.8K) [text/plain]\n",
      "Saving to: ‘./config/heuristic_filter_non-en.yaml’\n",
      "\n",
      "./config/heuristic_ 100%[===================>]   3.82K  --.-KB/s    in 0s      \n",
      "\n",
      "2024-10-24 09:56:51 (60.4 MB/s) - ‘./config/heuristic_filter_non-en.yaml’ saved [3911/3911]\n",
      "\n",
      "Reading 256 files\n",
      "Writing to disk complete for 256 partitions\n"
     ]
    }
   ],
   "source": [
    "from nemo_curator.utils.config_utils import build_filter_pipeline\n",
    "import warnings\n",
    "\n",
    "# Define paths for input data and output data after heuristic filtering\n",
    "HF_input_data_dir = deduped_output_dir\n",
    "HF_output_path = os.path.join(data_dir, \"heuristic_filtering\")\n",
    "\n",
    "# Create a directory for the configuration file if it doesn't exist\n",
    "os.makedirs(\"config\", exist_ok=True)\n",
    "# Download the YAML configuration file for heuristic filtering\n",
    "!wget https://raw.githubusercontent.com/NVIDIA/NeMo-Curator/main/config/heuristic_filter_non-en.yaml -O ./config/heuristic_filter_non-en.yaml\n",
    "\n",
    "# Specify the path to the configuration file\n",
    "filter_config_file = \"./config/heuristic_filter_non-en.yaml\"\n",
    "os.makedirs(HF_output_path, exist_ok=True)\n",
    "\n",
    "# Load the filters from the YAML configuration file\n",
    "filter_pipeline = build_filter_pipeline(filter_config_file)\n",
    "\n",
    "# Load the dataset\n",
    "dataset = DocumentDataset.read_parquet(HF_input_data_dir, backend=\"pandas\")\n",
    "\n",
    "# Suppress specific warnings during filtering\n",
    "with warnings.catch_warnings():\n",
    "    warnings.simplefilter(\"ignore\", category=UserWarning)\n",
    "    # Apply the heuristic filters to the dataset\n",
    "    result_data = filter_pipeline(dataset)\n",
    " \n",
    "    # Save the filtered dataset to disk\n",
    "    result_data.to_parquet(HF_output_path, write_to_filename=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classifier-based Quality Filtering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Classifier-based filtering uses a trained classifier model to sort content as high or low quality, offering a smarter and more flexible way to handle diverse datasets that simple rules might miss."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Prepare Data for Training Classifier**\n",
    "\n",
    "To train a quality classifier, we need representative samples of both high-quality and low-quality content. For high-quality data, we use articles from Wikipedia's Vietnamese edition, which are generally well-structured and reliable. The low-quality samples come from unfiltered crawled Vietnamese news corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from datasets import load_dataset as load_hf_dataset\n",
    "from datasets import DownloadConfig "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Creating parquet from Arrow format: 100%|██████████| 162/162 [00:04<00:00, 36.29ba/s]\n",
      "Creating parquet from Arrow format: 100%|██████████| 162/162 [00:04<00:00, 35.98ba/s]\n",
      "Creating parquet from Arrow format: 100%|██████████| 162/162 [00:04<00:00, 36.65ba/s]\n",
      "Creating parquet from Arrow format: 100%|██████████| 162/162 [00:04<00:00, 36.56ba/s]\n",
      "Creating parquet from Arrow format: 100%|██████████| 162/162 [00:04<00:00, 36.52ba/s]\n",
      "Creating parquet from Arrow format: 100%|██████████| 162/162 [00:04<00:00, 36.80ba/s]\n",
      "Creating parquet from Arrow format: 100%|██████████| 162/162 [00:04<00:00, 36.05ba/s]\n",
      "Creating parquet from Arrow format: 100%|██████████| 162/162 [00:04<00:00, 34.72ba/s]\n",
      "Downloading readme: 100%|██████████| 507/507 [00:00<00:00, 2.16MB/s]\n",
      "Downloading data: 100%|██████████| 398M/398M [00:03<00:00, 108MB/s]  \n",
      "Downloading data: 100%|██████████| 414M/414M [00:03<00:00, 109MB/s]  \n",
      "Downloading data: 100%|██████████| 434M/434M [00:04<00:00, 102MB/s]  \n",
      "Downloading data: 100%|██████████| 453M/453M [00:04<00:00, 110MB/s]  \n",
      "Downloading data: 100%|██████████| 475M/475M [00:04<00:00, 109MB/s]  \n",
      "Downloading data: 100%|██████████| 508M/508M [00:04<00:00, 111MB/s]  \n",
      "Downloading data: 100%|██████████| 557M/557M [00:05<00:00, 106MB/s]  \n",
      "Downloading data: 100%|██████████| 633M/633M [00:05<00:00, 108MB/s]  \n",
      "Downloading data: 100%|██████████| 908M/908M [00:08<00:00, 108MB/s]  \n",
      "Generating train split: 100%|██████████| 19365593/19365593 [00:25<00:00, 774462.93 examples/s]\n",
      "Creating parquet from Arrow format: 100%|██████████| 4/4 [00:00<00:00, 145.34ba/s]\n",
      "Creating parquet from Arrow format: 100%|██████████| 4/4 [00:00<00:00, 156.00ba/s]\n",
      "Creating parquet from Arrow format: 100%|██████████| 4/4 [00:00<00:00, 156.70ba/s]\n",
      "Creating parquet from Arrow format: 100%|██████████| 4/4 [00:00<00:00, 157.92ba/s]\n",
      "Creating parquet from Arrow format: 100%|██████████| 4/4 [00:00<00:00, 157.50ba/s]\n",
      "Creating parquet from Arrow format: 100%|██████████| 4/4 [00:00<00:00, 156.67ba/s]\n",
      "Creating parquet from Arrow format: 100%|██████████| 4/4 [00:00<00:00, 157.67ba/s]\n",
      "Creating parquet from Arrow format: 100%|██████████| 4/4 [00:00<00:00, 156.73ba/s]\n",
      "Creating parquet from Arrow format: 100%|██████████| 4/4 [00:00<00:00, 156.00ba/s]\n",
      "Creating parquet from Arrow format: 100%|██████████| 4/4 [00:00<00:00, 157.54ba/s]\n",
      "Creating parquet from Arrow format: 100%|██████████| 4/4 [00:00<00:00, 156.95ba/s]\n",
      "Creating parquet from Arrow format: 100%|██████████| 4/4 [00:00<00:00, 157.07ba/s]\n",
      "Creating parquet from Arrow format: 100%|██████████| 4/4 [00:00<00:00, 155.89ba/s]\n",
      "Creating parquet from Arrow format: 100%|██████████| 4/4 [00:00<00:00, 157.90ba/s]\n",
      "Creating parquet from Arrow format: 100%|██████████| 4/4 [00:00<00:00, 155.33ba/s]\n",
      "Creating parquet from Arrow format: 100%|██████████| 4/4 [00:00<00:00, 157.15ba/s]\n",
      "Creating parquet from Arrow format: 100%|██████████| 4/4 [00:00<00:00, 156.74ba/s]\n",
      "Creating parquet from Arrow format: 100%|██████████| 4/4 [00:00<00:00, 156.10ba/s]\n",
      "Creating parquet from Arrow format: 100%|██████████| 4/4 [00:00<00:00, 155.56ba/s]\n",
      "Creating parquet from Arrow format: 100%|██████████| 4/4 [00:00<00:00, 156.92ba/s]\n",
      "Creating parquet from Arrow format: 100%|██████████| 4/4 [00:00<00:00, 157.26ba/s]\n",
      "Creating parquet from Arrow format: 100%|██████████| 4/4 [00:00<00:00, 157.25ba/s]\n",
      "Creating parquet from Arrow format: 100%|██████████| 4/4 [00:00<00:00, 156.46ba/s]\n",
      "Creating parquet from Arrow format: 100%|██████████| 4/4 [00:00<00:00, 156.83ba/s]\n",
      "Creating parquet from Arrow format: 100%|██████████| 4/4 [00:00<00:00, 157.14ba/s]\n",
      "Creating parquet from Arrow format: 100%|██████████| 4/4 [00:00<00:00, 156.38ba/s]\n",
      "Creating parquet from Arrow format: 100%|██████████| 4/4 [00:00<00:00, 155.85ba/s]\n",
      "Creating parquet from Arrow format: 100%|██████████| 4/4 [00:00<00:00, 157.09ba/s]\n",
      "Creating parquet from Arrow format: 100%|██████████| 4/4 [00:00<00:00, 156.94ba/s]\n",
      "Creating parquet from Arrow format: 100%|██████████| 4/4 [00:00<00:00, 156.79ba/s]\n",
      "Creating parquet from Arrow format: 100%|██████████| 4/4 [00:00<00:00, 156.45ba/s]\n",
      "Creating parquet from Arrow format: 100%|██████████| 4/4 [00:00<00:00, 150.18ba/s]\n"
     ]
    }
   ],
   "source": [
    "# Paths for high-quality and low-quality sample data\n",
    "hq_samples_path = os.path.join(data_dir, \"classifier_filtering/train_samples/hq\")\n",
    "lq_samples_path = os.path.join(data_dir, \"classifier_filtering/train_samples/lq\")\n",
    "\n",
    "# Load and shard the high-quality dataset\n",
    "ds = load_hf_dataset(\"wikimedia/wikipedia\", \"20231101.vi\")\n",
    "num_shards = 8\n",
    "for shard_idx in range(num_shards):\n",
    "    shard = ds[\"train\"].shard(index=shard_idx, num_shards=num_shards)\n",
    "    shard.to_parquet(os.path.join(hq_samples_path, f\"{shard_idx}.parquet\"))\n",
    "\n",
    "# Load and shard the low-quality dataset\n",
    "ds = load_hf_dataset(\"vietgpt/binhvq_news_vi\",split=\"train[:100000]\")\n",
    "num_shards = 32\n",
    "for shard_idx in range(num_shards):\n",
    "    shard = ds.shard(index=shard_idx, num_shards=num_shards)\n",
    "    shard.to_parquet(os.path.join(lq_samples_path, f\"{shard_idx}.parquet\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Training Classifier**\n",
    "\n",
    "The classifier is trained using FastText, which offers an efficient and effective method for text classification. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nemo_curator import Modify\n",
    "from nemo_curator.modifiers import UnicodeReformatter\n",
    "from nemo_curator.utils.distributed_utils import read_data, write_to_disk\n",
    "from nemo_curator.utils.file_utils import get_all_files_paths_under\n",
    "from nemo_curator.datasets import DocumentDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading 32 files\n",
      "Reading 8 files\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Read 24M words\n",
      "Number of words:  843245\n",
      "Number of labels: 2\n",
      "Progress: 100.2% words/sec/thread:   36585 lr: -0.000018 avg.loss:  0.052146 ETA:   0h 0m 0s100.0% words/sec/thread:   36600 lr:  0.000000 avg.loss:  0.052146 ETA:   0h 0m 0s\n"
     ]
    }
   ],
   "source": [
    "from nemo_curator.modifiers import FastTextLabelModifier\n",
    "import fasttext\n",
    "import random\n",
    "\n",
    "# Function to create labeled samples\n",
    "def create_samples(data_path, label, num_samples):\n",
    "    raw_dataset = DocumentDataset.read_parquet(data_path, backend=\"pandas\")\n",
    "    label_quality = Modify(FastTextLabelModifier(label))\n",
    "    labeled_dataset = label_quality(raw_dataset)\n",
    "    labeled_samples = labeled_dataset.df.sample(frac=num_samples / len(labeled_dataset.df))\n",
    "    \n",
    "    return labeled_samples[\"text\"].compute().values.tolist()\n",
    "\n",
    "# Prepare training data\n",
    "low_quality_samples = create_samples(lq_samples_path, \"__label__lq\", 100000)\n",
    "high_quality_samples = create_samples(hq_samples_path, \"__label__hq\", 100000)\n",
    "train_samples = low_quality_samples + high_quality_samples\n",
    "random.shuffle(train_samples)\n",
    "\n",
    "# Save training data to a file\n",
    "train_file = \"./cf_model_fasttext.train\"\n",
    "with open(train_file, \"w\", encoding=\"utf-8\") as f:\n",
    "    for sample in train_samples:\n",
    "        f.write(sample + \"\\n\")\n",
    "\n",
    "# Train the FastText classifier\n",
    "model = fasttext.train_supervised(input=train_file, lr=0.01, dim=100, epoch=5, wordNgrams=2)\n",
    "model_path = \"./cf_model_fasttext_model.bin\"\n",
    "model.save_model(model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Classify and Filter the Dataset**\n",
    "\n",
    "Once trained, the classifier is used to filter the dataset, categorizing documents into high and low quality based on the learned distinctions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading 256 files\n",
      "Writing to disk complete for 256 partitions\n"
     ]
    }
   ],
   "source": [
    "from nemo_curator.filters import FastTextQualityFilter\n",
    "from nemo_curator import ScoreFilter\n",
    "\n",
    "# Define paths and load the dataset\n",
    "CF_input_data_dir = HF_output_path\n",
    "CF_output_path = os.path.join(data_dir, \"classifier_filtering/output\")\n",
    "target_dataset = DocumentDataset.read_parquet(CF_input_data_dir, \"parquet\")\n",
    "\n",
    "# Set up the filtering pipeline\n",
    "filter_pipeline = ScoreFilter(FastTextQualityFilter(model_path), score_field=\"quality_score\", score_type=float)\n",
    "filtered_dataset = filter_pipeline(target_dataset)\n",
    "\n",
    "# Save the filtered dataset\n",
    "write_to_disk(filtered_dataset.df, output_file_dir=CF_output_path, write_to_filename=True, output_type=\"parquet\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Close the CPU Dask cluster:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "client.cluster.close()\n",
    "client.shutdown()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have completed the notebook! For other techniques such as Fuzzy Deduplication or PII redaction, you can go to [NeMo Curator example scripts](https://github.com/NVIDIA/NeMo-Curator/tree/main/examples)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "NLP",
   "language": "python",
   "name": "nlp"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
